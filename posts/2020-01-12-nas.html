<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>David Zmick</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script type='text/javascript' src='/static/scripts.js'></script>
  <link rel='stylesheet' type='text/css' href='/static/base.css' />
  <link rel='stylesheet' type='text/css' id='theme-css' href='/static/base16-tomorrow-night.css' />
</head>

<body>
  <div id="preamble" class="status">
    <div id='#preamble'>
      <h1><a href='/'>dpzmick.com</a></h1>
    </div>
  </div>
  <div id="content">
    <header>
      <h1 class="title">Homelab Act 3: NAS, ZFS, and NFS</h1>
    </header>

    <p>
For the last 172 days (according to uptime on one my routers), I've been setting up a small homelab.
Homelabs are kind of cool, and, the setup has been <i>interesting</i>.
I'll be writing a few posts explaining the steps I took.
</p>

<ul class="org-ul">
<li>Part 1 <a href="./2020-01-09-vpn.html">VPN setup</a></li>
<li>Part 2 <a href="./2020-01-11-server-network.html">Servers and 10GbE</a></li>
<li>Part 3 (this post)</li>
<li>Part 4 <a href="./2020-02-01-homelab4-cloud.html">Services and Applications</a></li>
</ul>

<p>
In this homelab post, I'll be detailing how I converted the R720 I bought on eBay into a NAS server on my local network.
This was an expensive project; I'm not sure it was worth the time or energy.
I'll add more discussion on the usability of something like this in the 4th part of this series.
</p>

<div id="outline-container-orgb167dae" class="outline-2">
<h2 id="orgb167dae">Why?</h2>
<div class="outline-text-2" id="text-orgb167dae">
<p>
I recently had an old SSD fail.
I had no idea it was going bad until I tried to open a handful of old photos and found that they were corrupt.
Some of the smart counters for the drive apparently had ticked up, but I never had set up anything automated to monitor this, so it was missed.
Of course, the files were all backed up, but this still isn't a great experience to have.
</p>

<p>
I wanted a safe place to store some important files.
I wanted these files to be available on all of my devices.
I didn't really want to use a cloud service for this, since I've been trying to scale back my cloud dependence and vendor lock-in problem (but maybe I should have).
</p>

<p>
For a local disk, I would certainly need some form of RAID array (and of course backups, but that's for a later post).
</p>

<p>
Currently in the root of the <code>/nas</code> contains:
</p>
<ul class="org-ul">
<li>audio samples</li>
<li>facebook/google data exports</li>
<li>iso files (windows iso, recent Arch iso, etc)</li>
<li>some ripped CDs that are not on streaming services</li>
<li>RAW photos</li>
<li>old programming projects zipped up</li>
<li>a bunch of papers as pdfs in a folder, unorganized</li>
<li>a bunch of old school work</li>
<li>org mode files</li>
</ul>

<p>
Right now, all of this is is only using about 500 gigs of space.
</p>

<p>
These files used to live on either 1) a spinning rust drive on my desktop or 2) my home directories on multiple machines (not really synced other occasional rsyncs of subsets of the data).
This post will discuss setting up the ZFS and mounting it as a NAS.
</p>
</div>
</div>

<div id="outline-container-orge77f849" class="outline-2">
<h2 id="orge77f849">Setup and Planning</h2>
<div class="outline-text-2" id="text-orge77f849">
<p>
Since I was only using about 500 gigs of space, I wasn't going to need some super high capacity system.
I decided to instead try and build something that would be 1) lowish power (spinning rust is power hungry), 2) pretty "fast", 3) "fun".
</p>

<p>
Since this is all going into my only server, I'd also need some space for all of the other services I wanted to toy with, so I came up with this:
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-left">

<col  class="org-left">

<col  class="org-left">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Data Category</th>
<th scope="col" class="org-left">Space</th>
<th scope="col" class="org-left">Speed</th>
<th scope="col" class="org-left">Redundancy</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Personal Data</td>
<td class="org-left">2TB should last years</td>
<td class="org-left">fast-ish</td>
<td class="org-left">needs to be highly redundant</td>
</tr>

<tr>
<td class="org-left">server drive</td>
<td class="org-left">500GB more than enough</td>
<td class="org-left">fast</td>
<td class="org-left">doesn't need to be redundant</td>
</tr>

<tr>
<td class="org-left">OS drive</td>
<td class="org-left">500GB</td>
<td class="org-left">fast</td>
<td class="org-left">doesn't need to be redundant</td>
</tr>
</tbody>
</table>

<p>
The server has 16 2.5 inch SAS/SATA drive bays connected to a "SAS backplane."
The backplane is connected to a Dell-rebranded LSI hardware RAID controller card.
The RAID controller card is connected to the CPU via 8 Gen 3 PCIe lanes (in a Dell-proprietary form factor slot).
</p>

<p>
The backplane on server hold 16 drives and has across 8 SAS ports.
Each SAS port can do 6 Gbit/s, so we can do (6*8)/8 = 6 Gbytes/s on the RAID controller + backplane, in theory.
This is well matched with the PCIe bandwidth, which is theoretically around 7.9 gigs a second.
</p>

<p>
The good thing about all of this is that the backplane/RAID controller were well integrated into Dell's remote management tools.
The bad thing is of course that many of these parts are proprietary and have strange feature sets, but more on that later.
</p>

<p>
Thinking about the bandwidth capabilities of the server, my redundancy desires, and my low capacity requirements, I decided to try and build this entire thing with SSDs.
SAS SSDs designed for servers aren't cheap, so I decided to look at low-end consumer SATA SSDs.
</p>

<p>
Apparently, most RAID systems don't really like expanding the number of disks in the array.
I decided to price out filling up the system with disks.
</p>

<p>
I ended up with 4 disks from 4 different vendors (reduce risk of all of them failing at the same time):
</p>
<ul class="org-ul">
<li>4x ADATA SU635 480GB 3D-NAND SATA 2.5 inch Internal SSD</li>
<li>4x SanDisk SSD PLUS 480GB Internal SSD - SATA III 6 Gb/s</li>
<li>4x Kingston 480GB A400 Sata3 2.5 Internal SSD</li>
<li>4x PNY CS900 480GB 2.5‚Äù SATA III Internal Solid State Drive</li>
</ul>

<p>
From amazon, this ended up costing like $800, which is, uh, not very cheap.
I also had to grab some disk enclosures on eBay to install these disk into the server.
</p>
</div>
</div>

<div id="outline-container-org4389243" class="outline-2">
<h2 id="org4389243">Filesystem</h2>
<div class="outline-text-2" id="text-org4389243">
<p>
Next thing up, I needed to pick a filesystem/RAID scheme to run on these drives.
</p>
</div>

<div id="outline-container-org5e2fd09" class="outline-3">
<h3 id="org5e2fd09">Hardware RAID</h3>
<div class="outline-text-3" id="text-org5e2fd09">
<p>
I bought the "upgraded" RAID controller when purchasing the server, since I wanted to keep my options open.
After thinking a bit harder about hardware RAID, it doesn't really seem that interesting to me.
Hardware RAID might be a win if I didn't have tons of RAM to spare, or if I was very CPU constrained.
Since neither of those are the case, it seems wiser to use my powerful Xeon CPUs and the large amount of ECC RAM available on the server to do fs checksumming and for caching purposes.
</p>
</div>
</div>

<div id="outline-container-orgcb75351" class="outline-3">
<h3 id="orgcb75351">ZFS</h3>
<div class="outline-text-3" id="text-orgcb75351">
<p>
Awesome ZFS features:
</p>
<ul class="org-ul">
<li>Great reliability features (checksumming in metadata)</li>
<li>Snapshots</li>
<li>Very flexible RAID configurations</li>
<li>Theoretically easy to setup and try out</li>
<li>Fantastic monitoring tools (check out the influxdb intergrations)</li>
<li>Great reviews online</li>
<li>Great arch-wiki documentation</li>
<li>Apparently no <a href="http://www.raid-recovery-guide.com/raid5-write-hole.aspx">RAID write hole problems</a> despite not having a dedicated write-flush backup battery like HW would have</li>
<li>many more</li>
</ul>
</div>
</div>

<div id="outline-container-org9413fa2" class="outline-3">
<h3 id="org9413fa2">Linux software RAID (mdadm and LVM)</h3>
<div class="outline-text-3" id="text-org9413fa2">
<p>
Looks great, but not as featureful as ZFS.
If I try ZFS out and it doesn't work, I figured it would be easy to switch.
</p>

<p>
BTRFS was eliminated early as it seems to still be fairly immature.
</p>
</div>
</div>
</div>

<div id="outline-container-orge70ca2f" class="outline-2">
<h2 id="orge70ca2f">Installation</h2>
<div class="outline-text-2" id="text-orge70ca2f">
</div>
<div id="outline-container-org99f192d" class="outline-3">
<h3 id="org99f192d">Physical Install</h3>
<div class="outline-text-3" id="text-org99f192d">
<p>
Getting these drives into the server was easy.
Just screw them into the enclosures:
</p>


<figure id="org31e791a">
<img src="../static/homelab/disk_in_enclosure.jpg" alt="disk_in_enclosure.jpg">

</figure>

<p>
Then pop them into the front mounting slots:
<img src="../static/homelab/all_installed.jpg" alt="all_installed.jpg">
</p>
</div>
</div>

<div id="outline-container-orga0040a7" class="outline-3">
<h3 id="orga0040a7">Configure RAID</h3>
<div class="outline-text-3" id="text-orga0040a7">
<p>
Next up was configuring the RAID controller to get out of the way.
I wanted the raid controller to just pass the disks through to the operating system.
It also seemed important to make sure that I could access the S.M.A.R.T. status of the devices.
</p>

<p>
Surprise surprise, the <b>upgraded</b> RAID controller I purchased is not able to do this!
Apparently, the lower end model is, but only if you flash the thing with some special alternative firmware that breaks all of the fancy Dell integration.
</p>

<p>
Regardless, I booted the machine with some of the drives installed to see what would happen.
The dell controller was not happy with the consumer drives.
It marked a number of them as degraded, and thought that the kingston drives were SAS drives (maybe they actually are? never figured this out).
Fortunately, it seemed like all of the drives were at least working.
</p>
</div>
</div>

<div id="outline-container-orgea3e387" class="outline-3">
<h3 id="orgea3e387">Downgrade controller</h3>
<div class="outline-text-3" id="text-orgea3e387">
<p>
After a <b>very very</b> large amount of time spent googling around, I found some references that said that, if you get the downgraded Dell H310 mini controller, it is possible to flash the controller to an alternative LSI "IT mode" firmware.
The IT mode firmware is supposed to allow you to just pass the disks through to the OS.
</p>

<p>
Standard flashing procedures won't work though, because Dell looks for some special "I'm a Dell Special Thing" from the device at boot time.
If you flash the board incorrectly, the server will refuse to boot in any way when the board is installed (so you can't reflash it).
</p>

<p>
There's a guy on eBay who will sell you on of these pre-flashed.
Search for "Dell H310 mini monolithic K09CJ with LSI 9211-8i P20 IT Mode" then just buy one from him if you want to do this.
</p>

<p>
I of course didn't go down this path.
Instead, I found some PDF file on archive.org that contained some instructions for flashing the controller.
Since references to this file seem to all go stale, I'm mirroring it <a href="../static/homelab/h310_it.pdf">here</a>, although I keep redoing my blog so this link will probably go stale too.
I booted an Arch Linux iso through the remote managment interface and configured everything from Arch.
</p>

<p>
To follow these instructions, you'll have to find the LSI firmware files.
Since LSI has been acquired like 30 times, its not entirely clear where to find them.
To find these file names, figure out who owns LSI now and go look for their firmware downloads page.
</p>

<p>
You're looking for:
</p>
<ul class="org-ul">
<li><code>9211_8i_Package_P20_IR_IT_FW_BIOS_for_MSDOS_Windows.zip</code> or <code>9211_8i_Package_P20_IR_IT_FW_BIOS_for_MSDOS_Windows.zip</code>. After unzipping, you'll find <code>Firmware/HBA_9211_8i_IT/Firmware/HBA_9211_8i_IT.bin</code></li>
<li><code>UEFI_BSD_P20.zip</code>. After unzipping, you'll find <code>uefi_bsd_rel/Signed/x64sas2.rom</code></li>
</ul>

<p>
Once you have these, you should be able to follow the remaining instructions in the PDF.
</p>

<p>
There's a note in the PDF that says:
</p>
<blockquote>
<p>
Should you want to boot off a drive attached to the H310MM, you will also have to flash the appropriate bootrom (mptsas2.rom for BIOS, x64sas2.rom for UEFI). 
</p>
</blockquote>

<p>
This is a very true statement and you'll be stuck scrathing your head for a long time if you miss/ignore it.
<b>Make sure to also flash the efi firmware to the device.</b>
</p>
</div>
</div>

<div id="outline-container-org2b0a9c6" class="outline-3">
<h3 id="org2b0a9c6">Disk inventory</h3>
<div class="outline-text-3" id="text-org2b0a9c6">
<p>
Since the Dell firmware integration is all broken with the new firmware, I needed to be able to keep track of which drive was which without having being able to easily toggle the chassis LEDs.
I booted an Arch ISO and started <code>dd=ing zeros to each disk through =/dev/disk/by-id/</code>, then recording the serial numbers of the disks whose activity LEDs lit up.
For some reason, the activity LEDs won't light up on the ADAT disks, so I just popped those in and out and watched the kernel logs.
</p>

<p>
All of the serial numbers and slot assignments are saved in a safe place.
This is probably important to have when disks need replacing.
</p>
</div>
</div>

<div id="outline-container-orge13895b" class="outline-3">
<h3 id="orge13895b">Install Operating System</h3>
<div class="outline-text-3" id="text-orge13895b">
<p>
From the Arch iso, I partioned the disk in the 0th slot, and installed Arch using the standard install guide.
</p>
</div>
</div>

<div id="outline-container-org0941e14" class="outline-3">
<h3 id="org0941e14">rootdelay</h3>
<div class="outline-text-3" id="text-org0941e14">
<p>
The OS install when smoothly, so I thought I was finally done with this ordeal once Arch finished bootstrapping the system.
</p>

<p>
Wrong!
</p>

<p>
Linux consistently failed to boot.
I'd get through a GRUB screen, load initrd, then consistently fail to find the root partition.
The root partition was on the same drive as GRUB, so this doesn't really make sense.
</p>

<p>
Apparently, when booting, the EFI system initializes the controller to get the bootloader, Linux initrd, etc.
But then, when the initrd starts, something in Linux's drivers causes the SAS controller to reinit.
The controller takes a long time to initialize, so Linux will have a hard time finding it's boot disk.
</p>

<p>
Adding <code>rootdelay=600</code> to my kernel command line got me passed this problem; now Linux waits for root partition to show up for 5 minutes before giving up on the filesystem.
</p>
</div>
</div>

<div id="outline-container-orgbdeff6b" class="outline-3">
<h3 id="orgbdeff6b">Configure ZFS</h3>
<div class="outline-text-3" id="text-orgbdeff6b">
<p>
Just follow the instructions on the <a href="https://wiki.archlinux.org/index.php/ZFS">Arch Wiki</a>.
I installed the DKMS version of ZFS so that I would be able to <code>pacman -Syu</code> and have <code>pacman</code> attempt to rebuild ZFS with the latest kernel.
</p>

<p>
I setup two zpools.
One for my personal files named <code>nas</code> and another for server stuff named <code>server</code>.
These are mounted, creatively, at <code>/nas</code> and <code>/server</code>.
</p>
</div>

<div id="outline-container-orgdb4391e" class="outline-4">
<h4 id="orgdb4391e"><code>nas</code></h4>
<div class="outline-text-4" id="text-orgdb4391e">
<p>
For the <code>nas</code> zpool, I'm using 12 disk with data striped across two RAIDZ2 zpools.
In other words, each of the RAIDZ2 pools can loose two disk without failing.
All of my data is striped across these two pools.
</p>

<p>
From a performance perspective, check out <a href="https://www.delphix.com/blog/delphix-engineering/zfs-raidz-stripe-width-or-how-i-learned-stop-worrying-and-love-raidz">this post</a>:
</p>
<blockquote>
<p>
For performance on random IOPS, each RAID-Z group has approximately the performance of a single disk in the group. 
</p>
</blockquote>

<p>
So, the performance isn't going to be fantastic on the <code>nas</code> array, if I set it up like this.
I'll pretty only be aggregating across the two stripes, so, assuming read/write of 500mb/s on a standard SATA ssd, I should expect read/write speeds around a gig a second for the pool.
Fortunately, that's exactly what I'm getting.
</p>

<p>
I have no idea if this striping/raidz combination is a good idea or not, but it seems like a reasonable safety/performance tradeoff.
</p>
</div>
</div>

<div id="outline-container-org46d8c06" class="outline-4">
<h4 id="org46d8c06"><code>server</code></h4>
<div class="outline-text-4" id="text-org46d8c06">
<p>
The <code>server</code> array is just a single raidz1 array with 3 disks in it.
This array isn't that interesting and I haven't tried to push it very hard yet.
</p>

<p>
Contiguous reads/writes run at a blistering ~400-500mb/s, as expected.
</p>
</div>
</div>

<div id="outline-container-orgac73648" class="outline-4">
<h4 id="orgac73648">Perfomance Testing</h4>
<div class="outline-text-4" id="text-orgac73648">
<p>
Here's some <i>naive</i> <code>dd</code> performance tests on the disk arrays.
These tests are all performed on the server.
</p>

<p>
For the <code>nas</code> array:
</p>
<div class="org-src-container">
<pre class="src src-bash"><span class="org-comment-delimiter"># </span><span class="org-comment">copy 5 GiB file of random bytes from /tmp (ramdisk), to the ZFS array</span>
$ dd <span class="org-variable-name">if</span>=/tmp/test <span class="org-variable-name">of</span>=test <span class="org-variable-name">bs</span>=2M
2560+0 records<span class="org-keyword"> in</span>
2560+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 5.51472 s, 974 MB/s

<span class="org-comment-delimiter"># </span><span class="org-comment">read the file we just copied to nowhere (immediately after writing)</span>
$ dd <span class="org-variable-name">if</span>=test <span class="org-variable-name">of</span>=/dev/null <span class="org-variable-name">bs</span>=2M
2560+0 records<span class="org-keyword"> in</span>
2560+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 2.96748 s, 1.8 GB/s

<span class="org-comment-delimiter"># </span><span class="org-comment">same thing again (should get some caching effects, sort of getting that)</span>
$ dd <span class="org-variable-name">if</span>=test <span class="org-variable-name">of</span>=/dev/null <span class="org-variable-name">bs</span>=2M
2560+0 records<span class="org-keyword"> in</span>
2560+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 2.2822 s, 2.4 GB/s

<span class="org-comment-delimiter"># </span><span class="org-comment">drop page cache and zfs arc cache, then reread same file</span>
$ dd <span class="org-variable-name">if</span>=test <span class="org-variable-name">of</span>=/dev/null <span class="org-variable-name">bs</span>=2M
2560+0 records<span class="org-keyword"> in</span>
2560+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 4.96745 s, 1.1 GB/s
</pre>
</div>
</div>
</div>

<div id="outline-container-org8fc0232" class="outline-4">
<h4 id="org8fc0232">Sidebar: NVMe</h4>
<div class="outline-text-4" id="text-org8fc0232">
<p>
My desktop has a single $300 NVMe drive in it.
Compare:
</p>
<div class="org-src-container">
<pre class="src src-bash"><span class="org-comment-delimiter"># </span><span class="org-comment">copy 5 GiB file of random bytes to NVMe</span>
$ dd <span class="org-variable-name">if</span>=/tmp/test <span class="org-variable-name">of</span>=test <span class="org-variable-name">bs</span>=4M
1280+0 records<span class="org-keyword"> in</span>
1280+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 4.76216 s, 1.1 GB/s

<span class="org-comment-delimiter"># </span><span class="org-comment">copy to nowhere (pagecache)</span>
$ dd <span class="org-variable-name">if</span>=test <span class="org-variable-name">of</span>=/dev/null <span class="org-variable-name">bs</span>=4M
1280+0 records<span class="org-keyword"> in</span>
1280+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 0.468432 s, 11.5 GB/s

<span class="org-comment-delimiter"># </span><span class="org-comment">drop caches and try again</span>
$ dd <span class="org-variable-name">if</span>=test <span class="org-variable-name">of</span>=/dev/null <span class="org-variable-name">bs</span>=4M
1280+0 records<span class="org-keyword"> in</span>
1280+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 1.76705 s, 3.0 GB/s
</pre>
</div>

<p>
One NVMe/PCIe drive is destroying this expensive array, but that's expected.
If you are going for raw performance, get the NVMe drives and skip the server.
</p>

<p>
In theory, if I stripped across all of these SSDs I'd be able to get competitive, but I have bigger unresolved performance issues with NFS and I already have valuable data on this array, so I have not tried this yet.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org0637829" class="outline-2">
<h2 id="org0637829">NFS</h2>
<div class="outline-text-2" id="text-org0637829">
<p>
Trivial NFS is easy to setup with ZFS.
You can simply install the right NFS servers, then tell ZFS to export the mount point.
</p>
</div>

<div id="outline-container-org1715f2b" class="outline-3">
<h3 id="org1715f2b">NFS performance</h3>
<div class="outline-text-3" id="text-org1715f2b">
<p>
Unfortunately, NFS over my 10 GbE network doesn't perform as well as you'd hope.
</p>

<p>
From an NFS mount over 10 GbE (default mount options, few seem to make a difference but I have more to learn here):
</p>
<div class="org-src-container">
<pre class="src src-bash"><span class="org-comment-delimiter"># </span><span class="org-comment">copy a 5 GiB file of random bytes from /tmp (ramdisk), to the NFS mount</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">From switch stats: NFS isn't saturating the link for some reason.</span>
$ dd <span class="org-variable-name">if</span>=/tmp/test <span class="org-variable-name">of</span>=test <span class="org-variable-name">bs</span>=1M
5120+0 records<span class="org-keyword"> in</span>
5120+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 10.2211 s, 525 MB/s

<span class="org-comment-delimiter"># </span><span class="org-comment">read the file we just copied to nowhere (immediately after writing)</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">again, the switch maxed out at 4gbps during this transfer..</span>
<span class="org-comment-delimiter">#        </span><span class="org-comment">but mostly was nowhere close to the limit</span>
$ dd <span class="org-variable-name">if</span>=test <span class="org-variable-name">of</span>=/dev/null <span class="org-variable-name">bs</span>=1M
5120+0 records<span class="org-keyword"> in</span>
5120+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 16.3145 s, 329 MB/s

<span class="org-comment-delimiter"># </span><span class="org-comment">same thing again</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">better, this time I'm hitting the page cache on my RYZEN box</span>
$ dd <span class="org-variable-name">if</span>=test <span class="org-variable-name">of</span>=/dev/null <span class="org-variable-name">bs</span>=1M
5120+0 records<span class="org-keyword"> in</span>
5120+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 0.614145 s, 8.7 GB/s

<span class="org-comment-delimiter"># </span><span class="org-comment">drop page cache, reread same file</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">again, same deal</span>
$ dd <span class="org-variable-name">if</span>=test <span class="org-variable-name">of</span>=/dev/null <span class="org-variable-name">bs</span>=1M
5120+0 records<span class="org-keyword"> in</span>
5120+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 9.64609 s, 557 MB/s
</pre>
</div>

<p>
As of this time, I haven't attempted to figure out why these rates are so poor.
Trivial network tests with <code>iperf3</code> and some custom code indicate that my NIC drivers and switch are all working properly, so there must be something I need to tune somewhere in the NFS layer.
</p>

<p>
I can trivally saturate gigabit with these rates, which means I'm also trivialy saturating the uplink through my <a href="./2020-01-09-vpn.html">VPN</a> as well.
Since I'm currently spending more of my time connected to the VPN from remote places (with less than gigabit bw), optimizing the NFS has not been a priority.
</p>
</div>
</div>
</div>

<div id="outline-container-org5398e8c" class="outline-2">
<h2 id="org5398e8c">Using the system</h2>
<div class="outline-text-2" id="text-org5398e8c">
<p>
NFS works as well as I'd expect it to, but I'll discuss this and a few other details in a future post.
</p>

<p>
I've copied a bunch of files onto the nas mount from my laptop and desktop, both locally and remotely.
ZFS has been rock solid and the DKMS builder has rebuilt the modules successfully so far during kernel upgrades.
</p>

<p>
A ZFS scrub detected on checksum error, but fixed itself.
All disk report that they are healthy.
Cosmic rays?
</p>

<p>
The biggest win by far is having my orgmode files available on all of my computers without using some third party to do syncing.
</p>

<p>
Overall, I'm reasonably happy with this setup, although I'm wondering if I should have just setup some sort of FUSE mount of B2 and moved on with life.
Getting this to work was a lot of work, and the amount number things that need to not break is large.
The local network performance doesn't help me at all when I'm remote, which is most of the time.
</p>
</div>
</div>

  </div>

  <div id="postamble" class="status">
    <div id='#postamble'>
      <a href='/'>home</a>
      <a class='need-js' href='#/' onclick='switchModes()'>switch-color-mode</a>
    </div>
  </div>
</body>
</html>

</body>

</html>
