<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 <title>dpzmick.com</title>
  <link rel="alternate" type="text/html" href="http://dpzmick.com"/>
  <link rel="self" type="application/atom+xml" href="https://dpzmick.com/feed.xml"/>
  <author>
    <name>David Zmick</name>
    <uri>http://dpzmick.com</uri>
    <email>david@dpzmick.com</email>
  </author>
  <entry>
    <title>Setting many Teensy 4.1 Pins at the same time</title>
    <link rel="alternate" type="text/html" href="http://dpzmick.com/posts/2021-10-26-teensy-4.1-set-many-pins.html"/>
    <id>urn:uuid:c567645f-5a21-569d-8c2f-1b488ff59822</id>
    <updated>2021-10-26</updated>
    <content type="html">
      <![CDATA[<style>.org-src-container {white-space: pre; font-family: monospace }</style>
               <p>
A project I've been working on recently requires that I set a group of pins (4 of them) on a Teensy 4.1 to certain values at roughly the same time.
For simplicity, I'm using the Arduino libraries for this project (but writing C++).
We can fortunately easily look under the covers with the Teensy's Arduino implementation.
In this post I'm going to walk through a few bad ways this can be done, and the correct way (for Teensy 4.1).
</p>

<div id="outline-container-org5ee7de9" class="outline-2">
<h2 id="org5ee7de9">Naive Implementation</h2>
<div class="outline-text-2" id="text-org5ee7de9">
<p>
The simplest possible way to set multiple pins is to just set them one at a time.
</p>

<p>
For example, we could write:
</p>
<div class="org-src-container">
<pre class="src src-cpp"><span class="org-type">int</span> <span class="org-function-name">main</span>( <span class="org-type">void</span> )
{
  <span class="org-comment-delimiter">// </span><span class="org-comment">configure a few pins as output pins</span>
  pinMode( 23, OUTPUT );
  pinMode( 22, OUTPUT );
  pinMode( 21, OUTPUT );
  pinMode( 20, OUTPUT );

  <span class="org-type">int</span> <span class="org-variable-name">state</span> = 0;
  <span class="org-keyword">while</span>( 1 ) {
    <span class="org-keyword">asm</span> <span class="org-keyword">volatile</span>( <span class="org-string">"@ --- writes happen here"</span> ); <span class="org-comment-delimiter">// </span><span class="org-comment">helpful for navigating generated asm</span>
    digitalWriteFast( 23, state );
    digitalWriteFast( 22, state );
    digitalWriteFast( 21, state );
    digitalWriteFast( 20, state );
    <span class="org-keyword">asm</span> <span class="org-keyword">volatile</span>( <span class="org-string">"@ --- done with writes"</span> );

    <span class="org-comment-delimiter">// </span><span class="org-comment">toggle state and wait for a bit</span>
    state = <span class="org-negation-char">!</span>state;
    delayMicroseconds( 10 );
  }
}
</pre>
</div>

<p>
Compiling and running this code against a teensy and some LEDs, we can see (if we bump up the delay) that the lights flash on and off at about the same time.
Let's try measuring the behavior with a slightly better tool than our eyes.
I connected each pin to an oscilloscope.
Channel 1 is pin 23, channel 2 is pin 22, and so on.
</p>

<p>
Let's take a look at both the:
</p>
<ul class="org-ul">
<li>Rising Edge. <code>state</code> was 0, is now 1. The pin output moves from 0V to 3.3V</li>
<li>Falling Edge. <code>state</code> was 1, is now 0. The pin output moves from 3.3V to 0V.</li>
</ul>

<p>
Rising edge:
</p>


<figure id="org4cf218e">
<img src="../static/teensy-many-pins/naive_rise.png" alt="naive_rise.png">

</figure>

<p>
Falling edge:
</p>


<figure id="org611c38b">
<img src="../static/teensy-many-pins/naive_fall.png" alt="naive_fall.png">

</figure>

<p>
The scope conveniently measured a roughly 4ish nanosecond rise time and a 3ish ns fall time for each individual pin outputs.
This is pretty fast (I think) for each <span class="underline">individual</span> pin, but for this project I want to set all of the pins <span class="underline">at the same time</span>.
</p>

<p>
These pin outputs are clearly not changing at the same time.
Eyeballing, pin 23 reaches it's steady state output value roughly 4-5ish nanoseconds before pin 22.
</p>

<p>
Of course, this should not be surprising given that the code is setting the pin values one at a time.
</p>

<p>
Let's dig in.
</p>
</div>

<div id="outline-container-orgbbb66bd" class="outline-3">
<h3 id="orgbbb66bd">ARM assembly</h3>
<div class="outline-text-3" id="text-orgbbb66bd">
<p>
This code compiles to something much more complicated that the simple "set pin x to value v" function calls would indicate.
The Teensy 4.1 doesn't actually have any facility with which it can set a <span class="underline">single</span> output pin!
</p>

<p>
Instead, the output pins are mapped to GPIO port.
Each port controls some number of pins, and, any assignment to a port will change the state of all the pins controlled by that port.
</p>

<p>
These ports are "mapped" into memory at well-documented address.
To write to a port from a program running on the Teensy, the program just needs to write a value to a well-known address in memory (also called "registers").
When the memory system for the microcontroller observes a store to these magic addresses, it will do the electrical magic required to change the voltage on the appropriate pins.
</p>

<p>
For each port, there are two registers that we care about (and one that we could use, but aren't):
</p>
<ul class="org-ul">
<li>A "set" register which turns "on" certain pins mapped to a given port</li>
<li>A "clear" register which turns "off" certain pins mapped to a given port</li>
<li>A "toggle" register to toggle the value on certain pins mapped to a given port</li>
</ul>

<p>
When writing to each of these registers, we supply a bitmask of the pins to modify.
For example, we could write a mask like <code>1010</code> to some "set" register to turn on every other pin, then write <code>1010</code> to a "clear" register to turn them all off again.
</p>

<p>
Rephrasing the code above in these terms, we actually are doing something like:
</p>
<div class="org-src-container">
<pre class="src src-cpp"><span class="org-type">int</span> <span class="org-variable-name">state</span> = 0;
<span class="org-keyword">while</span>( 1 ) {
  <span class="org-keyword">if</span>( state ) {
    assign_to_set_register( 0b1000 ); <span class="org-comment-delimiter">// </span><span class="org-comment">turn on Pin 1</span>
    assign_to_set_register( 0b1100 ); <span class="org-comment-delimiter">// </span><span class="org-comment">turn on Pin 1 and 2</span>
    assign_to_set_register( 0b1110 ); <span class="org-comment-delimiter">// </span><span class="org-comment">turn on Pin 1, 2, and 3</span>
    assign_to_set_register( 0b1111 ); <span class="org-comment-delimiter">// </span><span class="org-comment">turn on Pin 1, 2, 3, and 4</span>
  }
  <span class="org-keyword">else</span> {
    assign_to_clear_register( 0b1000 ); <span class="org-comment-delimiter">// </span><span class="org-comment">turn on Pin 1</span>
    assign_to_clear_register( 0b1100 ); <span class="org-comment-delimiter">// </span><span class="org-comment">turn on Pin 1 and 2</span>
    assign_to_clear_register( 0b1110 ); <span class="org-comment-delimiter">// </span><span class="org-comment">turn on Pin 1, 2, and 3</span>
    assign_to_clear_register( 0b1111 ); <span class="org-comment-delimiter">// </span><span class="org-comment">turn on Pin 1, 2, 3, and 4</span>
  }
}
</pre>
</div>

<p>
Expressed this way, it looks really silly!
We should clearly just turning every pin on in the first assignment (we'll get there, be patient).
</p>

<p>
Inspecting the generated assembly code (which is actually how I figured out what is going on), we can see this behavior:
</p>
<div class="org-src-container">
<pre class="src src-asm"><span class="org-comment-delimiter">; </span><span class="org-comment">compute the bitmask values into r9, r8, ip, lr</span>
<span class="org-comment-delimiter">;</span>
<span class="org-comment-delimiter">; </span><span class="org-comment">.... some other code ....</span>
<span class="org-comment-delimiter">;</span>
<span class="org-comment-delimiter">; </span><span class="org-comment">--- turn on using the SET registers</span>
<span class="org-function-name">str</span>     <span class="org-keyword">r9</span>, [r4, #132]  @ tmp149, MEM[(struct IMXRT_GPIO_t *)1107296256B].DR_SET
<span class="org-function-name">str</span>     <span class="org-keyword">r8</span>, [r4, #132]  @ tmp150, MEM[(struct IMXRT_GPIO_t *)1107296256B].DR_SET
<span class="org-function-name">str</span>     <span class="org-keyword">ip</span>, [r4, #132]  @ tmp151, MEM[(struct IMXRT_GPIO_t *)1107296256B].DR_SET
<span class="org-function-name">str</span>     <span class="org-keyword">lr</span>, [r4, #132]  @ tmp152, MEM[(struct IMXRT_GPIO_t *)1107296256B].DR_SET
<span class="org-comment-delimiter">;</span>
<span class="org-comment-delimiter">; </span><span class="org-comment">.... some other code ....</span>
<span class="org-comment-delimiter">;</span>
<span class="org-comment-delimiter">; </span><span class="org-comment">--- turn off using the CLEAR registers</span>
<span class="org-function-name">str</span>     <span class="org-keyword">r9</span>, [r4, #136]  @ tmp149, MEM[(struct IMXRT_GPIO_t *)1107296256B].DR_CLEAR
<span class="org-function-name">str</span>     <span class="org-keyword">r8</span>, [r4, #136]  @ tmp150, MEM[(struct IMXRT_GPIO_t *)1107296256B].DR_CLEAR
<span class="org-function-name">str</span>     <span class="org-keyword">ip</span>, [r4, #136]  @ tmp151, MEM[(struct IMXRT_GPIO_t *)1107296256B].DR_CLEAR
<span class="org-function-name">str</span>     <span class="org-keyword">lr</span>, [r4, #136]  @ tmp152, MEM[(struct IMXRT_GPIO_t *)1107296256B].DR_CLEAR
</pre>
</div>

<p>
In this code above, <code>r4</code> contains a base address for the list of registers.
The offset <code>[r4, #132]</code> contains the SET register for the pins we care about and <code>[r4, #136]</code> is the CLEAR register.
</p>

<p>
As demonstrated by the scope, we see a small (but predictable) amount of latency on each of these sets because we're running 4 store instructions in quick succession.
But, of course, we can do much better than this by getting away from the arduino apis.
</p>
</div>
</div>
</div>

<div id="outline-container-org4216265" class="outline-2">
<h2 id="org4216265">Port implementation</h2>
<div class="outline-text-2" id="text-org4216265">
<p>
It's easy to implement the appropriate GPIO port code by grabbing bits and pieces from the teensy arduino headers.
</p>

<div class="org-src-container">
<pre class="src src-cpp"><span class="org-type">int</span> <span class="org-function-name">main</span>( <span class="org-type">void</span> )
{
  <span class="org-comment-delimiter">// </span><span class="org-comment">use ardino functions for configuration</span>
  <span class="org-type">uint8_t</span> <span class="org-variable-name">pins</span>[] = { 23, 22, 21, 20 };
  <span class="org-keyword">for</span>( <span class="org-type">size_t</span> <span class="org-variable-name">i</span> = 0; i &lt; ARRAY_SIZE( pins ); ++i ) {
    pinMode( pins[i], OUTPUT );
  }

  <span class="org-comment-delimiter">// </span><span class="org-comment">bit pattern to set/clear bits</span>
  <span class="org-comment-delimiter">// </span><span class="org-comment">use the helpful bit patterns defined by core_pins.h as part of teensy support code</span>
  <span class="org-type">uint32_t</span> <span class="org-variable-name">pattern</span> = CORE_PIN23_BITMASK | CORE_PIN22_BITMASK | CORE_PIN21_BITMASK | CORE_PIN20_BITMASK;

  <span class="org-type">int</span> <span class="org-variable-name">state</span> = 0;
  <span class="org-keyword">while</span>( 1 ) {
    <span class="org-keyword">asm</span> <span class="org-keyword">volatile</span>( <span class="org-string">"@ --- writes happen here"</span> ); <span class="org-comment-delimiter">// </span><span class="org-comment">helpful for navigating generated asm</span>

    <span class="org-keyword">if</span>( state ) {
      GPIO6_DR_SET = pattern; <span class="org-comment-delimiter">// </span><span class="org-comment">turn pins on with SET register</span>
    }
    <span class="org-keyword">else</span> {
      GPIO6_DR_CLEAR = pattern; <span class="org-comment-delimiter">// </span><span class="org-comment">turn pins off with CLEAR register</span>
    }
    <span class="org-keyword">asm</span> <span class="org-keyword">volatile</span>( <span class="org-string">"@ --- done with writes"</span> );

    state = <span class="org-negation-char">!</span>state;
    delayMicroseconds( 10 );
  }
}
</pre>
</div>

<p>
The generated assembly does exactly what we're looking for:
</p>
<div class="org-src-container">
<pre class="src src-asm"><span class="org-comment-delimiter">; </span><span class="org-comment">-- set</span>
<span class="org-function-name">str</span>     <span class="org-keyword">r7</span>, [lr, #132]  @ tmp175, MEM[(struct IMXRT_GPIO_t *)1107296256B].DR_SET

<span class="org-comment-delimiter">; </span><span class="org-comment">-- clear</span>
<span class="org-function-name">str</span>     <span class="org-keyword">r7</span>, [lr, #136]  @ tmp175, MEM[(struct IMXRT_GPIO_t *)1107296256B].DR_CLEAR
</pre>
</div>

<p>
And, the scope shows a much nicer shape for both the rising edge and falling edge:
<img src="../static/teensy-many-pins/gpio_rise.png" alt="gpio_rise.png">
<img src="../static/teensy-many-pins/gpio_fall.png" alt="gpio_fall.png">
</p>
</div>
</div>

<div id="outline-container-orge9b3704" class="outline-2">
<h2 id="orge9b3704">Conclusion</h2>
<div class="outline-text-2" id="text-orge9b3704">
<p>
The Arduino APIs are fantastically useful for getting started quickly, but dropping to lower level APIs can be important.
Fortunately, the Teensy makes it fantastically easy to dig around and use the chip when needed, and, the headers are even documented with where to look in the official manual!
Awesome product.
</p>

<p>
I'm always looking for tools that have both a quick-and-easy beginner API, but don't necessarily sacrifice on depth for advanced use.
So far the Teensy is filling that role well.
</p>
</div>

<div id="outline-container-org1e6409a" class="outline-3">
<h3 id="org1e6409a">Additional Recommended Reading</h3>
<div class="outline-text-3" id="text-org1e6409a">
<ul class="org-ul">
<li>Arduino docs on port manipulation: <a href="https://www.arduino.cc/en/Reference/PortManipulation">https://www.arduino.cc/en/Reference/PortManipulation</a></li>
<li>Manipulating Teensy 3.6 pins without the arduino APIs: <a href="http://l0ner.github.io/2020-08-24_teensy-the-hard-way-blink/">http://l0ner.github.io/2020-08-24_teensy-the-hard-way-blink/</a></li>
<li>Fastest possible pin-flipping rate: <a href="https://metebalci.com/blog/stm32h7-gpio-toggling/">https://metebalci.com/blog/stm32h7-gpio-toggling/</a></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org0f705d9" class="outline-2">
<h2 id="org0f705d9">Appendix 1: Array Implementation</h2>
<div class="outline-text-2" id="text-org0f705d9">
<p>
Suppose we replace the code with something a bit more flexible (and that was recommended in the arduino forums).
</p>

<div class="org-src-container">
<pre class="src src-cpp"><span class="org-preprocessor">#define</span> <span class="org-function-name">ARRAY_SIZE</span>( <span class="org-variable-name">arr</span> ) (<span class="org-keyword">sizeof</span>(arr)/<span class="org-keyword">sizeof</span>(*arr))

<span class="org-keyword">extern</span> <span class="org-string">"C"</span> <span class="org-type">int</span> <span class="org-function-name">main</span>( <span class="org-type">void</span> )
{
  <span class="org-type">uint8_t</span> <span class="org-variable-name">pins</span>[] = { 23, 22, 21, 20 };
  <span class="org-keyword">for</span>( <span class="org-type">size_t</span> <span class="org-variable-name">i</span> = 0; i &lt; ARRAY_SIZE( pins ); ++i ) {
    pinMode( pins[i], OUTPUT );
  }

  <span class="org-type">int</span> <span class="org-variable-name">state</span> = 0;
  <span class="org-keyword">while</span>( 1 ){
    <span class="org-keyword">asm</span> <span class="org-keyword">volatile</span>( <span class="org-string">"@ --- writes happen here"</span> ); <span class="org-comment-delimiter">// </span><span class="org-comment">helpful for navigating generated asm</span>
    <span class="org-keyword">for</span>( <span class="org-type">size_t</span> <span class="org-variable-name">i</span> = 0; i &lt; ARRAY_SIZE( pins ); ++i ) {
      digitalWriteFast( pins[i], state );
    }
    <span class="org-keyword">asm</span> <span class="org-keyword">volatile</span>( <span class="org-string">"@ --- done with writes"</span> );

    state = <span class="org-negation-char">!</span>state;
    delayMicroseconds( 10 );
  }
}
</pre>
</div>

<p>
Something bad clearly happens when we run this code:
</p>


<figure id="org5835940">
<img src="../static/teensy-many-pins/array_rise.png" alt="array_rise.png">

</figure>

<p>
First, we notice that the time delta between writes has increased dramatically.
Second, we notice that the pin 23 and ping 22 write are further apart in time than the pin 22 and pin 21 write.
</p>

<p>
What happened?
</p>

<p>
The inner assignment loop this time compiled into:
</p>

<div class="org-src-container">
<pre class="src src-asm">    @ --- writes happen here
    <span class="org-keyword">add</span> r3, sp, #4      @ tmp202,,
<span class="org-function-name">.L5</span>:
    <span class="org-keyword">ldrb</span>        r2, [r3], #1    @ zero_extendqisi2      @ D.14694, MEM[base: _138, offset: 0B]
    <span class="org-keyword">lsls</span>        r2, r2, #4      @ tmp168, D.14694,
    <span class="org-keyword">adds</span>        r0, r6, r2      @ tmp169, tmp193, tmp168
    <span class="org-keyword">ldr</span> r2, [r6, r2]    @ D.14697,
    <span class="org-keyword">ldr</span> r0, [r0, #12]   @ D.14698,
    <span class="org-keyword">cbz</span> r5, .L3 @ state,
    <span class="org-keyword">str</span> r0, [r2, #132]  @ D.14698, MEM[(volatile uint32_t *)_22 + 132B]
<span class="org-function-name">.L4</span>:
    <span class="org-keyword">cmp</span> r3, r4  @ ivtmp.15, D.14699
    <span class="org-keyword">bne</span> .L5     @,
    @ --- done with writes
</pre>
</div>

<p>
Inspecting the source for <code>digitalWriteFast</code> we can see that we've taken the non-compile-time-constant code path:
</p>

<div class="org-src-container">
<pre class="src src-cpp"><span class="org-keyword">static</span> <span class="org-keyword">inline</span> <span class="org-type">void</span> <span class="org-function-name">digitalWriteFast</span>(<span class="org-type">uint8_t</span> <span class="org-variable-name">pin</span>, <span class="org-type">uint8_t</span> <span class="org-variable-name">val</span>)
{
  <span class="org-keyword">if</span> (__builtin_constant_p(pin)) {
    <span class="org-keyword">if</span> (val) {
      <span class="org-keyword">if</span> (pin == 0) {
        CORE_PIN0_PORTSET = CORE_PIN0_BITMASK;
      } <span class="org-keyword">else</span> <span class="org-keyword">if</span> (pin == 1) {
        <span class="org-comment-delimiter">// </span><span class="org-comment">.....</span>
      }
      <span class="org-comment-delimiter">// </span><span class="org-comment">.....</span>
    } <span class="org-keyword">else</span> {
      <span class="org-keyword">if</span> (pin == 0) {
        CORE_PIN0_PORTCLEAR = CORE_PIN0_BITMASK;
      } <span class="org-keyword">else</span> <span class="org-keyword">if</span> (pin == 1) {
        <span class="org-comment-delimiter">// </span><span class="org-comment">.....</span>
      }
      <span class="org-comment-delimiter">// </span><span class="org-comment">.....</span>
    }
  } <span class="org-keyword">else</span> {
    <span class="org-comment-delimiter">// </span><span class="org-comment">not a compile time constant</span>
    <span class="org-keyword">if</span>(val) *portSetRegister(pin) = digitalPinToBitMask(pin);
    <span class="org-keyword">else</span> *portClearRegister(pin) = digitalPinToBitMask(pin);
  }
</pre>
</div>

<p>
I'm guessing that the variability has something to do with additional memory access (lookups to figure out pin maps to what register).
This probably could have compiled down to use all compile time constants, but this massive change in behavior for roughly the same code is reasonably spooky.
</p>
</div>
</div>
]]>
    </content>
  </entry>
  <entry>
    <title>Badly Computing Polynomial Coefficients from Roots (correctly too!)</title>
    <link rel="alternate" type="text/html" href="http://dpzmick.com/posts/2021-03-28-polynomial-from-roots.html"/>
    <id>urn:uuid:2b91d6a0-0f72-5046-b22b-3ec9e513de6e</id>
    <updated>2021-03-28</updated>
    <content type="html">
      <![CDATA[<style>.org-src-container {white-space: pre; font-family: monospace }</style>
               <p>
Many DSP resources use <a href="https://ccrma.stanford.edu/~jos/filters/Pole_Zero_Analysis_I.html">"pole-zero"</a> plots to compactly represent audio filters.
These plots show where a filter's<a href="https://ccrma.stanford.edu/~jos/filters/Transfer_Function_Analysis.html"> Transfer Function</a> explodes to infinity (poles, <code>x</code> on plot) or goes to zero (zeros, circle on plot):
</p>


<figure id="org98e7dfd">
<img src="../static/poly-roots/pole-zero.png" alt="pole-zero.png" class="light-invert">

<figcaption><span class="figure-number">Figure 1: </span>pole-zero diagram</figcaption>
</figure>

<p>
Unfortunately, I was having a hard time connecting these visualizations with the way a filter "sounds".
There's a number of interactive "filter explorer" tools available, but, for the sake of learning/understanding, I decided to build my own.
</p>

<p>
My filter explorer more or less does one thing: convert a polynomial from one form to another.
This took me more time than it should have; I already knew most of what I needed to know, but had some trouble putting the tools together.
Fortunately, it was fun to figure this out, so here's the whole story, including the missteps.
</p>

<div id="outline-container-org73ac204" class="outline-2">
<h2 id="org73ac204">Symbolic Pole-Zero expansion</h2>
<div class="outline-text-2" id="text-org73ac204">
<p>
A Pole-Zero plot implies a transfer function in this form (\(q_i\) are zeros and all \(p_i\) are poles):
</p>

<p>
\[ H(z) = \frac{ (1-q_1 z^{-1})(1-q_2 z^{-1}) \ldots (1-q_M z^{-M}) }{ (1-p_1 z^{-1})(1-p_2 z^{-2}) \ldots (1-q_N z^{-N}) } \]
</p>

<p>
To actually run the filter (using the WebAudio <code>IIRFilter</code> class, for example), we need to get the function into this form:
</p>

<p>
\[ H(z) = \frac{ B(z) }{ A(z) } = \frac{ b_0 + b_1 z^{-1} + b_2 z^{-2} + \ldots + b_M z^{-M} }{ 1 + a_1 z^{-1} + a_2 z^{-2} + \ldots + z_N z^{-N} } \]
</p>

<p>
Basically, we need to go from \((1-q_1 z^{-1})(1-q_2 z^{-1}) \ldots\) to \(b_0 + b_1 z^{-1} + b_2 z^{-2} + \ldots\).
</p>

<p>
There are many polynomials with the same roots (\(gB(z)\) and \(B(z)\) have same roots), so we'll just pick the polynomial that is easiest to generate (let \(b_0 = 1\)).
This algebra is pretty straightforward by hand or with a computer algebra system.
</p>
</div>

<div id="outline-container-org49b16bf" class="outline-3">
<h3 id="org49b16bf">Closed Form (Wild Goose Chase)</h3>
<div class="outline-text-3" id="text-org49b16bf">
<p>
I correctly didn't want to import an entire Computer Algebra System into my app to do this algebra.
I <b>incorrectly</b> assumed that doing the multiplication without a computer algebra system would be tricky.
In retrospect, I already knew how to easily multiply polynomials (see next section), but my brain-map didn't connect these topics so I went on a wild goose chase.
</p>

<p>
I decided to see if there was a closed form expression for the \(b_i\) coefficient written in terms of the roots \(q_j\).
</p>

<p>
Consider the following:
</p>

<p>
\[ \begin{aligned}
\prod\nolimits_{i=1}^{2} (1-q_i z_{-1}) &= 1                       \\
                                        &- (q_1 + q_2) z^{-1}      \\
                                        &+ (q_1 q_2)   z^{-2}      \\
\prod\nolimits_{i=1}^{3} (1-q_i z_{-1}) &= 1                                    \\
                                        &- (q_1 + q_2 + q_3)             z^{-1} \\
                                        &+ (q_1 q_2 + q_1 q_3 + q_2 q_3) z^{-2} \\
                                        &- (q_1 q_2 q_3)                 z^{-3} \\
\prod\nolimits_{i=1}^{4} (1-q_i z_{-1}) &= 1                                                                  \\
                                        &- (q_1 + q_2 + q_3 + q_4)                                     z^{-1} \\
                                        &+ (q_1 q_2 + q_1 q_3 + q_1 q_4 + q_2 q_3 + q_2 q_4 + q_3 q_4) z^{-2} \\
                                        &- (q_1 q_2 q_3 + q_1 q_2 q_4 + q_1 q_3 q_4 + q_2 q_3 q_4)     z^{-3} \\
                                        &+ (q_1 q_2 q_3 q_4)                                           z^{-4}
\end{aligned} \]
</p>

<p>
It looks like there might be straightforward closed form for any of these coefficients (pardon the awkward notation):
</p>

<p>
\[ \begin{align}
b_0 &= 1 \\
b_i &= (-1)^i \sum_{ Q \in \text{combs}(i) } \bigg[ \prod_{ j \in Q } q_j \bigg]
\end{align} \]
</p>

<p>
Where \(\text{combs}(i): \mathbb{Z} \mapsto \mathbb{Z}^i\) is the set of all length-\(i\) combinations of coefficients.
</p>

<p>
Here's a possibly even harder to understand version in Julia/SymPy:
</p>

<div class="org-src-container">
<pre class="src src-julia">z = symbols(<span class="org-string">"z"</span>)
<span class="org-keyword">function</span> <span class="org-function-name">build_expr</span>(n_roots)
    qs = symbols(<span class="org-string">"q1:$(n_roots+1)"</span>) <span class="org-comment-delimiter"># </span><span class="org-comment">generates q1,q2,...</span>

    poly = 1
    <span class="org-keyword">for</span> i <span class="org-keyword">in</span> 1:n_roots
        prods = [reduce(*, c) <span class="org-keyword">for</span> c <span class="org-keyword">in</span> combinations(qs, i)]
        poly += ((-1)^i * reduce(+, prods))/z^i
    <span class="org-keyword">end</span>
    <span class="org-keyword">return</span> poly
<span class="org-keyword">end</span>
</pre>
</div>

<p>
Again, using SymPy, it's trivial to test if the code works for a given number of roots:
</p>

<div class="org-src-container">
<pre class="src src-julia"><span class="org-keyword">function</span> <span class="org-function-name">test_expr</span>(n_roots)
    mine = build_expr(n)

    <span class="org-comment-delimiter"># </span><span class="org-comment">first generate the actual solution</span>
    qs = symbols(<span class="org-string">"q1:$(n_roots+1)"</span>) <span class="org-comment-delimiter"># </span><span class="org-comment">generates q1,q2,...</span>
    actual = reduce(*, [(1-qs[i]/z) <span class="org-keyword">for</span> i <span class="org-keyword">in</span> 1:n])

    <span class="org-comment-delimiter"># </span><span class="org-comment">if these exprs cancel out, then the exprs are equivalent</span>
    <span class="org-keyword">return</span> (mine - actual).expand() == 0
<span class="org-keyword">end</span>

test_expr(1) <span class="org-comment-delimiter"># </span><span class="org-comment">true</span>
test_expr(2) <span class="org-comment-delimiter"># </span><span class="org-comment">true</span>
test_expr(3) <span class="org-comment-delimiter"># </span><span class="org-comment">true</span>
test_expr(4) <span class="org-comment-delimiter"># </span><span class="org-comment">true</span>
test_expr(5) <span class="org-comment-delimiter"># </span><span class="org-comment">true... must be correct!</span>
</pre>
</div>

<p>
This result is kind of cool, but not particularly easy to compute.
I'm also not quite sure how to prove correctness.
Filter-function-form conversion seems like it should be pretty common, so I figured if I google around using terms like "polynomial coefficients" and "combinations," I'd find some references to this method right away.
</p>

<p>
Wrong!
</p>

<p>
A few things I did find are:
</p>
<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Pascal%27s_triangle">Pascal's Triangle</a> (similar structure, solving a different problem).</li>
<li>Polynomial interpolation techniques (fitting a polynomial through the zeros).</li>
<li>A stack overflow post referencing <a href="https://en.wikipedia.org/wiki/Vieta%27s_formulas">Vieta's formulas</a> (has the same structure)</li>
<li><a href="https://en.wikipedia.org/wiki/Characteristic_polynomial">Characteristic Polynomial of a Matrix</a>. Slightly different problem, but has a similar form.</li>
<li><a href="https://en.wikipedia.org/wiki/Elementary_symmetric_polynomial">Elementary Symmetric Polynomials</a>. Wikipedia page that ties all of the above together</li>
</ul>

<p>
Alright, I'm reasonably convinced that this method is <span class="underline">correct</span>, I just don't quite have the abstract math tools to reason whatever mathematical object I'm manipulating.
</p>

<p>
What I couldn't find was any references to using this sum-of-products-of-combinations approach to find <span class="underline">filter coefficients</span>.
If I'm not finding references to this method it must not be a common technique.
</p>

<p>
I decided to go back to the JOS book and look for inspiration again.
The very lucky/very ADD story:
</p>
<ol class="org-ol">
<li>Read the book (<a href="https://ccrma.stanford.edu/~jos/fp/Partial_Fraction_Expansion.html">Chapter on Partial Fraction Expansion</a>)</li>
<li>Realize that the matlab <code>residuez</code> function is doing "filter function form" manipulation, kind of.</li>
<li>Go to matlab docs; Click through all of the additional "see more" functions referenced in the matlab help for <code>residuez</code>.</li>
<li>Find the obviously-named <code>tf2zpk</code> function, which does the opposite of what I want.</li>
<li>If <code>tf2zpk</code> exists, maybe <code>zpk2tf</code> also exists? Google that.</li>
<li>It does. In SciPy. Read SciPy docs for <code>zpk2tf</code>. The function: "Return[s] polynomial transfer function representation from zeros and poles"</li>
</ol>

<p>
Ah ha!
<code>zpk2tf</code> is <b>exactly</b> what I've been looking for.
</p>

<p>
Next question, what does <code>zpk2tf</code> do?
</p>
</div>
</div>
</div>

<div id="outline-container-org62b985d" class="outline-2">
<h2 id="org62b985d">The Correct Way</h2>
<div class="outline-text-2" id="text-org62b985d">
<p>
I grab the scipy source and start reading.
The function <code>zpk2tf</code> essentially just calls a numpy function <code>poly</code> to compute the coefficients of the polynomials \(A(z)\) and \(B(z)\) from their respective roots.
Aside: the docs for `np.poly` reference characteristic polynomials, so there some relationship here!
</p>

<p>
<code>np.poly</code> is very simple: it just does some convolutions.
Polynomial multiplication is just convolution of the polynomial coefficients (something I already knew, but didn't connect to this problem).
For example, the polynomial \(1 + 2x + 3x^2\) can be represented as the list <code>[1, 2, 3]</code>.
Then, to multiply \((1+2x+3x^2)(1-2x)\), we'd just need to <code>conv( [1,2,3], [1,-2] )</code>.
This produces the expected result <code>[1, 0, -1, -6]</code>, or \(1 - x^2 - 6x^3\)
</p>

<p>
So, in psuedo-python, the entire roots-to-coefficients transformation boils down to:
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">poly</span> = [1.0]                 <span class="org-comment-delimiter"># </span><span class="org-comment">start with the polynomal "1"</span>
<span class="org-keyword">for</span> root <span class="org-keyword">in</span> <span class="org-variable-name">roots</span>:
    term = [1.0, -root]      <span class="org-comment-delimiter"># </span><span class="org-comment">the term 1 - (q_i)x</span>
    <span class="org-variable-name">poly</span> = conv(poly, term)  <span class="org-comment-delimiter"># </span><span class="org-comment">multiply in new term for this root</span>
</pre>
</div>

<p>
In other words, we can just repeatedly multiply each \((1-q_i z^{-1})\) term into a final polynomial using a speedy convolution.
This is obviously much simpler than the nonsense above, so this is the method that my <a href="../static/filter-playground/">filter explorer</a> uses.
</p>
</div>
</div>
]]>
    </content>
  </entry>
  <entry>
    <title>Audio/Visual Latency Tests</title>
    <link rel="alternate" type="text/html" href="http://dpzmick.com/posts/2020-09-21-headset-latency.html"/>
    <id>urn:uuid:a9db5639-b4cb-5916-972a-d42e4811f881</id>
    <updated>2020-09-12</updated>
    <content type="html">
      <![CDATA[<style>.org-src-container {white-space: pre; font-family: monospace }</style>
               <p>
I've recently been toying around with different headset setups for zoom/casual online gaming (with voice) with friends.
</p>

<p>
As part of this experiment, I decided to try out a pair of true wireless bluetooth earbuds, but was astonished by how terrible the audio latency seemed to be.
</p>

<p>
In a (failed) attempt to measure the latency, I wrote these two little web "apps":
</p>

<ol class="org-ol">
<li><a href="../static/latency-tests/latency-audio-video.html">Proof of bad audio latency</a> - a canvas that flashes red the same time that a sound is played.</li>
<li><a href="../static/latency-tests/latency-reaction-time.html">Attempt at quantifying the latency</a> - a canvas flashes red, or a sound is played. User hits a button. Response times are recorded.</li>
</ol>

<p>
Note: not sure if either of them work on iOS. Something seems to be broken, but debugging on iOS apparently requires owning a macbook.
</p>

<p>
With (2), I was hoping that my response time variance would be low enough to effectively measure the bluetooth latency by comparing my audio and visual response times.
This doesn't seem to be the case, my response times are all over the place.
</p>

<p>
Likely I'll keep toying with how to measure this, maybe some fancy apparatus that records the audio my headphones hear?
</p>
]]>
    </content>
  </entry>
  <entry>
    <title>AMD Zen Memory Latency/Bandwidth Considerations</title>
    <link rel="alternate" type="text/html" href="http://dpzmick.com/posts/2020-05-10-ryzen-memory.html"/>
    <id>urn:uuid:1d578363-90dd-55a7-804f-8a41fa53845b</id>
    <updated>2020-05-10</updated>
    <content type="html">
      <![CDATA[<style>.org-src-container {white-space: pre; font-family: monospace }</style>
               <p>
I've built a micro-benchmark which sends data between CPU 2 cpu cores.
On my single AMD 3800X desktop CPU, I have crafted a microbenchmark which has different performance characteristics depending on pair of cores I select for testing.
</p>

<p>
Results like this should not be surprising on a multi-socket system, but this system has only a single processor.
I believe this micro-benchmark is highlighting some of the features of the novel architecture on the new AMD chips.
</p>

<div id="outline-container-org0bf78e1" class="outline-2">
<h2 id="org0bf78e1">Caches</h2>
<div class="outline-text-2" id="text-org0bf78e1">
<p>
Before diving in to Zen, I'd like to talk briefly CPU memory-caching systems.
</p>

<p>
Modern CPUs contain a variety of fast memory which cache accesses to the larger more expensive RAM.
Usually we call the fastest, smallest cache L1, the next fastest L2, and the largest, but slowest cache L3.
These caches communicate with each other to move data around (see <a href="https://en.wikipedia.org/wiki/MESI_protocol">MESI protocol</a>).
</p>

<p>
Assume for simplification that the L3 cache knows how to get data from main memory or from the L1 and L2 caches.
Also assume that the L3 cache is the only part of the memory system which is able to talk main memory.
</p>

<p>
If Core 0 wants to read from memory, it will ask the local L1 cache to perform the read.
If the data is in the L1 cache, the cache sends the data to the CPU registers.
If the data is not in the L1 cache, the cache asks L2.
Finally, the L2 cache asks L3 and the L3 cache goes and fetches the data.
</p>

<p>
Next, assume that all data is written into the L1 cache then propagated out to other caches or main memory as needed.
When the L1 cache is out of space, it will evict the data into the L2 cache.
When the L2 cache is out of space, it will evict the data into the L3 cache.
Finally, if L3 runs out of space, it will evict the data back to main memory.
</p>

<p>
Generally, each CPU has it's own L1 and L2 cache, but the L3 cache is shared by all of the cores.
Something like this:
</p>

<div class="org-src-container">
<pre class="src src-ditaa">/-----------------------------------\
|                                   |
|   L3 cache (shared by all cores)  |
|                                   |
+--------+--------+--------+--------+
|        |        |        |        |
| L{1,2} | L{1,2} | L{1,2} | L{1,2} |
|        |        |        |        |
+--------+--------+--------+--------+
|        |        |        |        |
|  Core  |  Core  |  Core  |  Core  |
|        |        |        |        |
\--------+--------+--------+--------/
</pre>
</div>

<p>
This style of L1/L2/L3 cache has been common in CPU designs for many years.
</p>
</div>
</div>

<div id="outline-container-org1e541ee" class="outline-2">
<h2 id="org1e541ee">Ryzen Architecture</h2>
<div class="outline-text-2" id="text-org1e541ee">
<p>
To talk about AMD's Zen architecture, there's a few terms we need to define:
</p>
<ul class="org-ul">
<li>CCX: Core Complex. Contains a max of 4 cores and a slice of the L3 cache.</li>
<li>CCD: Core Chiplet Die: A single die which contains 2 CCX (on Ryzen 3/5/7)</li>
<li>IO controller: A separate die containing DDR controllers, PCIe controllers. Also the hub for the "Infinity Fabric"</li>
<li>Infinity Fabric: High performance interconnect connecting CCXs</li>
</ul>

<p>
Each Ryzen processor has one IO controller and some number of CCD dies.
This CCD/CCX/IO controller strategy allows AMD to mass produce CCD chips, performance/correctness test the cores (this is called "binning"), then build a wide variety of different processor configurations.
</p>

<p>
The particular Ryzen 3800X I own has a single IO controller and a single CCD.
The CCD contains two CCXs.
Each CCX in my particular CPU has all 4 physical cores enabled.
Each of those physical cores is capable of running two threads (<a href="https://en.wikipedia.org/wiki/Simultaneous_multithreading">see Simultaneous Multithreading (SMT)</a>).
</p>

<p>
Here's how this looks in <a href="https://www.open-mpi.org/projects/hwloc/">hwloc</a>'s <code>lstopo</code> output:
<img src="../static/ryzen_numa/topo.png" alt="topo.png">
</p>

<p>
Interpreting the output, we can see that:
</p>
<ul class="org-ul">
<li>CCX1 contains the cores that the OS calls 0, 1, 2, 3, 4, 5, 6, and 7</li>
<li>CCX2 contains the cores that the OS calls 8, 9, 10, 11, 12, 13, 14, and 15</li>
</ul>

<p>
This likely means that cores in the same CCX can communicate with each other by reading and writing to their shared L3 cache.
If cores from different CCXs need to communicate, they will have to use the infinity interconnect.
</p>
</div>
</div>

<div id="outline-container-org9e91631" class="outline-2">
<h2 id="org9e91631">Latency Test</h2>
<div class="outline-text-2" id="text-org9e91631">
<p>
To verify my hypothesis, lets start with a classic pingpong roundtrip latency test.
</p>

<p>
This test runs two threads.
The first thread flips a value in memory to some value "Ping", then waits to see another value "Pong."
The second thread waits to see "Ping", then flips the value to "Pong."
</p>

<p>
The first thread starts timing right before sending "Ping", and stops timing once it sees "Pong."
</p>

<p>
I've plotted the mean core-to-core pingpong round-trip time for all pairs of cores (including the SMT cores).
Latency for A-&gt;B and B-&gt;A were both tested (for no particular reason).
</p>

<p>
That is:
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">for</span> core1 <span class="org-keyword">in</span> cores:
    <span class="org-keyword">for</span> core2 <span class="org-keyword">in</span> cores:
        <span class="org-keyword">if</span> core1 == core2: <span class="org-keyword">continue</span> <span class="org-comment-delimiter"># </span><span class="org-comment">skip self, both threads spin</span>
        results[ (core1, core2) ] = mean_of_many_tests( core1, core2 )
</pre>
</div>

<p>
This plot is a heatmap of core-&gt;core pingpong roundtrip time, in nanoseconds:
<img src="../static/ryzen_numa/pingpong.png" alt="pingpong.png">
</p>

<p>
As we can see, there's a clear difference in round-trip latency between pairs of cores in the same CCX and pairs of cores in a different CCX.
For example, core 0-&gt;core 3 round-trip latency is pinkish, but core 0-&gt;core 4 latency is dark blueish.
The actual values I got around ~96ns for 0-&gt;3 and ~196ns for 0-&gt;4.
</p>

<p>
I do not have a good explanation for the upper and lower diagonal lines (at around 48ish ns), but, we can clearly see that pairs of cores for which latency was low share a common L3 cache (they are in the same CCX).
</p>

<p>
For full details about this benchmark, please see the source and my results <a href="https://gitlab.com/dpzmick/ryzen-numa/-/tree/master/latency">here</a>.
</p>

<p>
So, we know that there is a difference in latencies, but, does this really matter?
Few applications actually have any reason to bounce a single value back and forth between cores.
</p>
</div>
</div>

<div id="outline-container-orgdfd7627" class="outline-2">
<h2 id="orgdfd7627">Bandwidth Test</h2>
<div class="outline-text-2" id="text-orgdfd7627">
<p>
When discussing latency, programmers who are more bandwidth focused will often (rightly) say something like "the latency doesn't matter, I only care about bandwidth."
Most programmers are more focused on bandwidth than latency, because generally we care a lot more about throughput than latency.
</p>
</div>

<div id="outline-container-orgf2f8119" class="outline-3">
<h3 id="orgf2f8119">Baseline with large core-to-core copies</h3>
<div class="outline-text-3" id="text-orgf2f8119">
<p>
Let's start by proving that, when moving around large chunks of data, none of this matters.
</p>

<p>
I've created a small tester which:
</p>
<ul class="org-ul">
<li>binds to a core</li>
<li>allocates memory on the core</li>
<li>writes to every byte of the memory on the core</li>
<li>binds to another core</li>
<li>copies all of the memory into a new region of memory</li>
</ul>

<p>
The region of memory that I am copying is 4 gigs large.
This is much larger than any of the local caches.
We should be able fire off a <code>memcpy</code> then sit back while the hardware prefetchers and cache hierarchy work their latency-hiding magic.
</p>

<p>
Here's the plot:
<img src="../static/ryzen_numa/bw_baseline.png" alt="bw_baseline.png">
</p>

<p>
As we can see, we're pretty much getting a ~14.5-15 GiB/s copy rate regardless of the cores selected.
This is good news.
The new AMD core layout makes no difference when copying around 4 gig chunks of data.
</p>

<p>
For full details about this benchmark, please see the source and my results <a href="https://gitlab.com/dpzmick/ryzen-numa/-/tree/master/bw_baseline">here</a>.
</p>
</div>
</div>

<div id="outline-container-org4bf452f" class="outline-3">
<h3 id="org4bf452f">When might it matter?</h3>
<div class="outline-text-3" id="text-org4bf452f">
<p>
We are often able to amortize high latency operations over large transfers or by work on something else to hide the costs of high latency.
However, as data sizes shrink, it becomes harder and harder to hide latency.
</p>

<p>
Recently, it seems like (anecdotally) there has been a trend of using queues to move data/send commands in throughput-oriented applications.
</p>

<ul class="org-ul">
<li>The <a href="https://jackaudio.org/">Jack</a> audio system uses ring buffers to transfer audio samples between multiple communicating applications</li>
<li>A number of async IO libraries use thread pools, event queues, and work stealing scheduling</li>
<li>Go's channels are fundamentally just queues between concurrent (sometimes running in parallel) threads</li>
<li>Erlang's fundamental abstraction is threads that communicate over queues</li>
<li>etc..</li>
</ul>

<p>
When messages sent over these queues are small, we might be able to observe Ryzen's latency characteristics as a drop in throughput.
</p>

<p>
I've built a simple microbenchmark to test this.
The benchmark runs a writer thread and some number of reader threads.
The threads share a region of memory that contains a bunch of "chunks".
The array of chunks looks sort of a like a single producer, single consumer queue:
</p>
<div class="org-src-container">
<pre class="src src-c"><span class="org-keyword">typedef</span> <span class="org-keyword">struct</span> {
  <span class="org-type">uint64_t</span> <span class="org-variable-name">ready</span>;          
  <span class="org-type">char</span>     <span class="org-variable-name">padding</span>[ CACHE_LINE_SIZE - <span class="org-keyword">sizeof</span>( <span class="org-type">uint64_t</span> ) ];
  <span class="org-type">char</span>     <span class="org-variable-name">data</span>[ N_DATA_LINES*CACHE_LINE_SIZE ];
} <span class="org-type">chunk_t</span>;
</pre>
</div>

<p>
The <code>ready</code> marker acts like a boolean.
The <code>ready</code> marker always sits at the start of a cache line.
The size <code>data</code> of the data region is always some multiple of the cache line size.
Padding is introduced to ensure that the data region and the <code>ready</code> marker are not ever in the same cache line.
</p>

<p>
In pseudocode, the writer looks something like this:
</p>
<div class="org-src-container">
<pre class="src src-c"><span class="org-keyword">for</span>( <span class="org-type">size_t</span> <span class="org-variable-name">i</span> = 0; i &lt; N_CHUNKS; ++i ) {
  populate_chunk_data( &amp;chunks[i] );
  <span class="org-comment-delimiter">// </span><span class="org-comment">signal that the reader should read the data</span>
  atomic_store( &amp;chunks[i].ready, <span class="org-constant">true</span> );
}
</pre>
</div>

<p>
The reader looks something like this:
</p>
<div class="org-src-container">
<pre class="src src-c"><span class="org-keyword">for</span>( <span class="org-type">size_t</span> <span class="org-variable-name">i</span> = 0; i &lt; N_CHUNKS; ++i ) {
  <span class="org-comment-delimiter">// </span><span class="org-comment">Wait for the chunk to be ready</span>
  <span class="org-keyword">while</span>( <span class="org-negation-char">!</span>atomic_load( &amp;chunks[i].ready ) ) { }
  read_and_discard_data( &amp;chunks[i] );
}
</pre>
</div>

<p>
In theory, chunk transfer will occur one at a time when the reader and the writer are running at more or less the same speed.
If the reader falls behind the writer, the writer should be able to plow ahead with its writes and the reader should be able to continue reading until it catches up or finishes.
</p>

<p>
In the benchmark, I've placed the array of chunks into a 2 megabyte <a href="https://kerneltalks.com/services/what-is-huge-pages-in-linux/">huge page</a>.
I created a region containing 8192 chunks, and each chunk's data segment contains 8 cache lines (512 bytes) of data.
The number of chunks doesn't seem to affect the benchmark much, but the size of the data segment does.
I've kept the size low to highlight the effects I'm looking for, but, these effects seem to still be present for data sizes up to a few KiB.
</p>
</div>
</div>

<div id="outline-container-orgb1c1209" class="outline-3">
<h3 id="orgb1c1209">Results</h3>
<div class="outline-text-3" id="text-orgb1c1209">
<p>
Using the same style of plots that we've already see, let's first look at bandwidth reported by the writer.
This plot shows the write rate when Y-axis core is sending data to X-axis core (rate is in GiB/s, I've again included the SMT cores):
</p>


<figure id="org2b45e1c">
<img src="../static/ryzen_numa/writerplot.png" alt="writerplot.png">

</figure>

<ul class="org-ul">
<li>When core 0 (CCX1) writes to core 2 (CCX2), writer rate is ~14.2 GiB/s</li>
<li>When core 0 (CCX1) writes to core 4 (CCX2), writer rate is ~13.9 GiB/s</li>
<li>When core 0 (CCX1) writes to core 5 (CCX2), writer rate is ~14.0 GiB/s</li>
</ul>

<p>
There's some patterns in this plot that I have not been able to explain, but, there doesn't seem to be any significant variation in write rate when the reader core is on a different CCX.
I'm reasonably confident claiming that the writer is unaffected by choice of core.
</p>

<p>
Here's the rates that the reader reported:
<img src="../static/ryzen_numa/readerplot.png" alt="readerplot.png">
</p>

<p>
From the reader's perspective, the differences are stark.
When the reader and writer share an L3 cache (are in the same CCX), we consistently see a read rate around 13-14 GiB/s.
When the reader and writer do not share the L3 cache (are in different CCX), the read rates are consistently lower, around 10-11 GiB/s.
</p>

<p>
This is a significant enough difference to warrant careful attention when building high performance software.
</p>

<p>
For full details about this benchmark, please see the source and my results <a href="https://gitlab.com/dpzmick/ryzen-numa/-/tree/master/q">here</a>.
</p>
</div>
</div>
</div>

<div id="outline-container-org0f4901e" class="outline-2">
<h2 id="org0f4901e">Conclusions</h2>
<div class="outline-text-2" id="text-org0f4901e">
<p>
This sort of behavior will not come as a surprised for programmers used to programming for multi-socket (multiple physical CPU) server systems (see <a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">NUMA</a>).
However, for those writing parallel applications, I feel like this effect is still notable and warrants slightly different considerations than traditional NUMA.
</p>

<p>
In a traditional multi-socket/multiple-physical CPU system, RAM is physically connected to only one of the many CPUS.
If core0 of cpu0 needs to access memory physically attached to cpu1, it will pay a latency penalty to access this memory (this is NUMA - Non-Uniform Memory Access).
However, we've also generally assumed that core0 can access any part of memory attached to cpu0 with the same latency.
Therefore, applications built for NUMA architectures often focused on getting their data into the memory CPU attached to a CPU, then using the cores on that CPU to process the data.
</p>

<p>
For platforms like AMD's (and recent Intel, see <a href="https://itpeernetwork.intel.com/intel-mesh-architecture-data-center/">Intel Mesh</a>) we have to now make NUMA-style considerations any time we are operating over a piece of data on multiple cores, not just when dealing with multiple sockets.
</p>

<p>
Consider the specs for the EPYC 7542:
</p>
<ul class="org-ul">
<li>32 cores</li>
<li>512 KiB L2 d-cache per core</li>
<li>128 MiB L3 cache for the entire chip</li>
</ul>

<p>
On the 7542, we have to divide this 128 MiB of L3 cache up into CCX cache slices.
Assuming each CCX has 4 cores (which I think is correct), we'd have 32/4=8 CCXs on this CPU.
Therefore, each CCX (4 cores) only has 16 MiB of "close L3 cache memory," accessing the rest of the cache will incur a small amount of extra latency.
</p>

<p>
For comparison, a 2016 Intel Broadwell Xeon could be configured with 22 cores and 55 MiB of L3 cache.
This means that the AMD has a core/fast-shared-l3 ratio of 0.25 MiB/core and the Broadwell has a core/fast-shared-l3 ratio of 2.5 MiB/core.
Building high performance code with 2.5 MiB of super fast shared memory per core is a slightly different game than building high performance applications with only 0.25 MiB of super fast shared memory per core.
</p>

<p>
I suspect that "networked" cpu architectures will become the new normal in the next few years.
These new platforms are going probably going to be increasingly more and more sensitive to data access and data movement than the systems of the past.
</p>
</div>
</div>
]]>
    </content>
  </entry>
  <entry>
    <title>Homelab Act 4: Services and Applications</title>
    <link rel="alternate" type="text/html" href="http://dpzmick.com/posts/2020-02-01-homelab4-cloud.html"/>
    <id>urn:uuid:fb3b3fe7-08b3-527e-b3a4-56140e22d51c</id>
    <updated>2020-02-23</updated>
    <content type="html">
      <![CDATA[<style>.org-src-container {white-space: pre; font-family: monospace }</style>
               <p>
For the last 192 days (according to uptime on one my routers), I've been setting up a small homelab.
Homelabs are kind of cool, and, the setup has been <i>interesting</i>.
I'll be writing a few posts explaining the steps I took.
</p>

<ul class="org-ul">
<li>Part 1 <a href="./2020-01-09-vpn.html">VPN setup</a></li>
<li>Part 2 <a href="./2020-01-11-server-network.html">Servers and 10GbE</a></li>
<li>Part 3 <a href="./2020-01-12-nas.html">NAS, ZFS, and NFS</a></li>
<li>Part 4 (this post)</li>
</ul>

<p>
Now that I have a VPN, a server, a "desktop computer" connected to the server over a fast network, and fast storage, I should do something useful.
</p>

<div id="outline-container-org745b1a5" class="outline-2">
<h2 id="org745b1a5">Administration Tools</h2>
<div class="outline-text-2" id="text-org745b1a5">
</div>
<div id="outline-container-org8201ee1" class="outline-3">
<h3 id="org8201ee1"><a href="https://www.influxdata.com/">InfluxDB</a>, <a href="https://grafana.com/">Grafana</a>, and <a href="https://www.influxdata.com/time-series-platform/telegraf/">Telegraf</a></h3>
<div class="outline-text-3" id="text-org8201ee1">
<p>
These tools are useful for general system monitoring.
</p>

<p>
Also, grafana plots are really pretty:
<img src="../static/homelab/grafana_temps.png" alt="grafana_temps.png">
</p>

<p>
I'm also a fan of using these tools to report your own metrics from your own small applications.
You can very easily write stats to output files then jam them through telegraf to collect them and produce plots.
See <a href="https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md">https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md</a>.
</p>
</div>
</div>

<div id="outline-container-org502154a" class="outline-3">
<h3 id="org502154a"><a href="https://healthchecks.io">healthchecks.io</a> for alerting</h3>
<div class="outline-text-3" id="text-org502154a">
<p>
<a href="https://healthchecks.io">healthchecks.io</a> is an altering service that sends alerts through email, slack, etc when it hasn't received a ping recently enough.
You can run the open-source software yourself, or use a hosted version.
</p>

<p>
To set it up, you register a number of "checks" and assign each check a schedule:
</p>

<p>
Sending updates is also trivial; I just use use <code>curl</code> to send "stating", "failed," and "succeeded" messages from a shell script:
</p>
<div class="org-src-container">
<pre class="src src-bash"><span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/</span><span class="org-keyword">bash</span>

<span class="org-builtin">set</span> -o pipefail
<span class="org-comment-delimiter"># </span><span class="org-comment">don't set -e because we want failure to make it all the way to the end, not</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">sure that's actually happening here</span>

<span class="org-comment-delimiter"># </span><span class="org-comment">retrying curl is questionable</span>

<span class="org-keyword">if</span> [ <span class="org-string">"$#"</span> -lt 2 ]; <span class="org-keyword">then</span>
    <span class="org-builtin">echo</span> <span class="org-string">"Usage: URL cmd..."</span>
    <span class="org-keyword">exit</span> 1
<span class="org-keyword">fi</span>

<span class="org-variable-name">url</span>=$<span class="org-variable-name">1</span>

<span class="org-builtin">echo</span> <span class="org-string">"Sending start message"</span>
curl --silent -fsS --retry 3 -X GET ${<span class="org-variable-name">url</span>}/start
<span class="org-builtin">echo</span>

<span class="org-comment-delimiter"># </span><span class="org-comment">run the rest of the arguments as a command</span>
${<span class="org-variable-name">@</span>:2}

<span class="org-variable-name">rc</span>=$<span class="org-variable-name">?</span>
<span class="org-keyword">if</span> [ ${<span class="org-variable-name">rc</span>} -eq 0 ]; <span class="org-keyword">then</span>
    <span class="org-builtin">echo</span> <span class="org-string">"Success"</span>
    curl --silent -fsS --retry 3 -X GET ${<span class="org-variable-name">url</span>} <span class="org-comment-delimiter"># </span><span class="org-comment">done!</span>
    <span class="org-builtin">echo</span>
<span class="org-keyword">else</span>
    <span class="org-builtin">echo</span> <span class="org-string">"Command ${@:2} failed"</span>
    <span class="org-builtin">echo</span> <span class="org-string">"Sending fail message"</span>
    curl --silent -fsS --retry 3 -X GET ${<span class="org-variable-name">url</span>}/fail
    <span class="org-builtin">echo</span>
<span class="org-keyword">fi</span>
<span class="org-keyword">exit</span> ${<span class="org-variable-name">rc</span>}
</pre>
</div>

<p>
Sending start/{success,fail} messages is helpful because:
</p>
<ol class="org-ol">
<li>If you send a start message but don't send a finish soon enough, the service goes down.</li>
<li>The healthchecks web displays the end-start runtime for each successful task in the web ui.</li>
</ol>
</div>
</div>

<div id="outline-container-orgee6951a" class="outline-3">
<h3 id="orgee6951a"><a href="https://duplicacy.com/">Duplicacy</a> and <a href="https://www.backblaze.com/b2/cloud-storage.html">Backblaze B2</a> for backups</h3>
<div class="outline-text-3" id="text-orgee6951a">
<p>
Backups are tricky.
</p>

<p>
My requirements:
</p>
<ul class="org-ul">
<li>Easy to restore data after a complete hardware failure
<ul class="org-ul">
<li>Can't depend on complicated local config files</li>
<li>Ideally doesn't depend on a large software stack either, downloads straight from a web UI are preferred</li>
<li>I don't need a carefully crafted mobile app</li>
</ul></li>
<li>Incremental snapshots</li>
<li>Cloud Storage supported easily (AWS, Backblaze, etc)
<ul class="org-ul">
<li>Can back up the same volume to many services</li>
</ul></li>
<li>Understandable, easy to debug when things go wrong</li>
<li>Very fast backups and very fast restores</li>
<li>If I'm paying for storage size, deduplication and some form of compression</li>
</ul>

<p>
For many years I've used and recommended <a href="https://www.crashplan.com/en-us/">CrashPlan</a>.
I still would recommend CrashPlan; their service satisfies almost all of the above requirements.
</p>

<p>
On a recent backup test though, I found their recovery to be somewhat slow.
I decided to see what else is available.
</p>

<p>
After assessing many of the open-source options, I settled on <a href="https://duplicacy.com/">duplicacy</a> and <a href="https://www.backblaze.com/b2/cloud-storage.html">Backblaze B2</a> for a first experiment.
Duplicacy satisfies many of thes objectives:
</p>
<ul class="org-ul">
<li>Easy(ish) to restore
<ul class="org-ul">
<li>No web app (need to stich files back together)</li>
<li>Rebuilding config files without access to the originals does not seem to be too difficult though</li>
</ul></li>
<li>Incremental snapshots</li>
<li>Supports all of the cloud storage</li>
<li>A bit tricky to understand, but extremely well documented
<ul class="org-ul">
<li>The <a href="https://forum.duplicacy.com/t/lock-free-deduplication-algorithm/1093">deduplication algorithm</a> is well thought out and well explained</li>
</ul></li>
<li>Quite fast to upload and download
<ul class="org-ul">
<li>Incremetal snapshots scan local files using mtime (pretty fast) or a hash (kinda slow) to determine what needs to be reuploaded</li>
<li>Upload/Download speed seems to be mostly a function of the storage provider and local network. With enough threads, I was able to get fairly close to my ISP bandwidth limit (500 Mbps) during a test restore</li>
</ul></li>
</ul>

<p>
So far, this experiment has gone well.
Each backup set I have configured backs up on a timer using systemd and pings healthchecks with its status.
A few of the snapshots from my laptop were corrupted I believe because I closed my laptop lid at a bad time, but, my <code>duplicacy check</code> job (which also reports to healthchecks), quickly noticed the issue and I was alerted.
</p>

<p>
Setting this up obviously takes more work than just downloading CrashPlan, but, so far I've been pretty happy with how this experiement has gone so I will be continuing it for now.
</p>
</div>
</div>

<div id="outline-container-org9d3ae84" class="outline-3">
<h3 id="org9d3ae84">Logs</h3>
<div class="outline-text-3" id="text-org9d3ae84">
<p>
Nothing.
</p>

<p>
I've wanted to alert on a few things from log files, so I've just thrown a <code>grep BAD_THING log_file</code> into a script, checked the return code, and alerted to healthchecks when the bad thing happened.
I don't need log searching/aggregation features for 3 computers.
</p>
</div>
</div>
</div>

<div id="outline-container-orgd7d4d7f" class="outline-2">
<h2 id="orgd7d4d7f">Developer Tools</h2>
<div class="outline-text-2" id="text-orgd7d4d7f">
<p>
The personal software development projects I work on can also take advantage of this server/vpn setup.
I'm able to ssh into both of my home machines from anywhere (because of the VPN), so I always have access to the larger CPUs available there (very handy when compiling rust/c++ code).
</p>

<p>
I'm also using the server for:
</p>
<ul class="org-ul">
<li>SQL server (I like SQL, usually I just throw stuff into SQlite but MariaDB is cool too and I don't have to remember which computer the file was left on)</li>
<li>A persistent Jupyter notebook server with julia and python available + tons of packages.
<ul class="org-ul">
<li>This is nice from my laptop, the server is a lot beefier than my laptop.</li>
</ul></li>
<li>git server
<ul class="org-ul">
<li><a href="https://git-scm.com/book/en/v2/Git-on-the-Server-Setting-Up-the-Server">https://git-scm.com/book/en/v2/Git-on-the-Server-Setting-Up-the-Server</a></li>
</ul></li>
<li>NoMachine for remote access to my desktop
<ul class="org-ul">
<li>This, along with <a href="https://wiki.archlinux.org/index.php/VirtualGL">VirtualGL</a> were very useful when doing some admittedly trivial experiments with <a href="https://www.khronos.org/vulkan/">Vulkan</a></li>
</ul></li>
<li>Setting up NFS on the ZFS pool was trivial and makes it much easier to move files around</li>
<li>Windows VM running all the time; Use remote desktop to access the VM</li>
</ul>
</div>
</div>

<div id="outline-container-org3bade61" class="outline-2">
<h2 id="org3bade61">Personal Tools that worked out</h2>
<div class="outline-text-2" id="text-org3bade61">
</div>
<div id="outline-container-org21b6213" class="outline-3">
<h3 id="org21b6213">General Archival</h3>
<div class="outline-text-3" id="text-org21b6213">
<p>
I have a small archive of:
</p>
<ul class="org-ul">
<li>Old school work and personal projects</li>
<li>Videos of high school marching band</li>
<li>Ripped cds that aren't available elsewhere</li>
<li>Important Documents</li>
</ul>

<p>
Saving these files on a backed-up, redundant storage system is working out well.
Also, since I can NFS mount this drive over my VPN, I can get at the files anywhere that a computer is available.
There's not a ton here, and there's not a ton that I want to keep readily accessible, so this is more of a dumping ground for things that I want to keep but don't otherwise need at my fingertips.
</p>
</div>
</div>

<div id="outline-container-org4fb8439" class="outline-3">
<h3 id="org4fb8439">Samples and Music Projects</h3>
<div class="outline-text-3" id="text-org4fb8439">
<p>
I currently do all of my music projects from an actual computer, so I'm storing all of the samples I've collected and recordings I've made on the NAS.
Since I run linux everywhere, I can NFS mount this drive from anywhere in my apartment and access my samples.
</p>
</div>
</div>

<div id="outline-container-orga2276c0" class="outline-3">
<h3 id="orga2276c0">Email/Contact backup</h3>
<div class="outline-text-3" id="text-orga2276c0">
<p>
As part of an experiment to try moving from GMail to literally anything else, I started trying to backup my emails and contacts.
I'm using <a href="http://isync.sourceforge.net/">isync</a> to download my emails, because it is <b>extremely</b> configurable.
I have isync setup to:
</p>
<ul class="org-ul">
<li>Download mail, move mail between folders, and delete mail from everything <span class="underline">except</span> the Trash folder</li>
<li>Download mail that makes it into the Trash folder, but do not delete the local copy, even if it disappears from the remote</li>
<li>Never sync any changes from my local backup back to my mail server</li>
</ul>

<p>
This config looks something like (with right column annotations):
</p>

<pre class="example">
Channel mail
Master :mail-remote:
Slave :mail-local:
Sync Pull All              # Pull all changes from remote-&gt;local
Patterns % !Trash          # Don't sync the Trash folder
Create Slave               # If a directory is missing on the slave, create it
Remove Slave               # Remove directory from slave if it doens't exist on master
CopyArrivalDate yes
SyncState *
Expunge Slave              # Actually delete stuff on the slave if it gets deleted on master

Channel mail-archive       # mostly the same, but filters down to Trash only and never expunge
Master :mail-remote:
Slave :mail-local:
Pattern Trash
Sync Pull All
Create Slave
Remove slave
CopyArrivalDate yes
SyncState *
</pre>
</div>
</div>

<div id="outline-container-org6d348d9" class="outline-3">
<h3 id="org6d348d9">RSS feeds</h3>
<div class="outline-text-3" id="text-org6d348d9">
<p>
I'm running <a href="https://github.com/wking/rss2email/">rss2email</a> on the server to send RSS feeds to my email.
Using a 32 core behemoth to run a python script daily is silly, but oh well.
</p>

<p>
rss2email works wonderfully and I highly recommend it.
This came about for me because I wanted to read RSS feeds in emacs, but I also wanted to be able to mark stuff as read on my phone.
My email is available on my phone, and I can make my email show up in emacs, so I'm trying this out.
</p>
</div>
</div>

<div id="outline-container-orga91cb33" class="outline-3">
<h3 id="orga91cb33">WeeChat</h3>
<div class="outline-text-3" id="text-orga91cb33">
<p>
WeeChat has a pretty nifty <a href="https://weechat.org/files/doc/devel/weechat_relay_protocol.en.html">relay protocol</a> that can be used to remotely access your WeeChat instance from a browser or mobile device.
I've been running WeeChat on my machine instead of figuring out how to configure ZNC or some other bouncer.
Im not super active on IRC, but I've been happy with this approach and these tools.
</p>
</div>
</div>
</div>

<div id="outline-container-org2f0205c" class="outline-2">
<h2 id="org2f0205c">Personal Tools that didn't work out</h2>
<div class="outline-text-2" id="text-org2f0205c">
</div>
<div id="outline-container-org47edf08" class="outline-3">
<h3 id="org47edf08">Photos</h3>
<div class="outline-text-3" id="text-org47edf08">
<p>
I have a large collection of RAW photos captured in a variety of vendor-specific RAW formats.
Getting these onto storage I owned and controlled was the main reason I built my NAS server.
</p>

<p>
Up until recently, Adobe's cloud version of Lightroom (Lightroom CC) has worked really well for me.
</p>
<ul class="org-ul">
<li>I don't really need or want Photoshop, I just want to process RAW files</li>
<li>The facial recognition it does is extremely useful</li>
<li>The AI powered search is neat and sometimes works</li>
<li>Great support for many RAW file formats</li>
<li>It's pretty hard to make a mistake and lose photos</li>
<li>Most importantly: I can edit my photos anywhere, and upload from any device</li>
</ul>

<p>
However, on a recent trip a few deficiences started to stand out:
</p>
<ul class="org-ul">
<li>The web-app really isn't very good yet; I've kept Windows installed on my laptop purely for lightroom
<ul class="org-ul">
<li>Booting windows and waiting for it to automatically update for an hour is annoying</li>
</ul></li>
<li>There's no seamless way to include locally-stored photos in your collection
<ul class="org-ul">
<li>If your network collection is too slow to upload a bunch of giant 50 meg RAW files (or Windows is sucking all your bandwidth to download an update), there's no a good way to view/edit the photos you took for the day from a local copy (with the sync running as a background task).</li>
</ul></li>
<li>Lightroom is pretty slow
<ul class="org-ul">
<li>Part of the problem is that some RAW files are extremely difficult to process quickly without a GPU</li>
</ul></li>
<li>It is nearly impossible to get photos out of lightroom easily</li>
</ul>

<p>
I tried to glue together pieces of open source software to get some important subset of the above features, but, this never totally worked.
</p>

<p>
The closest thing that currently exists is <a href="https://www.digikam.org/">DigiKam</a> with a <a href="https://userbase.kde.org/Digikam/Using_digiKam_with_MySQL">Remote Database</a>.
DigiKam is a really great piece of software.
Combine DigiKam with <a href="https://www.darktable.org/">Darktable</a> and <a href="https://rawtherapee.com/">rawtherapee</a> and you'll have a truly wonderful desktop photo editing suite.
</p>

<p>
But, if you try to access your NFS photo collection over a crappy internet connection, even with the MySQL database, this isn't going to work out so well.
Also, good luck doing anything with your phone.
</p>

<p>
I tried a bunch of crazy things to get this to work well from multiple machines and over poor internet connections, including keeping the entire DigiKam collection (and database) in <a href="https://git-annex.branchable.com/">git-annex</a>.
</p>

<p>
For now, I'll be sticking with lightroom, but I'm keeping my eyes open for something better.
I didn't trust any of the other solutions I came up with to be robust and reliable, and I didn't feel like I was gaining enough to give up the AI search, mobile editing, etc.
</p>

<p>
I don't mean for this to be an indictment of the open source stuff; the open source stuff is really good.
The photo editing world just really hasn't caught onto the "cloud" thing yet.
This space is also packed with proprietary software, proprietary file formats, and is sort of fundamentally unix-y.
Adobe is the only company currently offering a cloud RAW editor with these nifty AI features.
Everyone else that wants to use more than one machine generally has some crazy scheme involving Dropbox, partial syncing, and multiple databases (in flat files).
</p>

<p>
In a couple of months, I'll probably try this again.
Maybe I'll even build my own editor/manager software that works perfectly and does everything.
</p>
</div>
</div>

<div id="outline-container-orgbdfbe42" class="outline-3">
<h3 id="orgbdfbe42">Movies/TV/Music</h3>
<div class="outline-text-3" id="text-orgbdfbe42">
<p>
I considered turning this into a media streaming box, but it's nearly impossible to legally obtain copies of most of the TV and movies I want to keep around.
If I could easily get legal digital copies of the TV shows I want to watch, I'd be happy to pay, download, and manage the storage myself.
This isn't the case, so I'm just going to keep unhappily renting all of the media I consume.
</p>

<p>
For music, I've been pretty happy keeping unique, hard to stream music sitting around in FLAC files in my junk archive, then uploading the albums to Google Play Music for streaming from my phone and office.
</p>
</div>
</div>

<div id="outline-container-org1136b58" class="outline-3">
<h3 id="org1136b58">Notes, Papers, TODO lists</h3>
<div class="outline-text-3" id="text-org1136b58">
<p>
I'm currently using:
</p>
<ul class="org-ul">
<li><a href="https://www.mendeley.com/">Mendeley</a> to store a bunch of papers I've read and want to read, with good metadata</li>
<li><a href="https://todoist.com/">todoist</a> to manage todo lists</li>
<li><a href="https://orgmode.org/">org-mode</a> to take notes about things on a real computer</li>
</ul>

<p>
In theory, all of this could be done with orgmode, but I haven't been able to find a good way to sync org mode (with attachments) to mobile devices.
More or less I've decided that I don't need to or want to actually write orgmode notes on a mobile device.
I'm using todoist and email to send myself things I need to capture, then capturing them when I have a real computer.
I can export everything as HTML for read only access on my phone, in a form factor that works well on a small screen (not yet automated or searchable).
</p>

<p>
I haven't figured out papers because I haven't tried to wrangle orgmode correctly to manage/tag/adjust metadata for them.
Also, I regularly capture and sometimes read papers from my phone, so I need to find a proper workflow for managing these from my phone somehow.
</p>

<p>
Even if I could move all of this to orgmode, I'd still not be using the server for much of anything.
In the best case, I'd have a handful of text files in a git repo (or nextcloud or something), and a pile of PDFs on the NAS.
Again, none of this really requires a massive server.
</p>
</div>
</div>

<div id="outline-container-orgd1b737a" class="outline-3">
<h3 id="orgd1b737a">Everything Else</h3>
<div class="outline-text-3" id="text-orgd1b737a">
<p>
Most of the things I use on a regular basis in my life outside of work aren't realistically replacable with self-hosted versions of things.
</p>

<p>
If I refuse to use facebook messenger (or facebook) to stay in touch with people, they simply will not stay in touch or invite me to events.
Volunteer organizations I'm involved in use Google Docs to collaborate.
All of my financial applications are closed off and nearly impossible to access with custom code.
Lots of smart-junk needs to phone home to be fully functional.
</p>

<p>
I considered self-hosting email, but I'm not willing to deal with IP reputation issues and I'm not sure I want to have to keep my server running reliably enough to depend on it for email.
I'm not going to self-host a password manager because I'll be royally screwed if my server goes down.
</p>
</div>
</div>
</div>

<div id="outline-container-orgd5c7d88" class="outline-2">
<h2 id="orgd5c7d88">(false) Dichotomy?</h2>
<div class="outline-text-2" id="text-orgd5c7d88">
<p>
Since I like to jam the world into neat little dichotomies, here's some attempts explain why some things worked well for me and some things did not.
I've come up with two ways to word this.
I think they are equivalent:
</p>
<ul class="org-ul">
<li>Developer Tools vs Consumer Products</li>
<li>Power User Tools I use in a terminal vs Mobile/Web Apps I use on my phone or browser</li>
</ul>

<p>
I'm generally happy using developer tools, but generally unhappy when I interact with consumer products.
I'm generally extremely comfortable convoluted keybindings or jamming together sed/grep/awk pipelines, but am often irritated when I get stuck in some tool that is "user friendly."
</p>

<p>
There's a few possible (hyperbolic) explanations:
</p>
<ol class="org-ol">
<li>I'm a developer so it's obvious that I'd have the same mindset as other developers</li>
<li>Developer/sysadmin tools are pretty mature, or build on mature UNIX foundations. There are more integrations and tools already available for this reason.
<ul class="org-ul">
<li>IMAP syncing and healthchecks are examples of this.</li>
<li>I can <code>grep</code> almost anything that lives in developer/sysadmin land. I can't <code>grep</code> my <code>gmail</code>.</li>
</ul></li>
<li>There's some fundamental difference between the way "developers/power users" and "consumers" want to interact with their computers</li>
<li>"Modern" design is terrible/user-hostile because of an intense focus on ease of use
<ul class="org-ul">
<li>UNIXy stuff was designed for interoperability, not ease of use.</li>
<li>Modern apps are designed for ease of use.</li>
<li>Cloud providers have little incentive to make their products easily interoperate with other things, and even less incentive to let you easily extract your data.</li>
<li>Favorite Example: Instagram doesn't allow links anywhere other than a link in your user profile. This lets instagram control (through ads) most of the ways that people will leave the site. Good discussion on this <a href="https://anildash.com/2019/12/10/link-in-bio-is-how-they-tried-to-kill-the-web/">here</a>.</li>
</ul></li>
<li>It hard to make "power user" tools that are usable on a mobile device</li>
<li>I'm wrong. The tools I think are produtive are actually terrible.
<ul class="org-ul">
<li>Maybe I shouldn't be trying to type regex into gmail's search and just learn how to type "from:realist@gmail.com to:dreamer@not-gmail.com" into the search box.</li>
</ul></li>
</ol>

<p>
Reality is probably some combination of all of these.
</p>
</div>
</div>

<div id="outline-container-org51f4340" class="outline-2">
<h2 id="org51f4340">Closing Rant</h2>
<div class="outline-text-2" id="text-org51f4340">
<p>
Having an always-on machine, that I can get a virtual terminal on for adminstration has been convenient.
Running my own DNS server with ad-blocking baked in has been fantastic.
Having a rock-solid VPN server that I control, and that lets me ssh into my local machines from anywhere has been a game changer.
Having a sweet server to occasionally run some tests or play with my 10 gig network is pretty great.
</p>

<p>
Using a High Powered Xeon Server to backup some files, sometimes send me some emails, and host a metrics database is absurd.
</p>

<p>
I'm disappointed that having my own server hasn't turned out to be more useful.
Everything I use is a cloud service; everything I use to interact with other people is a cloud service.
Many modern cloud services are simply better than their self hosted alternatives.
</p>

<p>
From a practial perspective, I'm most disappointed that I haven't managed to glue together or build something open source/self hosted to mange my photos.
</p>

<p>
For a more philosophical perspective, I'm really disappointed that I keep running up against walled gardens with opaque protocols.
To be clear, I have no issue paying someone else to run services for me.
I also don't really have any issue with services that use funky protocols, or even if the entire service is closed-source and super proprietary.
I'm bothered that it is so difficult to move data between services and to easily hook things together to orchestrate some workflow I want.
</p>

<p>
In a perfect world, I'd be able to pay companies to host my email, do AI magic on my photos, provide storage for backups, etc.
Then, ideally, I'd be able to glue this all together myself.
I'd like to be able to move data between these services, bring my own storage, and build little custom tools to do cool tricks with the tools I have available.
</p>

<p>
In today's world, if I pay a company to host my email, I can't get push notifications on an Android phone, unless my email provider has built a custom app that uses google's servers to send messages to my phone (ironically the state of the world is very slightly better on apple devices).
</p>

<p>
In today's world, I can pay Adobe to host my photos, do magic AI stuff on them, and give me a nice editor, but I can barely get my photos out.
</p>

<p>
I can use Mendeley's free service to manage my PDFs, but I can't easily build a workflow that moves files between their cloud storage and my wifi-enabled eink reader (which can easily run custom code).
</p>

<p>
Maybe I just need to be patient and wait for all of these cloud products to become more mature.
Maybe there's something in the <a href="https://fediverse.network/about">fediverse</a> (<a href="https://matrix.org/">matrix</a> and <a href="https://www.w3.org/TR/activitypub/">ActivityPub</a> look promising) that solves some of these problems.
Maybe I just need to move on and go do something more productive with my time..
</p>
</div>
</div>
]]>
    </content>
  </entry>
  <entry>
    <title>Homelab Act 3: NAS, ZFS, and NFS</title>
    <link rel="alternate" type="text/html" href="http://dpzmick.com/posts/2020-01-12-nas.html"/>
    <id>urn:uuid:854e116f-7f18-552e-a6aa-dbd3b8c5420f</id>
    <updated>2020-01-12</updated>
    <content type="html">
      <![CDATA[<style>.org-src-container {white-space: pre; font-family: monospace }</style>
               <p>
For the last 172 days (according to uptime on one my routers), I've been setting up a small homelab.
Homelabs are kind of cool, and, the setup has been <i>interesting</i>.
I'll be writing a few posts explaining the steps I took.
</p>

<ul class="org-ul">
<li>Part 1 <a href="./2020-01-09-vpn.html">VPN setup</a></li>
<li>Part 2 <a href="./2020-01-11-server-network.html">Servers and 10GbE</a></li>
<li>Part 3 (this post)</li>
<li>Part 4 <a href="./2020-02-01-homelab4-cloud.html">Services and Applications</a></li>
</ul>

<p>
In this homelab post, I'll be detailing how I converted the R720 I bought on eBay into a NAS server on my local network.
This was an expensive project; I'm not sure it was worth the time or energy.
I'll add more discussion on the usability of something like this in the 4th part of this series.
</p>

<div id="outline-container-orged6e31d" class="outline-2">
<h2 id="orged6e31d">Why?</h2>
<div class="outline-text-2" id="text-orged6e31d">
<p>
I recently had an old SSD fail.
I had no idea it was going bad until I tried to open a handful of old photos and found that they were corrupt.
Some of the smart counters for the drive apparently had ticked up, but I never had set up anything automated to monitor this, so it was missed.
Of course, the files were all backed up, but this still isn't a great experience to have.
</p>

<p>
I wanted a safe place to store some important files.
I wanted these files to be available on all of my devices.
I didn't really want to use a cloud service for this, since I've been trying to scale back my cloud dependence and vendor lock-in problem (but maybe I should have).
</p>

<p>
For a local disk, I would certainly need some form of RAID array (and of course backups, but that's for a later post).
</p>

<p>
Currently in the root of the <code>/nas</code> contains:
</p>
<ul class="org-ul">
<li>audio samples</li>
<li>facebook/google data exports</li>
<li>iso files (windows iso, recent Arch iso, etc)</li>
<li>some ripped CDs that are not on streaming services</li>
<li>RAW photos</li>
<li>old programming projects zipped up</li>
<li>a bunch of papers as pdfs in a folder, unorganized</li>
<li>a bunch of old school work</li>
<li>org mode files</li>
</ul>

<p>
Right now, all of this is is only using about 500 gigs of space.
</p>

<p>
These files used to live on either 1) a spinning rust drive on my desktop or 2) my home directories on multiple machines (not really synced other occasional rsyncs of subsets of the data).
This post will discuss setting up the ZFS and mounting it as a NAS.
</p>
</div>
</div>

<div id="outline-container-orgfb3aeae" class="outline-2">
<h2 id="orgfb3aeae">Setup and Planning</h2>
<div class="outline-text-2" id="text-orgfb3aeae">
<p>
Since I was only using about 500 gigs of space, I wasn't going to need some super high capacity system.
I decided to instead try and build something that would be 1) lowish power (spinning rust is power hungry), 2) pretty "fast", 3) "fun".
</p>

<p>
Since this is all going into my only server, I'd also need some space for all of the other services I wanted to toy with, so I came up with this:
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-left">

<col  class="org-left">

<col  class="org-left">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Data Category</th>
<th scope="col" class="org-left">Space</th>
<th scope="col" class="org-left">Speed</th>
<th scope="col" class="org-left">Redundancy</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Personal Data</td>
<td class="org-left">2TB should last years</td>
<td class="org-left">fast-ish</td>
<td class="org-left">needs to be highly redundant</td>
</tr>

<tr>
<td class="org-left">server drive</td>
<td class="org-left">500GB more than enough</td>
<td class="org-left">fast</td>
<td class="org-left">doesn't need to be redundant</td>
</tr>

<tr>
<td class="org-left">OS drive</td>
<td class="org-left">500GB</td>
<td class="org-left">fast</td>
<td class="org-left">doesn't need to be redundant</td>
</tr>
</tbody>
</table>

<p>
The server has 16 2.5 inch SAS/SATA drive bays connected to a "SAS backplane."
The backplane is connected to a Dell-rebranded LSI hardware RAID controller card.
The RAID controller card is connected to the CPU via 8 Gen 3 PCIe lanes (in a Dell-proprietary form factor slot).
</p>

<p>
The backplane on server hold 16 drives and has across 8 SAS ports.
Each SAS port can do 6 Gbit/s, so we can do (6*8)/8 = 6 Gbytes/s on the RAID controller + backplane, in theory.
This is well matched with the PCIe bandwidth, which is theoretically around 7.9 gigs a second.
</p>

<p>
The good thing about all of this is that the backplane/RAID controller were well integrated into Dell's remote management tools.
The bad thing is of course that many of these parts are proprietary and have strange feature sets, but more on that later.
</p>

<p>
Thinking about the bandwidth capabilities of the server, my redundancy desires, and my low capacity requirements, I decided to try and build this entire thing with SSDs.
SAS SSDs designed for servers aren't cheap, so I decided to look at low-end consumer SATA SSDs.
</p>

<p>
Apparently, most RAID systems don't really like expanding the number of disks in the array.
I decided to price out filling up the system with disks.
</p>

<p>
I ended up with 4 disks from 4 different vendors (reduce risk of all of them failing at the same time):
</p>
<ul class="org-ul">
<li>4x ADATA SU635 480GB 3D-NAND SATA 2.5 inch Internal SSD</li>
<li>4x SanDisk SSD PLUS 480GB Internal SSD - SATA III 6 Gb/s</li>
<li>4x Kingston 480GB A400 Sata3 2.5 Internal SSD</li>
<li>4x PNY CS900 480GB 2.5” SATA III Internal Solid State Drive</li>
</ul>

<p>
From amazon, this ended up costing like $800, which is, uh, not very cheap.
I also had to grab some disk enclosures on eBay to install these disk into the server.
</p>
</div>
</div>

<div id="outline-container-org1682ae8" class="outline-2">
<h2 id="org1682ae8">Filesystem</h2>
<div class="outline-text-2" id="text-org1682ae8">
<p>
Next thing up, I needed to pick a filesystem/RAID scheme to run on these drives.
</p>
</div>

<div id="outline-container-org52c20ce" class="outline-3">
<h3 id="org52c20ce">Hardware RAID</h3>
<div class="outline-text-3" id="text-org52c20ce">
<p>
I bought the "upgraded" RAID controller when purchasing the server, since I wanted to keep my options open.
After thinking a bit harder about hardware RAID, it doesn't really seem that interesting to me.
Hardware RAID might be a win if I didn't have tons of RAM to spare, or if I was very CPU constrained.
Since neither of those are the case, it seems wiser to use my powerful Xeon CPUs and the large amount of ECC RAM available on the server to do fs checksumming and for caching purposes.
</p>
</div>
</div>

<div id="outline-container-org588c7b9" class="outline-3">
<h3 id="org588c7b9">ZFS</h3>
<div class="outline-text-3" id="text-org588c7b9">
<p>
Awesome ZFS features:
</p>
<ul class="org-ul">
<li>Great reliability features (checksumming in metadata)</li>
<li>Snapshots</li>
<li>Very flexible RAID configurations</li>
<li>Theoretically easy to setup and try out</li>
<li>Fantastic monitoring tools (check out the influxdb intergrations)</li>
<li>Great reviews online</li>
<li>Great arch-wiki documentation</li>
<li>Apparently no <a href="http://www.raid-recovery-guide.com/raid5-write-hole.aspx">RAID write hole problems</a> despite not having a dedicated write-flush backup battery like HW would have</li>
<li>many more</li>
</ul>
</div>
</div>

<div id="outline-container-orgebcd588" class="outline-3">
<h3 id="orgebcd588">Linux software RAID (mdadm and LVM)</h3>
<div class="outline-text-3" id="text-orgebcd588">
<p>
Looks great, but not as featureful as ZFS.
If I try ZFS out and it doesn't work, I figured it would be easy to switch.
</p>

<p>
BTRFS was eliminated early as it seems to still be fairly immature.
</p>
</div>
</div>
</div>

<div id="outline-container-orgcb1df24" class="outline-2">
<h2 id="orgcb1df24">Installation</h2>
<div class="outline-text-2" id="text-orgcb1df24">
</div>
<div id="outline-container-org914a246" class="outline-3">
<h3 id="org914a246">Physical Install</h3>
<div class="outline-text-3" id="text-org914a246">
<p>
Getting these drives into the server was easy.
Just screw them into the enclosures:
</p>


<figure id="orga6b373c">
<img src="../static/homelab/disk_in_enclosure.jpg" alt="disk_in_enclosure.jpg">

</figure>

<p>
Then pop them into the front mounting slots:
<img src="../static/homelab/all_installed.jpg" alt="all_installed.jpg">
</p>
</div>
</div>

<div id="outline-container-org6cdf6ea" class="outline-3">
<h3 id="org6cdf6ea">Configure RAID</h3>
<div class="outline-text-3" id="text-org6cdf6ea">
<p>
Next up was configuring the RAID controller to get out of the way.
I wanted the raid controller to just pass the disks through to the operating system.
It also seemed important to make sure that I could access the S.M.A.R.T. status of the devices.
</p>

<p>
Surprise surprise, the <b>upgraded</b> RAID controller I purchased is not able to do this!
Apparently, the lower end model is, but only if you flash the thing with some special alternative firmware that breaks all of the fancy Dell integration.
</p>

<p>
Regardless, I booted the machine with some of the drives installed to see what would happen.
The dell controller was not happy with the consumer drives.
It marked a number of them as degraded, and thought that the kingston drives were SAS drives (maybe they actually are? never figured this out).
Fortunately, it seemed like all of the drives were at least working.
</p>
</div>
</div>

<div id="outline-container-orga9d8548" class="outline-3">
<h3 id="orga9d8548">Downgrade controller</h3>
<div class="outline-text-3" id="text-orga9d8548">
<p>
After a <b>very very</b> large amount of time spent googling around, I found some references that said that, if you get the downgraded Dell H310 mini controller, it is possible to flash the controller to an alternative LSI "IT mode" firmware.
The IT mode firmware is supposed to allow you to just pass the disks through to the OS.
</p>

<p>
Standard flashing procedures won't work though, because Dell looks for some special "I'm a Dell Special Thing" from the device at boot time.
If you flash the board incorrectly, the server will refuse to boot in any way when the board is installed (so you can't reflash it).
</p>

<p>
There's a guy on eBay who will sell you on of these pre-flashed.
Search for "Dell H310 mini monolithic K09CJ with LSI 9211-8i P20 IT Mode" then just buy one from him if you want to do this.
</p>

<p>
I of course didn't go down this path.
Instead, I found some PDF file on archive.org that contained some instructions for flashing the controller.
Since references to this file seem to all go stale, I'm mirroring it <a href="../static/homelab/h310_it.pdf">here</a>, although I keep redoing my blog so this link will probably go stale too.
I booted an Arch Linux iso through the remote managment interface and configured everything from Arch.
</p>

<p>
To follow these instructions, you'll have to find the LSI firmware files.
Since LSI has been acquired like 30 times, its not entirely clear where to find them.
To find these file names, figure out who owns LSI now and go look for their firmware downloads page.
</p>

<p>
You're looking for:
</p>
<ul class="org-ul">
<li><code>9211_8i_Package_P20_IR_IT_FW_BIOS_for_MSDOS_Windows.zip</code> or <code>9211_8i_Package_P20_IR_IT_FW_BIOS_for_MSDOS_Windows.zip</code>. After unzipping, you'll find <code>Firmware/HBA_9211_8i_IT/Firmware/HBA_9211_8i_IT.bin</code></li>
<li><code>UEFI_BSD_P20.zip</code>. After unzipping, you'll find <code>uefi_bsd_rel/Signed/x64sas2.rom</code></li>
</ul>

<p>
Once you have these, you should be able to follow the remaining instructions in the PDF.
</p>

<p>
There's a note in the PDF that says:
</p>
<blockquote>
<p>
Should you want to boot off a drive attached to the H310MM, you will also have to flash the appropriate bootrom (mptsas2.rom for BIOS, x64sas2.rom for UEFI). 
</p>
</blockquote>

<p>
This is a very true statement and you'll be stuck scrathing your head for a long time if you miss/ignore it.
<b>Make sure to also flash the efi firmware to the device.</b>
</p>
</div>
</div>

<div id="outline-container-orgf2356fa" class="outline-3">
<h3 id="orgf2356fa">Disk inventory</h3>
<div class="outline-text-3" id="text-orgf2356fa">
<p>
Since the Dell firmware integration is all broken with the new firmware, I needed to be able to keep track of which drive was which without having being able to easily toggle the chassis LEDs.
I booted an Arch ISO and started <code>dd=ing zeros to each disk through =/dev/disk/by-id/</code>, then recording the serial numbers of the disks whose activity LEDs lit up.
For some reason, the activity LEDs won't light up on the ADAT disks, so I just popped those in and out and watched the kernel logs.
</p>

<p>
All of the serial numbers and slot assignments are saved in a safe place.
This is probably important to have when disks need replacing.
</p>
</div>
</div>

<div id="outline-container-org31f9524" class="outline-3">
<h3 id="org31f9524">Install Operating System</h3>
<div class="outline-text-3" id="text-org31f9524">
<p>
From the Arch iso, I partioned the disk in the 0th slot, and installed Arch using the standard install guide.
</p>
</div>
</div>

<div id="outline-container-orgbd1f788" class="outline-3">
<h3 id="orgbd1f788">rootdelay</h3>
<div class="outline-text-3" id="text-orgbd1f788">
<p>
The OS install when smoothly, so I thought I was finally done with this ordeal once Arch finished bootstrapping the system.
</p>

<p>
Wrong!
</p>

<p>
Linux consistently failed to boot.
I'd get through a GRUB screen, load initrd, then consistently fail to find the root partition.
The root partition was on the same drive as GRUB, so this doesn't really make sense.
</p>

<p>
Apparently, when booting, the EFI system initializes the controller to get the bootloader, Linux initrd, etc.
But then, when the initrd starts, something in Linux's drivers causes the SAS controller to reinit.
The controller takes a long time to initialize, so Linux will have a hard time finding it's boot disk.
</p>

<p>
Adding <code>rootdelay=600</code> to my kernel command line got me passed this problem; now Linux waits for root partition to show up for 5 minutes before giving up on the filesystem.
</p>
</div>
</div>

<div id="outline-container-org02a24bb" class="outline-3">
<h3 id="org02a24bb">Configure ZFS</h3>
<div class="outline-text-3" id="text-org02a24bb">
<p>
Just follow the instructions on the <a href="https://wiki.archlinux.org/index.php/ZFS">Arch Wiki</a>.
I installed the DKMS version of ZFS so that I would be able to <code>pacman -Syu</code> and have <code>pacman</code> attempt to rebuild ZFS with the latest kernel.
</p>

<p>
I setup two zpools.
One for my personal files named <code>nas</code> and another for server stuff named <code>server</code>.
These are mounted, creatively, at <code>/nas</code> and <code>/server</code>.
</p>
</div>

<div id="outline-container-org9a60436" class="outline-4">
<h4 id="org9a60436"><code>nas</code></h4>
<div class="outline-text-4" id="text-org9a60436">
<p>
For the <code>nas</code> zpool, I'm using 12 disk with data striped across two RAIDZ2 zpools.
In other words, each of the RAIDZ2 pools can loose two disk without failing.
All of my data is striped across these two pools.
</p>

<p>
From a performance perspective, check out <a href="https://www.delphix.com/blog/delphix-engineering/zfs-raidz-stripe-width-or-how-i-learned-stop-worrying-and-love-raidz">this post</a>:
</p>
<blockquote>
<p>
For performance on random IOPS, each RAID-Z group has approximately the performance of a single disk in the group. 
</p>
</blockquote>

<p>
So, the performance isn't going to be fantastic on the <code>nas</code> array, if I set it up like this.
I'll pretty only be aggregating across the two stripes, so, assuming read/write of 500mb/s on a standard SATA ssd, I should expect read/write speeds around a gig a second for the pool.
Fortunately, that's exactly what I'm getting.
</p>

<p>
I have no idea if this striping/raidz combination is a good idea or not, but it seems like a reasonable safety/performance tradeoff.
</p>
</div>
</div>

<div id="outline-container-org92ff7e4" class="outline-4">
<h4 id="org92ff7e4"><code>server</code></h4>
<div class="outline-text-4" id="text-org92ff7e4">
<p>
The <code>server</code> array is just a single raidz1 array with 3 disks in it.
This array isn't that interesting and I haven't tried to push it very hard yet.
</p>

<p>
Contiguous reads/writes run at a blistering ~400-500mb/s, as expected.
</p>
</div>
</div>

<div id="outline-container-orgc13b79d" class="outline-4">
<h4 id="orgc13b79d">Perfomance Testing</h4>
<div class="outline-text-4" id="text-orgc13b79d">
<p>
Here's some <i>naive</i> <code>dd</code> performance tests on the disk arrays.
These tests are all performed on the server.
</p>

<p>
For the <code>nas</code> array:
</p>
<div class="org-src-container">
<pre class="src src-bash"><span class="org-comment-delimiter"># </span><span class="org-comment">copy 5 GiB file of random bytes from /tmp (ramdisk), to the ZFS array</span>
$ dd <span class="org-variable-name">if</span>=/tmp/test <span class="org-variable-name">of</span>=test <span class="org-variable-name">bs</span>=2M
2560+0 records<span class="org-keyword"> in</span>
2560+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 5.51472 s, 974 MB/s

<span class="org-comment-delimiter"># </span><span class="org-comment">read the file we just copied to nowhere (immediately after writing)</span>
$ dd <span class="org-variable-name">if</span>=test <span class="org-variable-name">of</span>=/dev/null <span class="org-variable-name">bs</span>=2M
2560+0 records<span class="org-keyword"> in</span>
2560+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 2.96748 s, 1.8 GB/s

<span class="org-comment-delimiter"># </span><span class="org-comment">same thing again (should get some caching effects, sort of getting that)</span>
$ dd <span class="org-variable-name">if</span>=test <span class="org-variable-name">of</span>=/dev/null <span class="org-variable-name">bs</span>=2M
2560+0 records<span class="org-keyword"> in</span>
2560+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 2.2822 s, 2.4 GB/s

<span class="org-comment-delimiter"># </span><span class="org-comment">drop page cache and zfs arc cache, then reread same file</span>
$ dd <span class="org-variable-name">if</span>=test <span class="org-variable-name">of</span>=/dev/null <span class="org-variable-name">bs</span>=2M
2560+0 records<span class="org-keyword"> in</span>
2560+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 4.96745 s, 1.1 GB/s
</pre>
</div>
</div>
</div>

<div id="outline-container-orgf1f6347" class="outline-4">
<h4 id="orgf1f6347">Sidebar: NVMe</h4>
<div class="outline-text-4" id="text-orgf1f6347">
<p>
My desktop has a single $300 NVMe drive in it.
Compare:
</p>
<div class="org-src-container">
<pre class="src src-bash"><span class="org-comment-delimiter"># </span><span class="org-comment">copy 5 GiB file of random bytes to NVMe</span>
$ dd <span class="org-variable-name">if</span>=/tmp/test <span class="org-variable-name">of</span>=test <span class="org-variable-name">bs</span>=4M
1280+0 records<span class="org-keyword"> in</span>
1280+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 4.76216 s, 1.1 GB/s

<span class="org-comment-delimiter"># </span><span class="org-comment">copy to nowhere (pagecache)</span>
$ dd <span class="org-variable-name">if</span>=test <span class="org-variable-name">of</span>=/dev/null <span class="org-variable-name">bs</span>=4M
1280+0 records<span class="org-keyword"> in</span>
1280+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 0.468432 s, 11.5 GB/s

<span class="org-comment-delimiter"># </span><span class="org-comment">drop caches and try again</span>
$ dd <span class="org-variable-name">if</span>=test <span class="org-variable-name">of</span>=/dev/null <span class="org-variable-name">bs</span>=4M
1280+0 records<span class="org-keyword"> in</span>
1280+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 1.76705 s, 3.0 GB/s
</pre>
</div>

<p>
One NVMe/PCIe drive is destroying this expensive array, but that's expected.
If you are going for raw performance, get the NVMe drives and skip the server.
</p>

<p>
In theory, if I stripped across all of these SSDs I'd be able to get competitive, but I have bigger unresolved performance issues with NFS and I already have valuable data on this array, so I have not tried this yet.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org53cce24" class="outline-2">
<h2 id="org53cce24">NFS</h2>
<div class="outline-text-2" id="text-org53cce24">
<p>
Trivial NFS is easy to setup with ZFS.
You can simply install the right NFS servers, then tell ZFS to export the mount point.
</p>
</div>

<div id="outline-container-orgb7c5410" class="outline-3">
<h3 id="orgb7c5410">NFS performance</h3>
<div class="outline-text-3" id="text-orgb7c5410">
<p>
Unfortunately, NFS over my 10 GbE network doesn't perform as well as you'd hope.
</p>

<p>
From an NFS mount over 10 GbE (default mount options, few seem to make a difference but I have more to learn here):
</p>
<div class="org-src-container">
<pre class="src src-bash"><span class="org-comment-delimiter"># </span><span class="org-comment">copy a 5 GiB file of random bytes from /tmp (ramdisk), to the NFS mount</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">From switch stats: NFS isn't saturating the link for some reason.</span>
$ dd <span class="org-variable-name">if</span>=/tmp/test <span class="org-variable-name">of</span>=test <span class="org-variable-name">bs</span>=1M
5120+0 records<span class="org-keyword"> in</span>
5120+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 10.2211 s, 525 MB/s

<span class="org-comment-delimiter"># </span><span class="org-comment">read the file we just copied to nowhere (immediately after writing)</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">again, the switch maxed out at 4gbps during this transfer..</span>
<span class="org-comment-delimiter">#        </span><span class="org-comment">but mostly was nowhere close to the limit</span>
$ dd <span class="org-variable-name">if</span>=test <span class="org-variable-name">of</span>=/dev/null <span class="org-variable-name">bs</span>=1M
5120+0 records<span class="org-keyword"> in</span>
5120+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 16.3145 s, 329 MB/s

<span class="org-comment-delimiter"># </span><span class="org-comment">same thing again</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">better, this time I'm hitting the page cache on my RYZEN box</span>
$ dd <span class="org-variable-name">if</span>=test <span class="org-variable-name">of</span>=/dev/null <span class="org-variable-name">bs</span>=1M
5120+0 records<span class="org-keyword"> in</span>
5120+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 0.614145 s, 8.7 GB/s

<span class="org-comment-delimiter"># </span><span class="org-comment">drop page cache, reread same file</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">again, same deal</span>
$ dd <span class="org-variable-name">if</span>=test <span class="org-variable-name">of</span>=/dev/null <span class="org-variable-name">bs</span>=1M
5120+0 records<span class="org-keyword"> in</span>
5120+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 9.64609 s, 557 MB/s
</pre>
</div>

<p>
As of this time, I haven't attempted to figure out why these rates are so poor.
Trivial network tests with <code>iperf3</code> and some custom code indicate that my NIC drivers and switch are all working properly, so there must be something I need to tune somewhere in the NFS layer.
</p>

<p>
I can trivally saturate gigabit with these rates, which means I'm also trivialy saturating the uplink through my <a href="./2020-01-09-vpn.html">VPN</a> as well.
Since I'm currently spending more of my time connected to the VPN from remote places (with less than gigabit bw), optimizing the NFS has not been a priority.
</p>
</div>
</div>
</div>

<div id="outline-container-orgd642fb7" class="outline-2">
<h2 id="orgd642fb7">Using the system</h2>
<div class="outline-text-2" id="text-orgd642fb7">
<p>
NFS works as well as I'd expect it to, but I'll discuss this and a few other details in a future post.
</p>

<p>
I've copied a bunch of files onto the nas mount from my laptop and desktop, both locally and remotely.
ZFS has been rock solid and the DKMS builder has rebuilt the modules successfully so far during kernel upgrades.
</p>

<p>
A ZFS scrub detected on checksum error, but fixed itself.
All disk report that they are healthy.
Cosmic rays?
</p>

<p>
The biggest win by far is having my orgmode files available on all of my computers without using some third party to do syncing.
</p>

<p>
Overall, I'm reasonably happy with this setup, although I'm wondering if I should have just setup some sort of FUSE mount of B2 and moved on with life.
Getting this to work was a lot of work, and the amount number things that need to not break is large.
The local network performance doesn't help me at all when I'm remote, which is most of the time.
</p>
</div>
</div>
]]>
    </content>
  </entry>
  <entry>
    <title>Homelab Act 2: Servers and 10GbE</title>
    <link rel="alternate" type="text/html" href="http://dpzmick.com/posts/2020-01-11-server-network.html"/>
    <id>urn:uuid:dcf29147-2346-5ce5-b739-8c90f9c72e73</id>
    <updated>2020-01-11</updated>
    <content type="html">
      <![CDATA[<style>.org-src-container {white-space: pre; font-family: monospace }</style>
               <p>
For the last 171 days (according to uptime on one my routers), I've been setting up a small homelab.
Homelabs are kind of cool, and, the setup has been <i>interesting</i>.
I'll be writing a few posts explaining the steps I took.
</p>

<ul class="org-ul">
<li>Part 1 <a href="./2020-01-09-vpn.html">VPN setup</a></li>
<li>Part 2 (this post)</li>
<li>Part 3 <a href="./2020-01-12-nas.html">NAS, ZFS, and NFS</a></li>
<li>Part 4 <a href="./2020-02-01-homelab4-cloud.html">Services and Applications</a></li>
</ul>

<p>
Since I could access my system remotely, I had started <a href="../static/homelab/desktop_under_desk.jpg">sticking interesting network cards into my desktop</a> around this time and working on projects remotely.
Unfortunately, I didn't have enough PCIe slots, or PCIe lanes, to keep all of these plugged in at the same time.
Plugging and unplugging stuff all the time was a bit of a pain.
</p>

<p>
Since part of this was running in the cloud, I figured I'd also start setting up the cool stuff.
I was hoping to set up small cloud instances to run:
</p>
<ul class="org-ul">
<li><a href="https://www.influxdata.com/">influx</a> and <a href="https://grafana.com/">grafana</a>
<ul class="org-ul">
<li>Having a place to blast metrics and make plots is tremendously useful, even for personal projects.</li>
</ul></li>
<li>Some sort of NFS server or cloud syncing server
<ul class="org-ul">
<li>I've been trying out <a href="https://orgmode.org/">orgmode</a> and was looking for a way to sync files between devices</li>
<li>I also have a large photo collection, a few ripped CDs that can't be streamed, some home videos, etc</li>
</ul></li>
<li>IRC bouncer/or headless weechat with <a href="https://weechat.org/files/doc/stable/weechat_relay_protocol.en.html">remoting enabled</a></li>
<li>Maybe a self-hosted RSS reader?</li>
<li>Perhaps a sql server for <a href="https://www.digikam.org/news/2010-10-15_using_digikam_with_mysql/">digikam's remote database</a> (if I can find a place to store the files)</li>
<li><a href="https://github.com/awesome-selfhosted/awesome-selfhosted">lots of stuff</a></li>
</ul>

<div id="outline-container-org985cf99" class="outline-2">
<h2 id="org985cf99">Cloud Cost</h2>
<div class="outline-text-2" id="text-org985cf99">
<p>
A single, low ram, shared CPU, tiny storage cloud instance can cost something like $3.50 a month.
If you want a real CPU, or some real storage, the prices go up quite a bit.
I wanted to run, at a minumum, influx, grafana, weechat, and an NFS (or owncloud) server with 20-30gigs of space.
Influx (or a SQL server) needs real-ish servers, with real-ish CPUs.
Neither should need much storage for my workload.
</p>

<p>
A single core (real core, not shared core) server costs like $10 a month (ish kindof), so, if I wanted two of them, I'd be paying $240 a year for 2 cores.
This is sort of okay, but this won't come close to addressing my photo problem.
I'd also still be swapping PCIe cards back and forth for my other projects (annoying!).
</p>

<p>
I know I've seen used rackmount servers on eBay for about this price, so I thought it might be worthwhile to put a server in my apartment.
</p>
</div>
</div>

<div id="outline-container-orge004ab0" class="outline-2">
<h2 id="orge004ab0">eBay time</h2>
<div class="outline-text-2" id="text-orge004ab0">
<p>
The first order of business was finding a "quiet, low power, expandable, powerful server."
I eventually settled on a Dell R720 with 2.5 inch drive bays.
Specs:
</p>
<ul class="org-ul">
<li>2x xeon E5-2650 v2 (8 physical cores per socket)</li>
<li>32 gig ECC ram</li>
<li>2x 10k SAS drives (300gigs each)</li>
<li>H710p controller (this was the upgraded choice, seemed wise in case I wanted to do RAID. This was very wrong, see <a href="./2020-01-12-nas.html">NAS, ZFS, and NFS</a>)</li>
<li>4 ports of gigabit ethernet</li>
<li>Dell's iDRAC remote management system</li>
</ul>

<p>
This was a $379 server.
Passmark gives the dual xeons an <a href="https://www.cpubenchmark.net/cpu.php?cpu=Intel+Xeon+E5-2650+v2+%40+2.60GHz&amp;id=2042&amp;cpuCount=2">18813</a>.
A new 2016 i7 (the one I used until RYZEN happened) sells in 2020 for $300 and passmarks at <a href="https://www.cpubenchmark.net/cpu.php?cpu=Intel+Core+i7-6700K+%40+4.00GHz&amp;id=2565">11108</a>.
The RYZEN chip I have now blows them both away at <a href="https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+7+3800X&amp;id=3499">24503</a> on passmark, but this isn't an AMD fanboy post.
The Xeon is a 2013 CPU, so the performance/power isn't going to be as good as newer cpus, but the performance/dollar pretty much blows away the cloud deal, if you only consider CPU cores.
</p>

<p>
This decision wasn't easy, but after reading almost every post on <a href="https://www.reddit.com/r/homelab/">r/homelab</a> I decided I'd give this a try.
</p>
</div>

<div id="outline-container-orga0790a2" class="outline-3">
<h3 id="orga0790a2">Barebones server setup</h3>
<div class="outline-text-3" id="text-orga0790a2">
</div>
<div id="outline-container-org143b53c" class="outline-4">
<h4 id="org143b53c">Setup a RAID 0 array across the two drives</h4>
<div class="outline-text-4" id="text-org143b53c">
<p>
For testing, I just stuck a RAID 0 array across the two drives.
This is done using the remote management web ui, or by booting the machine and tweaking settings from the remote management virtual display.
</p>
</div>
</div>

<div id="outline-container-org547b3c6" class="outline-4">
<h4 id="org547b3c6">Install an operating system</h4>
<div class="outline-text-4" id="text-org547b3c6">
<p>
Again, super straightforward.
I mounted an arch ISO using the remote management tools, booted the box, and installed arch the standard way.
</p>

<p>
While the server is booting, the fans spin at something like 75% their max RPM.
This is loud enough to be heard through a closed door.
</p>

<p>
Once an OS is installed and booted, the fans in the server will spin down to a pretty reasonably low volume.
I can still hear the machine when the room is silent, but, if I'm typing, playing music, or doing pretty much anything else, I can't really tell it is there anymore.
If I put a large amount of load on the machine, the fans will spin up, but that's expected and doesn't really bother me.
</p>
</div>
</div>

<div id="outline-container-orgf12e400" class="outline-4">
<h4 id="orgf12e400">Adding PCIe cards</h4>
<div class="outline-text-4" id="text-orgf12e400">
<p>
To install PCIe cards, all you have to do is lift the lid, pop out a little tool-less bay and plop the card in.
Its useful to read the server documentation to make sure each card is installed is attached to the appropriate socket, if you are installing multiple cards.
</p>

<p>
After installing new cards, the machine booted, fans spun at max (as expected), but, they never spun down.
Apparently Dell doesn't like it when you install "non-certified" cards in the server, since they are not aware of the thermal requirements of the card.
</p>

<p>
The internet gave two pieces of advice:
</p>
<ol class="org-ol">
<li>Update to the latest iDRAC.</li>
<li>Issue some magical incantations that tell the server to chill out about certification.</li>
</ol>

<p>
I tried (1), but it didn't make any difference.
For (2), I found the solution <a href="https://serverfault.com/questions/715387/how-do-i-stop-dell-r730xd-fans-from-going-full-speed-when-broadcom-qlogic-netxtr/733064#733064">here</a>, reposted for longevity:
</p>

<div class="org-src-container">
<pre class="src src-bash"><span class="org-comment-delimiter"># </span><span class="org-comment">check if the fans will get loud (do this first to make sure these instructions actually work)</span>
$ ipmitool raw 0x30 0xce 0x01 0x16 0x05 0x00 0x00 0x00

<span class="org-comment-delimiter"># </span><span class="org-comment">response like below means Disabled (fans will not get loud)</span>
16 05 00 00 00 05 00 01 00 00

<span class="org-comment-delimiter"># </span><span class="org-comment">response like below means Enabled (fans will get loud)</span>
16 05 00 00 00 05 00 00 00 00

<span class="org-comment-delimiter"># </span><span class="org-comment">if that worked, you can Disable the "Default Cooling Response Logic" with</span>
$ ipmitool raw 0x30 0xce 0x00 0x16 0x05 0x00 0x00 0x00 0x05 0x00 0x01 0x00 0x00

<span class="org-comment-delimiter"># </span><span class="org-comment">to turn it back on</span>
$ ipmitool raw 0x30 0xce 0x00 0x16 0x05 0x00 0x00 0x00 0x05 0x00 0x00 0x00 0x00
</pre>
</div>

<p>
To connect to my server, I needed to run <code>ipmitool</code> like this (use the idrac user/password):
</p>

<div class="org-src-container">
<pre class="src src-bash"><span class="org-comment-delimiter"># </span><span class="org-comment">at this point, server hostname was `worf` and idrac hostname was `idrac-worf`</span>
$ ipmitool -I lanplus -H idrac-worf -U root raw 0x30 0xce 0x01 0x16 0x05 0x00 0x00 0x00
Password:
 16 05 00 00 00 05 00 01 00 00
</pre>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-orgfa25500" class="outline-2">
<h2 id="orgfa25500">Networking</h2>
<div class="outline-text-2" id="text-orgfa25500">
<p>
As discussed in my <a href="./2020-01-09-vpn.html">previous post</a>, I already had a small TP-Link switch sitting behind the VPN/router box.
Four ports was getting tight (I was unplugging my smart light hub to play with a network card).
</p>

<p>
I had some project ideas that might benefit from having fast ethernet, and, I really wanted statistics from the switches.
To keep a long story short, I've ended up with a combination of a few different network cards and two Mikrotik switches:
</p>
<ul class="org-ul">
<li>CSS326-24G-2S+RM: 24 port 1gib, 2 SFP+ 10gib. My primary network lives on this switch</li>
<li>CRS309-1G-8S+IN: 8 SFP+ 10gib, 1x 1gib for managment. A secondary experimental network lives here.</li>
</ul>

<p>
Buying this networking gear (especially the 10 GbE switch), pushed me a little bit over the "saving money over cloud" limit if I'm only planning on running a small number of services.
However, as I'll discuss in my <a href="./2020-01-12-nas.html">next post</a>, I'm also pushing a lot of bandwidth over this network and I'm not sure what that would cost on AWS.
</p>
</div>
</div>

<div id="outline-container-org886b2d3" class="outline-2">
<h2 id="org886b2d3">Rack</h2>
<div class="outline-text-2" id="text-org886b2d3">
<p>
I wired a bunch of stuff up and threw it under my desk:
</p>


<figure id="orgd65b23e">
<img src="../static/homelab/server_desktop_wire_mess.jpg" alt="server_desktop_wire_mess.jpg">

</figure>

<p>
That wasn't going to work, so I threw all of this into a small rack, moved my desktop to a (crappy) rack mount case, and here we are:
</p>


<figure id="org36d49c9">
<img src="../static/homelab/battlestation.jpg" alt="battlestation.jpg">

</figure>
</div>
</div>

<div id="outline-container-orga595bf9" class="outline-2">
<h2 id="orga595bf9">Do something with it</h2>
<div class="outline-text-2" id="text-orga595bf9">
<p>
At first I used this server mostly to poke at the network and write kernel bypass drivers for an Intel I350-T4 quad-gigabit-port network card.
Side note: The <a href="https://github.com/emmericp/ixy">ixy</a> project is pretty neat and 100% worth poking at if you are interested in networking.
I was able to get a driver working for the previously mentioned intel card by reading intel's documentation, ixy's other drivers, and <a href="https://spdk.io/">spdk</a> in 500 lines of code.
</p>

<p>
I've also used this as a bunch of CPUs for some brute forcing I tried on a few advent of code problems (I also solved them the right way) and for a few other projects where I wanted a quiet system to benchmark on.
</p>

<p>
Having a large, remotely managable server available has been pretty convenient (even though the hardware is a little old).
Also, it looks really cool.
</p>

<p>
Currently, this machine is NAS and runs a handful of services.
See my <a href="./2020-01-12-nas.html">next post</a> for the continuation of this series.
</p>
</div>
</div>
]]>
    </content>
  </entry>
  <entry>
    <title>Homelab Act 1: VPNs</title>
    <link rel="alternate" type="text/html" href="http://dpzmick.com/posts/2020-01-09-vpn.html"/>
    <id>urn:uuid:9e831c77-d15e-5ab6-b1ef-35b741e7b9ff</id>
    <updated>2020-01-09</updated>
    <content type="html">
      <![CDATA[<style>.org-src-container {white-space: pre; font-family: monospace }</style>
               <p>
For the last 170 days (according to uptime on one my routers), I've been setting up a small homelab.
Homelabs are kind of cool, and, the setup has been <i>interesting</i>.
I'll be writing a few posts explaining the steps I took.
</p>

<ul class="org-ul">
<li>Part 1 (this post)</li>
<li>Part 2 <a href="./2020-01-11-server-network.html">Servers and 10GbE</a></li>
<li>Part 3 <a href="./2020-01-12-nas.html">NAS, ZFS, and NFS</a></li>
<li>Part 4 <a href="./2020-02-01-homelab4-cloud.html">Services and Applications</a></li>
</ul>

<div id="outline-container-org7a8b109" class="outline-2">
<h2 id="org7a8b109">VPNs</h2>
<div class="outline-text-2" id="text-org7a8b109">
<p>
Unfortunately, I don't have a public IP in my building, so setting up remote access had to be a little more involved than just opening a port for ssh.
I found a tiny, passively cooled, quad-port <b>intel</b> NIC, celeron box <a href="https://www.amazon.com/Firewall-Gigabit-Celeron-AES-NI-Barebone/dp/B07G7H4M73">on amazon</a>, and figured I'd try setting up my own linux router and run my own VPN server in AWS or something.
</p>

<p>
This took a bit of work.
</p>
</div>

<div id="outline-container-org72b2a85" class="outline-3">
<h3 id="org72b2a85">Networking</h3>
<div class="outline-text-3" id="text-org72b2a85">
<p>
Before diving in, I needed to decide how to layout my network and what VPN tools to use.
I read a ton about best practices, but this wasn't super helpful, so instead, I just started spinning up VMs and messing with VPN software and playing with my network settings.
</p>

<p>
Eventually I settled on wireguard for a VPN, mostly because I never actually got OpenVPN working anywhere.
I also spent bunch of time trying to get IPSec and layer 2 tunneling to work, but decided that I didn't really want that anyway.
Wireguard is easy, fast, and probably secure, so I'm using that for now.
</p>

<p>
The network is organized as:
</p>
<ul class="org-ul">
<li>One subnet for each "region"
<ul class="org-ul">
<li>My apartment is on the <code>172.16.1.xxx</code> subnet (for snobs: <code>172.16.1.0/24</code>)</li>
<li>A cloud region on the <code>172.16.2.xxx</code> subnet
<ul class="org-ul">
<li>I wanted to be able to use cloud VMs for other services, so I found a provider that suported private networking</li>
</ul></li>
<li>Wireguard speaks IP, so it needs a subnet too. It got <code>172.16.255.xxx</code>
<ul class="org-ul">
<li>Each device connected to the VPN gets an IP on this subnet</li>
</ul></li>
</ul></li>
<li>Use one private, made-up subdomain for each "region." For example, you could use <code>apartment.me.com</code>, <code>cloud.me.com</code>, and <code>vpn.me.com</code>.
<ul class="org-ul">
<li>I'm using a private subdomains of a domain that I control to ensure that I never clash with anything that actually exists.</li>
</ul></li>
<li>Have a DHCP server and DNS server available in each region (the DHCP host will always be <code>xxx.xxx.xxx.1</code> and have hostname/dns name <code>gateway</code>)
<ul class="org-ul">
<li>DNS is authoritative for its subdomain</li>
<li>VPN region just routes DNS to the cloud region</li>
<li>The VPN <code>gateway</code> is just the VPN server</li>
<li>There is no DHCP for VPN subnet since all VPN clients wil have a static IP</li>
</ul></li>
<li>The VPN server/cloud gateway has a public IP
<ul class="org-ul">
<li>Probably also want a firewall that drops everything other than VPN traffic (I'm using cloud provider firewall and a firewall on the server)</li>
</ul></li>
</ul>

<p>
I haven't figured out ipv6 yet because I'm a bad person.
</p>

<p>
The <code>172.16.xxx.xxx</code> prefix was selected to try and avoid conflicting with commercial subnets (<code>10.xxx.xxx.xxx</code>) and common private subnets (<code>192.168.xxx.xxx</code>).
The entire <code>172.16.0.0/12</code> subnet is private, so we can do whatever we want in this range.
Unfortunately, a local coffee shop I frequent uses an IP range that clashes with mine, so I have to get clever with routing rules when I am working there.
</p>
</div>
</div>

<div id="outline-container-org1024767" class="outline-3">
<h3 id="org1024767">Install <a href="https://alpinelinux.org/">alpine linux</a> on local router</h3>
<div class="outline-text-3" id="text-org1024767">
<p>
The official install guides are good and more up to date than anything I'd have to say about this.
</p>
</div>

<div id="outline-container-orgf51c6b8" class="outline-4">
<h4 id="orgf51c6b8">Setup Basic Networking</h4>
<div class="outline-text-4" id="text-orgf51c6b8">
<p>
In <code>/etc/network/interfaces</code>:
</p>

<div class="org-src-container">
<pre class="src src-conf">auto eth0
iface eth0 inet dhcp
        hostname gateway

auto eth1
iface eth1 inet static
        address 172.16.1.1
        netmask 255.255.255.0
</pre>
</div>

<p>
<code>eth0</code> is hooked to my ISP (so I get a DHCP ip), and <code>eth1</code> was hooked to a tiny TP-Link switch I had laying around.
</p>
</div>
</div>

<div id="outline-container-org7e01b5b" class="outline-4">
<h4 id="org7e01b5b">Firewall and NAT</h4>
<div class="outline-text-4" id="text-org7e01b5b">
<p>
After spending time banging my head against the <code>iptable</code>, I gave up and tried using a this thing built into alpine called <a href="https://wiki.alpinelinux.org/wiki/How-To_Alpine_Wall">awall</a>.
There's a pretty good <a href="https://wiki.alpinelinux.org/wiki/Zero-To-Awall">Zero to Awall</a> guide available which can get you started.
</p>

<p>
I don't want to explain a full example (the docs are again better than what I can do), but here's some highlights.
</p>
</div>

<ul class="org-ul">
<li><a id="org29fea5c"></a>Enable packet forwarding<br>
<div class="outline-text-5" id="text-org29fea5c">
<p>
The linux kernel must be told that it is allowed to forward packets.
Put <code>net.ipv4.ip_forward = 1</code> in a <code>sysctl.conf</code> file on alpine, see <a href="https://wiki.alpinelinux.org/wiki/Sysctl.conf">https://wiki.alpinelinux.org/wiki/Sysctl.conf</a>
This is probably needed for ipv6 as well, if you aren't a bad person who is ignoring ipv6, like me
</p>
</div>
</li>

<li><a id="org5d3de45"></a>Most (all?) for awalls config files can be written in <code>yaml</code><br>
<div class="outline-text-5" id="text-org5d3de45">
<p>
The Zero To Awall guide has this example:
</p>

<p>
<code>/etc/awall/private/custom-services.json</code>:
</p>

<div class="org-src-container">
<pre class="src src-json">{
    "service": {
	"openvpn": [
	    { "proto": "udp", "port": 1194 },
	    { "proto": "tcp", "port": 1194 }
	]
    }
}
</pre>
</div>

<p>
But, you could also create an equivelent <code>/etc/awall/private/custom-services.yaml</code> if you want:
</p>
<div class="org-src-container">
<pre class="src src-yaml">service:
  openvpn:
    - { proto: udp, port: 1194 }
    - { proto: tcp, port: 1194 }
</pre>
</div>
</div>
</li>
</ul>
</div>

<div id="outline-container-orgad74c9b" class="outline-4">
<h4 id="orgad74c9b">Tricks</h4>
<div class="outline-text-4" id="text-orgad74c9b">
<p>
In case the internet every goes down, I sometimes need to refresh my ISP DHCP lease to get it to come back up.
I stuck a <code>checkinit.sh</code> script into my <code>$PATH</code> somewhere, then added it to <code>cron</code> to run once a minute:
</p>

<div class="org-src-container">
<pre class="src src-bash">gateway:~# crontab -l
<span class="org-comment-delimiter"># </span><span class="org-comment">min   hour    day     month   weekday command</span>
*       *       *       *       *       checkinet.sh| logger -t checkinet

gateway:~# cat $(<span class="org-builtin">which</span> checkinet.sh)
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/sh</span>

<span class="org-builtin">echo</span> <span class="org-string">"Checking if internet still up"</span>

<span class="org-comment-delimiter"># </span><span class="org-comment">does not use our dns server, uses isp</span>
<span class="org-keyword">if</span> <span class="org-negation-char">!</span> ping -c5 google.com; <span class="org-keyword">then</span>
        <span class="org-builtin">echo</span> <span class="org-string">"bouncing network interface"</span>
        ifdown eth0
        ifup eth0
        <span class="org-comment-delimiter">#</span><span class="org-comment">unbound needed to be restarted, dnsmasq appears to be fine with this</span>
        <span class="org-comment-delimiter">#</span><span class="org-comment">sleep 30</span>
        <span class="org-comment-delimiter">#</span><span class="org-comment">/etc/init.d/unbound restart # idk why this needs to happen</span>
<span class="org-keyword">else</span>
        <span class="org-builtin">echo</span> <span class="org-string">"Internet still up!"</span>
<span class="org-keyword">fi</span>
</pre>
</div>

<p>
This is really only testing if I can resolve <code>google.com</code>, since ping will probably work if I can reach DNS to resolve google, but whatever.
The script gets me back up and going if I unplug stuff or if my ISP flakes out for some reason (which has only happened twice ever, this fixed it the second time), and it's never killed my internet spuriously, so I guess it works?
</p>

<p>
I also:
</p>
<ul class="org-ul">
<li>Cranked up the syslog file size and max files to keep around by editing an init file (probably the wrong way to do it)</li>
<li>Installed the S.M.A.R.T. tools (since there's an SSD in the thing)</li>
<li>Created a cron job to run smart tests sometimes and log it somewhere (which I've never looked at)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgc077907" class="outline-3">
<h3 id="orgc077907">Setup <a href="http://www.thekelleys.org.uk/dnsmasq/doc.html">dnsmasq</a> as a DHCP server and DNS server</h3>
<div class="outline-text-3" id="text-orgc077907">
<p>
The arch wiki has wonderful docs for this. Just go read those.
</p>

<p>
All I really had to do in the end was:
</p>
<ul class="org-ul">
<li>Turn on DHCP and DNS servers
<ul class="org-ul">
<li>Enable <code>dhcp-authoritative</code></li>
<li>Provide useful defaults to connected clients: <code>dhcp-option=option:router,172.16.1.1</code></li>
</ul></li>
<li>Tell dnsmasq what interfaces to listen on and from where to allow DNS queries</li>
<li>Tell dnsmasq which domain it is going to be authoritative for
<ul class="org-ul">
<li><code>domain=&lt;whatever&gt;.me.com</code> and <code>local=/&lt;whatever&gt;.me.com/</code></li>
</ul></li>
<li>Configure dnsmasq to resolve gateway.&lt;whatever&gt;.me.com to the <code>172.16.1.1</code> host
<ul class="org-ul">
<li>Create a file called <code>/etc/hosts.dnsmasq</code> with the only the line <code>172.16.1.1	gateway</code></li>
<li>Tell dnsmasq not to read the <code>/etc/hosts</code> file with the <code>no-hosts</code> configuration option</li>
<li>Then, give dnsmasq the configuration <code>addn-hosts=/etc/hosts.dnsmasq</code></li>
<li>This way, the local networking does not have to be tainted by anything I might want a fixed IP for.</li>
</ul></li>
<li>Log a lot
<ul class="org-ul">
<li><code>dhcp-script=/bin/echo</code>, <code>log-queries</code>, and <code>log-dhcp</code></li>
</ul></li>
</ul>
</div>

<div id="outline-container-org05432b8" class="outline-4">
<h4 id="org05432b8">Download <a href="https://pi-hole.net/">pi-hole</a>'s ad domain blacklist</h4>
<div class="outline-text-4" id="text-org05432b8">
<p>
From <a href="https://github.com/notracking/hosts-blocklists">https://github.com/notracking/hosts-blocklists</a>.
Put the tracking domain lists somewhere then just set:
</p>

<pre class="example">
conf-file=/path/to/domains.txt
addn-hosts=/path/to/hostnames.txt
</pre>

<p>
In the dnsmasq config file.
See the dnsmasq docs for an explanation of the difference.
</p>
</div>
</div>
</div>

<div id="outline-container-orgadfcaaa" class="outline-3">
<h3 id="orgadfcaaa">Pay for and plug in some sort of Wireless Access Point</h3>
<div class="outline-text-3" id="text-orgadfcaaa">
<p>
I bought a Unifi AP and followed the instructions to set it up.
It works.
</p>
</div>
</div>

<div id="outline-container-org4cffa43" class="outline-3">
<h3 id="org4cffa43">Setup alpine and DNS on a cloud server somewhere</h3>
<div class="outline-text-3" id="text-org4cffa43">
<p>
Same as above mostly, just with a different made-up star trek themed subdomain.
</p>
</div>
</div>

<div id="outline-container-org9a3fe59" class="outline-3">
<h3 id="org9a3fe59">Wireguard</h3>
<div class="outline-text-3" id="text-org9a3fe59">
<p>
Each device that can connect to the server needs a private/public key pair.
The server contains a list of recognized public keys; only the devices in the server config can connect.
</p>

<p>
There's a wireguard-tooling package available that you can use to generate keys.
Generate keys for each device (including the server):
</p>

<div class="org-src-container">
<pre class="src src-bash">$ umask 077 <span class="org-comment-delimiter"># </span><span class="org-comment">make sure no one can read your files</span>
$ wg genkey | tee private_key | wg pubkey &gt; public_key
$ ls
private_key public_key
</pre>
</div>

<p>
Once you are done copying the contents of these files into the wireguard configs, delete them.
</p>
</div>

<div id="outline-container-org2632bc4" class="outline-4">
<h4 id="org2632bc4">On the VPN server (cloud instance)</h4>
<div class="outline-text-4" id="text-org2632bc4">
<p>
Create a wireguard server config at <code>/etc/wireguard/wg0.conf</code>.
Note that I am not using the <code>wg-quick</code> interface for this or the apartment router.
</p>

<div class="org-src-container">
<pre class="src src-conf">gateway:~<span class="org-comment-delimiter"># </span><span class="org-comment">cat /etc/wireguard/wg0.conf</span>
[<span class="org-type">Interface</span>]
<span class="org-variable-name">PrivateKey</span> = ..... <span class="org-comment-delimiter"># </span><span class="org-comment">put the contents of the private key file here</span>
<span class="org-variable-name">ListenPort</span> = .... <span class="org-comment-delimiter"># </span><span class="org-comment">51820 seems to be standard port</span>

<span class="org-comment-delimiter"># </span><span class="org-comment">For each device that can connect to the VPN, create a [Peer] block</span>

<span class="org-comment-delimiter"># </span><span class="org-comment">gateway router in apartment</span>
[<span class="org-type">Peer</span>]
<span class="org-variable-name">PublicKey</span> = ..... <span class="org-comment-delimiter"># </span><span class="org-comment">put the contents of the public key file here</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">The AllowedIPs list is sort of like a routing table</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">In this section, we specify which IPs may be reached by directing traffic to this peer.</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">For the apartment router:</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">- assign the VPN IP: 172.16.255.2 and</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">- allow wireguard to route traffic from the VPN subnet to the 172.16.1.0/24 using this peer</span>
<span class="org-variable-name">AllowedIPs</span> = 172.16.255.2/32, 172.16.1.0/24

<span class="org-comment-delimiter"># </span><span class="org-comment">laptop</span>
[<span class="org-type">Peer</span>]
<span class="org-variable-name">PublicKey</span> = ..... <span class="org-comment-delimiter"># </span><span class="org-comment">put the contents of the public key file here</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">laptop is assigned a static ip.</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">this static ip is the only thing I'm allowing the VPN network to access</span>
<span class="org-variable-name">AllowedIps</span> = 172.16.255.3/32

<span class="org-comment-delimiter"># </span><span class="org-comment">.... more peers here</span>

</pre>
</div>

<p>
Next, configure kernel's networking stack:
</p>
<ol class="org-ol">
<li>create a new interface named <code>wg0</code></li>
<li>use the <code>wg</code> tool to set the interface config file</li>
<li>set a static ip/netmask for this interface/subnet</li>
<li>Add a routing table entry to route traffic from the cloud subnet to the apartment subnet over the <code>wg0</code> interface</li>
</ol>

<p>
This is done on alpine by adding more stuff to <code>/etc/network/interfaces</code>:
</p>

<div class="org-src-container">
<pre class="src src-conf">auto wg0
iface wg0 inet static
        address 172.16.255.1
        netmask 255.255.255.0
        pre-up ip link add dev wg0 type wireguard
        pre-up wg setconf wg0 /etc/wireguard/wg0.conf
        post-up ip route add 172.16.1.0/24 dev wg0
        post-down ip link delete wg0
</pre>
</div>
</div>
</div>

<div id="outline-container-org5de5aae" class="outline-4">
<h4 id="org5de5aae">On the apartment <code>gateway</code></h4>
<div class="outline-text-4" id="text-org5de5aae">
<p>
The router in my apartment is a VPN client, maintaining a persistent connection to the VPN server.
</p>

<p>
In <code>/etc/wireguard/wg0.conf</code> put something like:
</p>
<div class="org-src-container">
<pre class="src src-conf">[<span class="org-type">Interface</span>]
<span class="org-variable-name">PrivateKey</span> = .... <span class="org-comment-delimiter"># </span><span class="org-comment">private key associated with this peer</span>

[<span class="org-type">Peer</span>]
<span class="org-variable-name">Endpoint</span> = &lt;public ip of VPN server&gt;:&lt;port of VPN server&gt;
<span class="org-variable-name">PublicKey</span> = ...... <span class="org-comment-delimiter"># </span><span class="org-comment">public key goes here</span>
<span class="org-variable-name">PersistentKeepalive</span> = 25  <span class="org-comment-delimiter"># </span><span class="org-comment">keep the connection alive at all times</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">Allow the apartment router to route traffic into:</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">- VPN subnet</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">- cloud subnet</span>
<span class="org-variable-name">AllowedIPs</span> = 172.16.255.0/24, 172.16.2.0/24
</pre>
</div>

<p>
Create the new interface in <code>/etc/network/interfaces</code>:
</p>

<div class="org-src-container">
<pre class="src src-conf">auto wg0
iface wg0 inet static
        address 172.16.255.2
        netmask 255.255.255.0
        pre-up ip link add dev wg0 type wireguard
        pre-up wg setconf wg0 /etc/wireguard/wg0.conf
        post-up ip route add 172.16.2.0/24 dev wg0
        post-down ip link delete wg0
</pre>
</div>
</div>
</div>

<div id="outline-container-org2a8edf9" class="outline-4">
<h4 id="org2a8edf9">On a "dynamic" VPN client</h4>
<div class="outline-text-4" id="text-org2a8edf9">
<p>
On machines like my laptop, I want to easily bring the VPN up and down.
This is easy to do with the <code>wg-quick</code> tool.
<code>wg-quick</code> allows you to add a few more entries to the config file.
When you run <code>wg-quick up wg0</code>, it will bring up the interface, configure routing, and PostUp/PostDown scripts.
</p>

<p>
Here's the config from my (arch linux/systemd) laptop:
</p>

<div class="org-src-container">
<pre class="src src-conf">[<span class="org-type">Interface</span>]
<span class="org-variable-name">Address</span> = 172.16.255.3/32
<span class="org-variable-name">PrivateKey</span> = .... <span class="org-comment-delimiter"># </span><span class="org-comment">private key for this device</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">After coming up, reconfigure my domain resolution.</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">I'm on the vpn subdomain now. I resolve DNS queries with the cloud region's DNS server</span>
<span class="org-variable-name">PostUp</span> = printf <span class="org-string">'domain vpn.me.com\nnameserver 172.16.2.1'</span> | resolvconf -a %i -m 0 -x
<span class="org-comment-delimiter"># </span><span class="org-comment">dnsmasq caches queries, so restart it to make sure the cache is clean</span>
<span class="org-variable-name">PostUp</span> = systemctl restart dnsmasq
<span class="org-comment-delimiter"># </span><span class="org-comment">on teardown, undo the DNS resolver tweaks</span>
<span class="org-variable-name">PostDown</span> = resolvconf -d %i

[<span class="org-type">Peer</span>]
<span class="org-variable-name">Endpoint</span> = &lt;server public ip&gt;:&lt;server public port&gt;
<span class="org-variable-name">PublicKey</span> = ...... <span class="org-comment-delimiter"># </span><span class="org-comment">public key for the server</span>
<span class="org-variable-name">PersistentKeepalive</span> = 25
<span class="org-comment-delimiter"># </span><span class="org-comment">Route *all traffic* through the VPN</span>
<span class="org-variable-name">AllowedIPs</span> = 0.0.0.0/0, ::/0
<span class="org-comment-delimiter"># </span><span class="org-comment">Alternatively, we could use a list like:</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">AllowedIPs = 172.16.255.0/24, 172.16.2.0/24, 172.16.1.0/24</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">to route only internal traffic through the VPN.</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">This list can be as precise as you need it to be.</span>
</pre>
</div>
</div>

<ul class="org-ul">
<li><a id="orga42a6c8"></a>Laptop Lid<br>
<div class="outline-text-5" id="text-orga42a6c8">
<p>
When my laptop lid closes, I kill the wireguard connection with a systemd unit file.
This seems to minimize confusion when I close my laptop and take it somewhere.
</p>

<p>
In <code>/etc/systemd/system/wg-down.service</code>:
</p>
<div class="org-src-container">
<pre class="src src-conf">[<span class="org-type">Unit</span>]
<span class="org-variable-name">Description</span>=Kill wg when machine goes to sleep
<span class="org-variable-name">After</span>=suspend.target

[<span class="org-type">Service</span>]
<span class="org-variable-name">Type</span>=oneshot
<span class="org-variable-name">ExecStart</span>=sh -c <span class="org-string">'(ip link show wg0 &amp;&amp; wg-quick down wg0) || true'</span>

[<span class="org-type">Install</span>]
<span class="org-variable-name">WantedBy</span>=suspend.target
</pre>
</div>
</div>
</li>
</ul>
</div>
</div>

<div id="outline-container-org29ee92f" class="outline-3">
<h3 id="org29ee92f">Tweak dnsmasq config again</h3>
<div class="outline-text-3" id="text-org29ee92f">
<p>
Make sure that the DNS servers know how to send queries to each other:
</p>

<p>
In the apt.me.com dnsmasq config:
</p>
<div class="org-src-container">
<pre class="src src-conf"><span class="org-comment-delimiter"># </span><span class="org-comment">Add other name servers here, with domain specs if they are for</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">non-public domains.</span>
<span class="org-variable-name">server</span>=/cloud.me.com/172.16.2.1
<span class="org-variable-name">server</span>=/2.16.172.in-addr.arpa/172.16.2.1
</pre>
</div>

<p>
In the cloud.me.com dnsmasq config:
</p>
<div class="org-src-container">
<pre class="src src-conf"><span class="org-comment-delimiter"># </span><span class="org-comment">Add other name servers here, with domain specs if they are for</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">non-public domains.</span>
<span class="org-variable-name">server</span>=/apt.me.com/172.16.1.1
<span class="org-variable-name">server</span>=/1.16.172.in-addr.arpa/172.16.1.1

<span class="org-comment-delimiter"># </span><span class="org-comment">Allow VPN to use the cloud-region's DNS server</span>
<span class="org-variable-name">server</span>=172.16.2.1@wg0
</pre>
</div>
</div>
</div>

<div id="outline-container-org4b7fc70" class="outline-3">
<h3 id="org4b7fc70">Plug it all in</h3>
<div class="outline-text-3" id="text-org4b7fc70">
<p>
I plugged the new router box into the wall (on port 0), and plugged a small 4-port TP-link switch into port 1.
Everything else is plugged into the TP-link switch.
</p>
</div>
</div>
</div>

<div id="outline-container-orgf2b9bc5" class="outline-2">
<h2 id="orgf2b9bc5">Finally: use the system</h2>
<div class="outline-text-2" id="text-orgf2b9bc5">
</div>
<div id="outline-container-org3e088c5" class="outline-3">
<h3 id="org3e088c5">Good</h3>
<div class="outline-text-3" id="text-org3e088c5">
<ul class="org-ul">
<li>Wireguard is rock solid, even on my phone and from an airplane.</li>
<li>My local network performance is incredible</li>
<li>The tracker block lists noticeably effect load times for some sites</li>
<li>The latency/bandwidth I get back to my apartment is low/high, from everywhere I've been in Chicago</li>
</ul>
</div>
</div>
<div id="outline-container-org76c96e1" class="outline-3">
<h3 id="org76c96e1">Bad</h3>
<div class="outline-text-3" id="text-org76c96e1">
<ul class="org-ul">
<li>DHCP lease refreshes are slow for me right now
<ul class="org-ul">
<li>When my lease expires, sometimes I'll see connectivity blips</li>
</ul></li>
<li>The latency/bandwidth I get from when connecting to my apartment or cloud instance in Chicago from somewhere like Florida seems poor
<ul class="org-ul">
<li>This is probably an issue with my choice of cloud vendor</li>
</ul></li>
<li>The tracker block lists break lots of things, which is sometimes annoying
<ul class="org-ul">
<li>Many tracker links are broken (emailed, google sponsored, etc)</li>
<li>Facebook behaves strangely</li>
</ul></li>
<li>I haven't setup ipv6</li>
</ul>

<p>
Overall, I'm extremely happy with how this turned out.
</p>
</div>
</div>
</div>
]]>
    </content>
  </entry>
  <entry>
    <title>Rust for C/C++ developers</title>
    <link rel="alternate" type="text/html" href="http://dpzmick.com/posts/2019-11-26-rust-tips.html"/>
    <id>urn:uuid:7814e93d-87c4-59b1-98b4-13d915025ed8</id>
    <updated>2019-11-26</updated>
    <content type="html">
      <![CDATA[<style>.org-src-container {white-space: pre; font-family: monospace }</style>
               <p>
List of c-induced misconceptions I had to clarify when learning rust:
</p>

<ul class="org-ul">
<li>move is memcpy (no move ctor): <a href="./2019-05-25-pin-rs-cpp.html">pin misconceptions</a></li>
<li>type-punning is (sort of) okay: <a href="https://www.reddit.com/r/rust/comments/e24ss9/typepunning_on_mut_u8/">reddit post</a></li>
</ul>
]]>
    </content>
  </entry>
  <entry>
    <title>Understaning Pin (for C and C++ Developers)</title>
    <link rel="alternate" type="text/html" href="http://dpzmick.com/posts/2019-05-25-pin-rs-cpp.html"/>
    <id>urn:uuid:28ebff63-6149-5341-9434-7c77e53e2786</id>
    <updated>2019-11-15</updated>
    <content type="html">
      <![CDATA[<style>.org-src-container {white-space: pre; font-family: monospace }</style>
               <p>
<a href="https://doc.rust-lang.org/std/pin/">Pin</a> is pretty important for Rust's recently-released <code>async.await</code>
features. I read the docs. I didn't get it<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>. This exercise is what it
took for me to understand why <code>Pin</code> is important.
</p>

<p>
Opening up the documentation, the page starts with a discussion about
<code>Unpin</code>. <code>Unpin</code> is weird. Basically, <code>Unpin</code> says "yeah I know this
is pinned but you are free to ignore that." My gut reaction to <code>Unpin</code>
was "why would you need this at all?" Doesn't this defeat the purpose
of <code>Pin</code>?  Why is everything <code>Unpin</code> by default??
</p>

<p>
Continuing on, there's a list of rules which must be adhered to in the
<code>unsafe</code> constructor for <code>Pin</code>. I found this constraint for types
which are <code>!Unpin</code> to be particularly mysterious:
</p>

<blockquote>
<p>
It must not be possible to obtain a <code>&amp;mut P::Target</code> and then move out
of that reference (using, for example <code>mem::swap</code>).
</p>
</blockquote>

<p>
Other guides to <code>Pin</code> also noted that calling <code>mem::replace</code>, which
also takes a mutable reference, cannot not be allowed.
</p>

<p>
Let's look at this again:
</p>

<blockquote>
<p>
It must not be possible to obtain a <code>&amp;mut P::Target</code> and then <b>move out</b>
of that reference (using, for example <code>mem::swap</code>).
</p>
</blockquote>

<p>
Clearly moving is significant here, what does that mean exactly, and
why is this such a big deal?
</p>

<div id="outline-container-orgd38e9c4" class="outline-2">
<h2 id="orgd38e9c4">C++</h2>
<div class="outline-text-2" id="text-orgd38e9c4">
<p>
I'm more familiar with C++ and my familiarity is probably where my
misunderstandings are coming from. Let's start by understanding what
it means to move something in C++.
</p>

<p>
Consider the following <code>struct</code>:
</p>

<div class="org-src-container">
<pre class="src src-cpp" id="org405be0b"><span class="org-keyword">struct</span> <span class="org-type">Thing</span> {
  <span class="org-function-name">Thing</span>(<span class="org-type">uint64_t</span> <span class="org-variable-name">id</span>)
    : id(id)
  { }

  <span class="org-comment-delimiter">// </span><span class="org-comment">The move constructor is only required to leave the object in a</span>
  <span class="org-comment-delimiter">// </span><span class="org-comment">well defined state</span>
  <span class="org-function-name">Thing</span>(<span class="org-type">Thing</span>&amp;&amp; <span class="org-variable-name">other</span>)
    : id(other.id)
  {
    other.id = 0;
  }

  <span class="org-type">Thing</span>&amp; <span class="org-keyword">operator</span><span class="org-function-name">=</span>(<span class="org-type">Thing</span>&amp;&amp; <span class="org-variable-name">other</span>)
  {
    id       = other.id;
    other.id = 0;
    <span class="org-keyword">return</span> *<span class="org-keyword">this</span>;
  }

  <span class="org-comment-delimiter">// </span><span class="org-comment">non-copyable for clarity</span>
  <span class="org-function-name">Thing</span>(<span class="org-type">Thing</span> <span class="org-keyword">const</span>&amp;)            = <span class="org-keyword">delete</span>;
  <span class="org-type">Thing</span>&amp; <span class="org-keyword">operator</span><span class="org-function-name">=</span>(<span class="org-type">Thing</span> <span class="org-keyword">const</span>&amp;) = <span class="org-keyword">delete</span>;

  <span class="org-type">uint64_t</span> <span class="org-variable-name">id</span>;
};
</pre>
</div>

<p>
C++ says that a move ctor must leave the object moved from in an
undefined, <b>but valid</b> state.
</p>

<div class="org-src-container">
<pre class="src src-cpp">

<span class="org-type">int</span> <span class="org-function-name">main</span>() {
  <span class="org-type">Thing</span> <span class="org-variable-name">a</span>(10);
  <span class="org-type">Thing</span> <span class="org-keyword">const</span>&amp; <span class="org-variable-name">ref</span> = a;

  <span class="org-type">Thing</span> <span class="org-variable-name">c</span> = <span class="org-constant">std</span>::move(a);      <span class="org-comment-delimiter">// </span><span class="org-comment">moves a, but leave in defined state</span>
  printf(<span class="org-string">"ref %zu\n"</span>, ref.id); <span class="org-comment-delimiter">// </span><span class="org-comment">prints 0</span>
}
</pre>
</div>

<p>
Next, consider this<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup> implementation of <code>swap</code> and it's usage:
</p>

<div class="org-src-container">
<pre class="src src-cpp">

<span class="org-keyword">template</span> &lt;<span class="org-keyword">typename</span> <span class="org-type">T</span>&gt;
<span class="org-type">void</span> <span class="org-function-name">swap</span>(<span class="org-type">T</span>&amp; <span class="org-variable-name">a</span>, <span class="org-type">T</span>&amp; <span class="org-variable-name">b</span>)
{
  <span class="org-type">T</span> <span class="org-variable-name">tmp</span> = <span class="org-constant">std</span>::move(a); <span class="org-comment-delimiter">// </span><span class="org-comment">lots of moves</span>
  a = <span class="org-constant">std</span>::move(b);     <span class="org-comment-delimiter">// </span><span class="org-comment">move again</span>
  b = <span class="org-constant">std</span>::move(tmp);   <span class="org-comment-delimiter">// </span><span class="org-comment">oh look, move again!</span>
}

<span class="org-type">int</span> <span class="org-function-name">main</span>() {
  <span class="org-type">Thing</span> <span class="org-variable-name">a</span>(1);
  <span class="org-type">Thing</span> <span class="org-variable-name">b</span>(2);

  <span class="org-type">Thing</span>&amp; <span class="org-variable-name">ref</span> = a;
  swap(a, b);
  printf(<span class="org-string">"ref %zu\n"</span>, ref.id); <span class="org-comment-delimiter">// </span><span class="org-comment">prints 2</span>
}
</pre>
</div>

<p>
As far as I know, this is totally valid C++. The reference is just a
pointer to some chunk of memory, and, all of the moves that we did are
defined to leave the moved-from object in a "valid" state (you might
just have to be careful with them).
</p>

<p>
Let's consider one last struct.
</p>

<div class="org-src-container">
<pre class="src src-cpp" id="org9eceb88"><span class="org-keyword">template</span> &lt;<span class="org-keyword">typename</span> <span class="org-type">T</span>, <span class="org-type">size_t</span> <span class="org-variable-name">N</span>&gt;
<span class="org-keyword">struct</span> <span class="org-type">ring_buffer</span> {
  <span class="org-constant">std</span>::<span class="org-type">array</span>&lt;<span class="org-type">T</span>, N+1&gt; <span class="org-variable-name">entries</span>; <span class="org-comment-delimiter">// </span><span class="org-comment">use one extra element for easy book-keeping</span>

  <span class="org-comment-delimiter">// </span><span class="org-comment">Store pointers. This is bad, there are better ways to make a ring</span>
  <span class="org-comment-delimiter">// </span><span class="org-comment">buffer, but the demonstration is useful.</span>
  <span class="org-type">T</span>* <span class="org-variable-name">head</span> = entries;
  <span class="org-type">T</span>* <span class="org-variable-name">tail</span> = head+1;

  <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
};
</pre>
</div>

<p>
<code>head</code> and <code>tail</code> both point to elements of entries.  C++ will
generate a default move constructor for us, but the default is just a
<code>memcpy</code>. If it runs, we'll end up with pointers that point into the
wrong array. We must write a custom move constructor.
</p>

<div class="org-src-container">
<pre class="src src-cpp"><span class="org-function-name">ring_buffer</span>(<span class="org-type">ring_buffer</span>&amp;&amp; <span class="org-variable-name">other</span>)
  : entries( <span class="org-constant">std</span>::move(other.entries) )
  , head( entries.data() + (other.head - other.entries.data())) <span class="org-comment-delimiter">// </span><span class="org-comment">adjust pointer</span>
  , tail( entries.data() + (other.tail - other.entries.data())) <span class="org-comment-delimiter">// </span><span class="org-comment">adjust pointer</span>
{
  other.head = other.entries.data();
  other.tail = other.head + 1;
}
</pre>
</div>

<p>
So, in C++, a <code>move</code> is just another user defined operation that you
can take advantage of in some special places.
</p>
</div>
</div>

<div id="outline-container-org0138094" class="outline-2">
<h2 id="org0138094">Rust</h2>
<div class="outline-text-2" id="text-org0138094">
<p>
Let's do the same exercises again in Rust, starting with the <code>Thing</code>
struct.
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-keyword">struct</span> <span class="org-type">Thing</span> {
    <span class="org-keyword">pub</span> <span class="org-variable-name">id</span>: <span class="org-type">u64</span>
}

<span class="org-keyword">impl</span> <span class="org-type">Thing</span> {
    <span class="org-keyword">pub</span> <span class="org-keyword">fn</span> <span class="org-function-name">new</span>(<span class="org-variable-name">id</span>: <span class="org-type">u64</span>) -&gt; <span class="org-type">Self</span> {
        <span class="org-type">Self</span> { id }
    }
}
</pre>
</div>

<p>
Trying to port the first example directly into Rust won't work.
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-keyword">fn</span> <span class="org-function-name">main</span>() {
    <span class="org-keyword">let</span> <span class="org-variable-name">a</span> = <span class="org-type">Thing</span>::new(10);
    <span class="org-keyword">let</span> <span class="org-variable-name">r</span> = &amp;a;

    <span class="org-keyword">let</span> <span class="org-variable-name">c</span> = a; <span class="org-comment-delimiter">// </span><span class="org-comment">this is a move, but won't compile</span>
    <span class="org-rust-builtin-formatting-macro">println!</span>(<span class="org-string">"ref </span><span class="org-rust-string-interpolation">{}</span><span class="org-string">"</span>, r.id);
}
</pre>
</div>

<p>
The compiler doesn't like this. It says:
</p>

<div class="org-src-container">
<pre class="src src-rust">error[<span class="org-type">E0505</span>]: cannot <span class="org-keyword">move</span> out of `a` because it is borrowed
  --&gt; ex1.<span class="org-variable-name">rs</span>:1<span class="org-variable-name">6</span>:13
   |
15 |     <span class="org-keyword">let</span> <span class="org-variable-name">r</span> = &amp;a;
   |             -- borrow of `a` occurs here
16 |     <span class="org-keyword">let</span> <span class="org-variable-name">c</span> = a; <span class="org-comment-delimiter">// </span><span class="org-comment">this is a move, but won't compile</span>
   |             ^ <span class="org-keyword">move</span> out of `a` occurs here
17 |
18 |     <span class="org-rust-builtin-formatting-macro">println!</span>(<span class="org-string">"ref </span><span class="org-rust-string-interpolation">{}</span><span class="org-string">"</span>, r.id);
   |                        ---- borrow later used here
</pre>
</div>

<p>
Rust is telling us that it knows we moved the value, and, since we
moved it, we can't use it anymore. What does this mean though? What is
actually going?
</p>

<p>
Let's try to find out with some unsafe and undefined-behavior inducing
Rust. The first time I tried something like this, I wasn't sure what
to expect, but hopefully this example is clear.
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-keyword">fn</span> <span class="org-function-name">main</span>() {
    <span class="org-keyword">let</span> <span class="org-variable-name">a</span> = <span class="org-type">Thing</span>::new(1);
    <span class="org-keyword">let</span> <span class="org-variable-name">r</span>: *<span class="org-keyword">const</span> <span class="org-type">Thing</span> = &amp;a;

    <span class="org-keyword">let</span> <span class="org-variable-name">c</span> = a;
    <span class="org-rust-builtin-formatting-macro">println!</span>(<span class="org-string">"ref </span><span class="org-rust-string-interpolation">{}</span><span class="org-string">"</span>, <span class="org-rust-unsafe">unsafe</span> { (*r).id });
}
</pre>
</div>

<p>
This prints "1" because the compiler reused the stack space used by
the object named <code>a</code> to store the object named <code>b</code>. There was no
"empty valid husk" left behind.
</p>

<p>
This behavior is very different from the C++ move. The Rust compiler
knows about the move and can take advantage of the move to save some
stack space. Without writing unsafe code, there is no way you'd ever
be able to access fields from <code>a</code> again, so how the compiler wants to
use that space occupied by <code>a</code> after the move is entirely the
compiler's decision.
</p>

<p>
Rule number 1 of Rust move: The compiler knows you moved. The compiler
can use this to optimize.
</p>

<p>
The next C++ example was a <code>swap</code>. In C++, <code>swap</code> calls some move
constructors to shuffle the data around. In the C++ swap example,
these (implicit) <code>move</code> constructors where just <code>memcpy</code>.
</p>

<p>
Swap in Rust isn't as straightforward as the C++ version. In the C++
version, we just call the user defined move constructor to do all of
the hard work. In Rust, we don't have this user defined function to
call, so we'll have to actually be explicit about what swap does.
This version of swap is adapted from Rust's standard library:
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-keyword">fn</span> <span class="org-function-name">swap</span>&lt;<span class="org-type">T</span>&gt;(<span class="org-variable-name">a</span>: &amp;<span class="org-keyword">mut</span> <span class="org-type">T</span>, <span class="org-variable-name">b</span>: &amp;<span class="org-keyword">mut</span> <span class="org-type">T</span>) {
    <span class="org-comment-delimiter">// </span><span class="org-comment">a and b are both valid pointers</span>
    <span class="org-rust-unsafe">unsafe</span> {
        <span class="org-keyword">let</span> <span class="org-variable-name">tmp</span>: <span class="org-type">T</span> = <span class="org-constant">std</span>::<span class="org-constant">ptr</span>::read(a); <span class="org-comment-delimiter">// </span><span class="org-comment">memcpy</span>
        <span class="org-constant">std</span>::<span class="org-constant">ptr</span>::copy(b, a, 1);        <span class="org-comment-delimiter">// </span><span class="org-comment">memcpy</span>
        <span class="org-constant">std</span>::<span class="org-constant">ptr</span>::write(b, tmp);        <span class="org-comment-delimiter">// </span><span class="org-comment">memcpy</span>
    }
}
</pre>
</div>

<p>
Roaming again into undefined-behavior territory:
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-keyword">fn</span> <span class="org-function-name">main</span>() {
    <span class="org-keyword">let</span> <span class="org-keyword">mut</span> <span class="org-variable-name">a</span> = <span class="org-type">Thing</span>::new(1);
    <span class="org-keyword">let</span> <span class="org-keyword">mut</span> <span class="org-variable-name">b</span> = <span class="org-type">Thing</span>::new(2);

    <span class="org-keyword">let</span> <span class="org-variable-name">r</span>: *<span class="org-keyword">const</span> <span class="org-type">Thing</span> = &amp;a;

    swap(&amp;<span class="org-keyword">mut</span> a, &amp;<span class="org-keyword">mut</span> b);

    <span class="org-rust-builtin-formatting-macro">println!</span>(<span class="org-string">"</span><span class="org-rust-string-interpolation">{}</span><span class="org-string">"</span>, <span class="org-rust-unsafe">unsafe</span> { (*r).id }); <span class="org-comment-delimiter">// </span><span class="org-comment">prints 2</span>
}
</pre>
</div>

<p>
This example is nice because it does what you'd expect, but it
highlights something critical about Rust's move semantics: <code>move</code> is
<span class="underline">always</span> a <code>memcpy</code>. <code>move</code> in Rust couldn't be anything other than a
<code>memcpy</code>. Rust doesn't define anything else associated with the struct
that would let the user specify any other operation.
</p>

<p>
Rule number 2: Rust move is always <span class="underline">just</span> a <code>memcpy</code>.
</p>

<p>
Now, let's think about the ring buffer. It is not even remotely
idiomatic to write anything like the C++ version of the ring-buffer in
Rust<sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup>, but let's do it anyway. I'm also going to pretend that <a href="https://github.com/rust-lang/rust/issues/44580">const
generics</a> are finished for the sake of clarity.
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-keyword">struct</span> <span class="org-type">RingBuffer</span>&lt;<span class="org-type">T</span>, <span class="org-keyword">const</span> <span class="org-variable-name">N</span>: <span class="org-type">usize</span>&gt; {
    <span class="org-variable-name">entries</span>: [<span class="org-type">T</span>; <span class="org-type">N</span>+1],
    <span class="org-variable-name">head</span>: *<span class="org-keyword">const</span> <span class="org-type">T</span>,   <span class="org-comment-delimiter">// </span><span class="org-comment">next pop location, T is moved (memcpy) out</span>
    <span class="org-variable-name">tail</span>: *<span class="org-keyword">mut</span> <span class="org-type">T</span>,     <span class="org-comment-delimiter">// </span><span class="org-comment">next push location, T is moved (memcpy) in</span>
}
</pre>
</div>

<p>
The problem now is that we can't define a custom move constructor. If
this struct is ever moved (including the move-by-<code>memcpy</code> in
swap/replace), the pointers stored will be point to the wrong piece of
memory.
</p>

<p>
The rust solution to this is to mark your type as <code>!Unpin</code>.
</p>

<p>
Once something is marked as <code>!Unpin</code>, getting a mutable reference to
it becomes unsafe. If you get a mutable reference to a pinned type
which is <code>!Unpin</code>, you must promise to never call anything that moves
out of the type. I have thoughts on the actual feasibility of
following these rules, but that's a topic for another time.
</p>
</div>
</div>

<div id="outline-container-orgc03c284" class="outline-2">
<h2 id="orgc03c284">Futures/async.await</h2>
<div class="outline-text-2" id="text-orgc03c284">
<p>
Hopefully now, we can understand why this is prerequisite for
async.await support in Rust.
</p>

<p>
Consider this async function:
</p>
<div class="org-src-container">
<pre class="src src-rust"><span class="org-keyword">async</span> <span class="org-keyword">fn</span> <span class="org-function-name">foo</span>() -&gt; <span class="org-type">u32</span> {
    <span class="org-comment-delimiter">// </span><span class="org-comment">First call to poll runs until the line with the await</span>
    <span class="org-keyword">let</span> <span class="org-variable-name">x</span> = [1, 2, 3, 4];
    <span class="org-keyword">let</span> <span class="org-variable-name">y</span> = &amp;x[1];
    <span class="org-keyword">let</span> nxt_idx= make_network_request().<span class="org-keyword">await</span>;

    <span class="org-comment-delimiter">// </span><span class="org-comment">next call to poll runs the last line</span>
    <span class="org-keyword">return</span> y + x[nxt_idx];
}
</pre>
</div>

<p>
The compiler will roughly translate this function into a state machine
with 2 states. That state machine is represented by some struct, and
the state is updated by calling the <code>poll</code> function. The struct used
to store the data for this state machine will look something like
this:
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-keyword">struct</span> <span class="org-type">StateMachineData_State1</span> {
    <span class="org-variable-name">x</span>: [<span class="org-type">u32</span>, 4],
    <span class="org-variable-name">y</span>: &amp;<span class="org-type">u32</span>,      <span class="org-comment-delimiter">// </span><span class="org-comment">ignore lifetime. This will point into `x`</span>
}
</pre>
</div>

<p>
Since <code>y</code> is a reference (pointer), if we <code>move</code> (<code>memcpy</code>) the
intermediate state, we'll be messing up our pointers. This is why
<code>Pin</code> matters for async.
</p>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
The docs are great, but something just wasn't clicking for me.
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
probably incorrect. there's no such thing as correct C++ code.
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
The idiomatic Rust version is better (use indicies instead of pointers).
This isn't an indictment of Rust. The idiomatic Rust version would be
better in C++ too.
</p></div></div>


</div>
</div>]]>
    </content>
  </entry>
  <entry>
    <title>Outsmarting the Compiler</title>
    <link rel="alternate" type="text/html" href="http://dpzmick.com/posts/2017-05-25-outsmarting-the-compiler.html"/>
    <id>urn:uuid:9d49df50-5144-586c-9e6c-e357181194a3</id>
    <updated>2017-05-25</updated>
    <content type="html">
      <![CDATA[<style>.org-src-container {white-space: pre; font-family: monospace }</style>
               <p>
Suppose we have two very similar structs which we need to partially populate "ahead of time" and store somewhere.
Then, a bit later, we need to <b><b>very quickly</b></b> finish populating the structs.
Here are some example structs:
</p>

<div class="org-src-container">
<pre class="src src-cpp"><span class="org-keyword">struct</span> <span class="org-keyword">__attribute__</span>((packed)) <span class="org-type">A</span> {
  int64_t a;
  <span class="org-type">int64_t</span> <span class="org-variable-name">b</span>;
  <span class="org-type">char</span>    <span class="org-variable-name">arr</span>[PADDING1];
  <span class="org-type">int64_t</span> <span class="org-variable-name">c</span>;
};

<span class="org-keyword">struct</span> <span class="org-keyword">__attribute__</span>((packed)) <span class="org-type">B</span> {
  int64_t a;
  <span class="org-type">int64_t</span> <span class="org-variable-name">b</span>;
  <span class="org-type">char</span>    <span class="org-variable-name">arr</span>[PADDING2];
  <span class="org-type">int64_t</span> <span class="org-variable-name">c</span>;
};
</pre>
</div>

<p>
The "padding" arrays are populated ahead of time, so we just need to set <code>a</code>, <code>b</code>, and <code>c</code> for each struct (quickly):
</p>

<div class="org-src-container">
<pre class="src src-cpp"><span class="org-keyword">template</span> &lt;<span class="org-keyword">typename</span> <span class="org-type">T</span>&gt;
<span class="org-type">void</span> <span class="org-function-name">writeFields</span>(<span class="org-type">T</span>* <span class="org-variable-name">t</span>)
{
  t-&gt;a = 12;
  t-&gt;b = 25;
  t-&gt;c = 16;
}
</pre>
</div>

<p>
Unfortunately, we don't statically know what struct we are going to have to operate on; we only get this information at runtime.
We just have a blob of memory and a tag which indicates which of the two variants of the struct is sitting in the blob of memory:
</p>

<div class="org-src-container">
<pre class="src src-cpp"><span class="org-keyword">enum</span> <span class="org-keyword">class</span> <span class="org-type">Variant</span> { <span class="org-variable-name">eA</span>, <span class="org-variable-name">eB</span> };

<span class="org-keyword">struct</span> <span class="org-type">Wrapper</span> {
  <span class="org-type">Variant</span> <span class="org-variable-name">v</span>;
  <span class="org-type">char</span> <span class="org-variable-name">payload</span>[];
};
</pre>
</div>

<p>
So, our fast path <code>write</code> function will need to take a wrapper struct, switch on the tag, then call the appropriate version of <code>writeFields</code>:
</p>

<div class="org-src-container">
<pre class="src src-cpp"><span class="org-type">void</span> <span class="org-function-name">write</span>(<span class="org-type">Wrapper</span>* <span class="org-variable-name">w</span>)
{
  <span class="org-keyword">if</span> (w-&gt;v == <span class="org-constant">Variant</span>::eA) {
    writeFields&lt;A&gt;(<span class="org-keyword">reinterpret_cast</span>&lt;<span class="org-type">A</span>*&gt;(w-&gt;payload));
  }
  <span class="org-keyword">else</span> {
    writeFields&lt;B&gt;(<span class="org-keyword">reinterpret_cast</span>&lt;<span class="org-type">B</span>*&gt;(w-&gt;payload));
  }
}
</pre>
</div>

<p>
If <code>PADDING1 =</code> PADDING2=, then, regardless of the value of the tag (which struct we are populating), we will need to write to the same offsets.
The cast and the templated function call will all compile out.
Take a look (<code>clang-4.0 --std=c++1z -O3</code>):
</p>

<div class="org-src-container">
<pre class="src src-asm"><span class="org-function-name">.LCPI2_0</span>:
        <span class="org-keyword">.quad</span>   12                      # 0xc
        <span class="org-keyword">.quad</span>   25                      # 0x19
<span class="org-function-name">write</span>(Wrapper*):                      # @write(Wrapper*)
        <span class="org-keyword">movaps</span>  xmm0, xmmword ptr [rip + .LCPI2_0] # xmm0 = [12,25]
        <span class="org-keyword">movups</span>  xmmword ptr [rdi + 4], xmm0
        <span class="org-keyword">mov</span>     qword ptr [rdi + 36], 16
        <span class="org-keyword">ret</span>
</pre>
</div>

<p>
Before we move on, take a moment to appreciate what your compiler just did for you:
</p>
<ol class="org-ol">
<li>It allowed you to write a type safe <code>writeFields</code> method. If the layout of the struct changes for some reason, this part of the code will not begin to misbehave.</li>
<li>It removed the cost of the abstraction when it could figure out how to.</li>
</ol>

<p>
Unfortunately, if <code>PADDING1 !</code> PADDING2=, we will need to write the value of <code>c</code> in a different location in struct <code>A</code> and struct <code>B</code>.
In this case, it looks like we will need read the tag out of the <code>Wrapper*</code>, then branch to the appropriate <code>writeFields</code> method.
We are good programmers, we know that branches might be expensive, so we really want avoid any branching.
</p>

<p>
We can skip the branch by storing the offset in our wrapper struct and precomputing the offset when the wrapper is set up.
Introduce a new wrapper type (and abandon all type safety):
</p>

<div class="org-src-container">
<pre class="src src-cpp"><span class="org-keyword">struct</span> <span class="org-type">WrapperWithOffset</span> {
  <span class="org-type">Variant</span> <span class="org-variable-name">v</span>;
  <span class="org-type">size_t</span> <span class="org-variable-name">offset</span>;
  <span class="org-type">char</span> <span class="org-variable-name">payload</span>[];
};
</pre>
</div>

<p>
Next, we can write a new function which will operate on structs of type <code>A</code> or type <code>B</code>, but, instead of writing to <code>c</code> directly, it computes a pointer to <code>c</code> using the offset we've stored in the wrapper, then writes to that pointer.
</p>

<div class="org-src-container">
<pre class="src src-cpp"><span class="org-type">void</span> <span class="org-function-name">writeFieldsWithOffset</span>(<span class="org-type">A</span>* <span class="org-variable-name">t</span>, <span class="org-type">size_t</span> <span class="org-variable-name">c_offset</span>)
{
  <span class="org-comment-delimiter">// </span><span class="org-comment">make sure a and b are always at the same offset in struct A and struct B</span>
  static_assert(offsetof(A, a) == offsetof(B, a), <span class="org-string">"!"</span>);
  static_assert(offsetof(A, b) == offsetof(B, b), <span class="org-string">"!"</span>);

  t-&gt;a = 12;
  t-&gt;b = 25;

  <span class="org-comment-delimiter">// </span><span class="org-comment">c will be at the offset we've provided</span>
  *(<span class="org-type">int64_t</span>*)(((<span class="org-type">char</span>*)t + c_offset)) = 16;
}

<span class="org-type">void</span> <span class="org-function-name">writeLessSafe</span>(<span class="org-type">WrapperWithOffset</span>* <span class="org-variable-name">w</span>)
{
  <span class="org-type">A</span>* <span class="org-variable-name">a</span> = <span class="org-keyword">reinterpret_cast</span>&lt;<span class="org-type">A</span>*&gt;(w-&gt;payload);
  writeFieldsWithOffset(a, w-&gt;offset);
}
</pre>
</div>

<p>
Checking the code, this compiles down to exactly what we were hoping it would (again with clang-4.0)!
</p>

<div class="org-src-container">
<pre class="src src-asm"><span class="org-function-name">.LCPI1_0</span>:
        <span class="org-keyword">.quad</span>   12                      # 0xc
        <span class="org-keyword">.quad</span>   25                      # 0x19
<span class="org-function-name">writeLessSafe</span>(WrapperWithOffset*): # @writeLessSafe(WrapperWithOffset*)
        <span class="org-keyword">mov</span>     rax, qword ptr [rdi + 8]
        <span class="org-keyword">movaps</span>  xmm0, xmmword ptr [rip + .LCPI1_0] # xmm0 = [12,25]
        <span class="org-keyword">movups</span>  xmmword ptr [rdi + 16], xmm0
        <span class="org-keyword">mov</span>     qword ptr [rdi + rax + 16], 16
        <span class="org-keyword">ret</span>
</pre>
</div>

<p>
Hooray, no conditional generated, exactly as we desired.
We've outsmarted the compiler!
</p>

<ul class="org-ul">
<li><a id="orgb5661c4"></a>Assertion Failed: smarter_than_compiler<br>
<div class="outline-text-5" id="text-orgb5661c4">
<p>
Let's set <code>PADDING1 = 16</code> and <code>PADDING2 = 17</code>.
The code generated on clang-4.0 for <code>write(Wrapper*)</code> looks quite interesting:
</p>

<div class="org-src-container">
<pre class="src src-asm"><span class="org-function-name">.LCPI2_0</span>:
        <span class="org-keyword">.quad</span>   12                      # 0xc
        <span class="org-keyword">.quad</span>   25                      # 0x19
<span class="org-function-name">write</span>(Wrapper*):                      # @write(Wrapper*)
        <span class="org-keyword">xor</span>     eax, eax
        <span class="org-keyword">cmp</span>     dword ptr [rdi], 0
        <span class="org-keyword">movaps</span>  xmm0, xmmword ptr [rip + .LCPI2_0] # xmm0 = [12,25]
        <span class="org-keyword">movups</span>  xmmword ptr [rdi + 4], xmm0
        <span class="org-keyword">setne</span>   al
        <span class="org-keyword">mov</span>     qword ptr [rdi + rax + 36], 16
        <span class="org-keyword">ret</span>
</pre>
</div>

<p>
This code is still very slightly longer than the unsafe code written previously, but, its really not bad at all.
</p>

<p>
The compiler has succeeded in avoiding a branch using a rather clever <code>cmp</code> and <code>setne</code> instruction pair.
Essentially, clang figured out that it could compute the offset of <code>c</code> using the tag we've placed in the <code>Wrapper</code>'s <code>Variant</code> field.
In this case, I've allowed the enum values to default to \(0\) and \(1\) (hence the <code>cmp dword ptr [rdi], 0</code> checking if the first thing in the functions first arg is equal to \(0\)).
</p>

<p>
What happens if we change the values?
</p>

<div class="org-src-container">
<pre class="src src-cpp"><span class="org-keyword">enum</span> <span class="org-keyword">class</span> <span class="org-type">Variant</span> { <span class="org-variable-name">eA</span> = 666, <span class="org-variable-name">eB</span> = 1337 };
</pre>
</div>

<div class="org-src-container">
<pre class="src src-asm"><span class="org-function-name">.LCPI2_0</span>:
        <span class="org-keyword">.quad</span>   12                      # 0xc
        <span class="org-keyword">.quad</span>   25                      # 0x19
<span class="org-function-name">write</span>(Wrapper*):                      # @write(Wrapper*)
        <span class="org-keyword">mov</span>     eax, dword ptr [rdi]
        <span class="org-keyword">movaps</span>  xmm0, xmmword ptr [rip + .LCPI2_0] # xmm0 = [12,25]
        <span class="org-keyword">movups</span>  xmmword ptr [rdi + 4], xmm0
        <span class="org-keyword">xor</span>     ecx, ecx
        <span class="org-keyword">cmp</span>     eax, 666
        <span class="org-keyword">setne</span>   cl
        <span class="org-keyword">mov</span>     qword ptr [rdi + rcx + 36], 16
        <span class="org-keyword">ret</span>
</pre>
</div>

<p>
The code has changed slightly to account for the new potential values of <code>Wrapper::v</code>, but it looks much nicer than a branch.
</p>
</div>
</li>
</ul>

<div id="outline-container-org582f5fd" class="outline-4">
<h4 id="org582f5fd">Meaner PADDING</h4>
<div class="outline-text-4" id="text-org582f5fd">
<p>
Reminder: In the previous examples <code>PADDING1 = 16</code> and <code>PADDING2 = 17</code>.
What happens to the generated code if we make the paddings completely wacky?
</p>

<p>
With <code>PADDING1 = 16</code> and <code>PADDING2 = 173</code>, and with the enum values reverted to their defaults:
</p>

<div class="org-src-container">
<pre class="src src-cpp">.LCPI1_0:
        .quad   12                      # 0xc
        .quad   25                      # 0x19
writeLessSafe(<span class="org-type">WrapperWithOffset</span>*): # @writeLessSafe(<span class="org-type">WrapperWithOffset</span>*)
        <span class="org-type">mov</span>     <span class="org-variable-name">rax</span>, <span class="org-variable-name">qword</span> ptr [rdi + 8]
        movaps  xmm0, <span class="org-variable-name">xmmword</span> ptr [rip + .LCPI1_0] # xmm0 = [12,25]
        movups  xmmword ptr [rdi + 16], <span class="org-variable-name">xmm0</span>
        mov     qword ptr [rdi + rax + 16], 16
        ret

.LCPI2_0:
        .quad   12                      # 0xc
        .quad   25                      # 0x19
write(<span class="org-type">Wrapper</span>*):                      # @write(<span class="org-type">Wrapper</span>*)
        cmp     <span class="org-type">dword</span> <span class="org-variable-name">ptr</span> [rdi], 0
        movaps  xmm0, <span class="org-type">xmmword</span> <span class="org-variable-name">ptr</span> [rip + .LCPI2_0] # xmm0 = [12,25]
        movups  xmmword ptr [rdi + 4], xmm0
        <span class="org-type">mov</span>     <span class="org-variable-name">eax</span>, 32
        mov     ecx, 189
        cmove   rcx, rax
        <span class="org-type">mov</span>     <span class="org-type">qword</span> <span class="org-variable-name">ptr</span> [rdi + rcx + 4], 16
        ret
</pre>
</div>

<p>
<code>writeLessSafe</code> doesn't change, as expected.
<code>write</code> does get tweaked a bit to account for the new offsets, but its still pretty great code.
</p>

<p>
So, have we beaten the compiler?
The answer to that depends on which compiler you ask.
</p>
</div>
</div>

<div id="outline-container-orgebad7e2" class="outline-4">
<h4 id="orgebad7e2">gcc 7.1 (&#x2013;std=c++1z -O3)</h4>
<div class="outline-text-4" id="text-orgebad7e2">
</div>
<ul class="org-ul">
<li><a id="org998375e"></a><code>PADDING1</code> <code>= =PADDING2</code><br>
<div class="outline-text-5" id="text-org998375e">
<div class="org-src-container">
<pre class="src src-asm"><span class="org-function-name">writeLessSafe</span>(WrapperWithOffset*):
        <span class="org-keyword">mov</span>     rax, QWORD PTR [rdi+8]
        <span class="org-keyword">mov</span>     QWORD PTR [rdi+16], 12
        <span class="org-keyword">mov</span>     QWORD PTR [rdi+24], 25
        <span class="org-keyword">mov</span>     QWORD PTR [rdi+16+rax], 16
        <span class="org-keyword">ret</span>
<span class="org-function-name">write</span>(Wrapper*):
        <span class="org-keyword">mov</span>     eax, DWORD PTR [rdi]
        <span class="org-keyword">mov</span>     QWORD PTR [rdi+4], 12
        <span class="org-keyword">mov</span>     QWORD PTR [rdi+12], 25
        <span class="org-keyword">mov</span>     QWORD PTR [rdi+36], 16
        <span class="org-keyword">test</span>    eax, eax
        <span class="org-keyword">je</span>      .L7
        <span class="org-keyword">rep</span> ret
<span class="org-function-name">.L7</span>:
        <span class="org-keyword">rep</span> ret
</pre>
</div>

<p>
That's a little odd.
</p>
</div>
</li>

<li><a id="orgf315dea"></a><code>PADDING1 = 16</code> and <code>PADDING2 = 17</code><br>
<div class="outline-text-5" id="text-orgf315dea">
<div class="org-src-container">
<pre class="src src-asm"><span class="org-function-name">write</span>(Wrapper*):
        <span class="org-keyword">mov</span>     eax, DWORD PTR [rdi]
        <span class="org-keyword">mov</span>     QWORD PTR [rdi+4], 12
        <span class="org-keyword">mov</span>     QWORD PTR [rdi+12], 25
        <span class="org-keyword">test</span>    eax, eax
        <span class="org-keyword">je</span>      .L7
        <span class="org-keyword">mov</span>     QWORD PTR [rdi+37], 16
        <span class="org-keyword">ret</span>
<span class="org-function-name">.L7</span>:
        <span class="org-keyword">mov</span>     QWORD PTR [rdi+36], 16
        <span class="org-keyword">ret</span>
</pre>
</div>
</div>
</li>

<li><a id="org67b628c"></a><code>PADDING1 = 16</code> and <code>PADDING2 = 173</code><br>
<div class="outline-text-5" id="text-org67b628c">
<div class="org-src-container">
<pre class="src src-asm"><span class="org-function-name">write</span>(Wrapper*):
        <span class="org-keyword">mov</span>     eax, DWORD PTR [rdi]
        <span class="org-keyword">mov</span>     QWORD PTR [rdi+4], 12
        <span class="org-keyword">mov</span>     QWORD PTR [rdi+12], 25
        <span class="org-keyword">test</span>    eax, eax
        <span class="org-keyword">je</span>      .L7
        <span class="org-keyword">mov</span>     QWORD PTR [rdi+193], 16
        <span class="org-keyword">ret</span>
<span class="org-function-name">.L7</span>:
        <span class="org-keyword">mov</span>     QWORD PTR [rdi+36], 16
        <span class="org-keyword">ret</span>
</pre>
</div>

<p>
Interesting.
This branch felt <b>almost</b> detectable in some micro-benchmarks, but I would require additional testing before I'm willing to declare that it is harmful.
At the moment I'm not convinced that it hurts much.
</p>
</div>
</li>
</ul>
</div>

<div id="outline-container-org2aed0d4" class="outline-2">
<h2 id="org2aed0d4">Conclusion</h2>
<div class="outline-text-2" id="text-org2aed0d4">
<p>
No conclusion.
None of my benchmarks have managed to detect any convincing cost for this branch (even when variants are randomly chosen inside of a loop in an attempt to confuse branch predictor) so none of this actually matters (probably).
The only interesting fact my benchmarks showed is that clang 4.0 looked very very slightly faster than gcc 6.3, possibly because of the vector instructions clang is generating, but also possibly because benchmarking is hard and I'm not benchmarking on isolated cores.
Here's some code: <a href="https://gist.github.com/dpzmick/a8f937c5e35185092b6af9a5ed87a7b8">gist</a>.
</p>
</div>
</div>
]]>
    </content>
  </entry>
  <entry>
    <title>&quot;Garbage Collection&quot; for Rust Arc Pointers</title>
    <link rel="alternate" type="text/html" href="http://dpzmick.com/posts/2017-01-23-rust-arc-gc-realtime.html"/>
    <id>urn:uuid:d61c56fd-050b-51f6-bc7d-00386f4c5f1d</id>
    <updated>2017-01-23</updated>
    <content type="html">
      <![CDATA[<style>.org-src-container {white-space: pre; font-family: monospace }</style>
               <p>
If you don't know anything at all about realtime audio programming, you might want to read the first post in this pseudo-series, <a href="./2016-12-17-audio-basics.html">Audio Programming 101</a>, or watch <a href="https://www.youtube.com/watch?v=SJXGSJ6Zoro">this talk</a> from the Audio Developers Conference to get a little bit of background.
</p>

<p>
In short, there's a realtime thread that can never be blocked in any way.
The realtime thread is responsible for sending all of the audio which an application will produce to an audio system, at exactly the right moments.
If the realtime thread ever fails to generate the audio it needs to generate, bad things happen.
That means locks, I/O, allocation are all off limits in the realtime thread.
</p>

<p>
Sending messages from non-realtime threads to the realtime thread is trickier than it might be in a "normal" application because we can't do these things.
There are many, many techniques which can be used to work around this trickiness.
This post is a discussion of one such method (presented in <a href="https://www.youtube.com/watch?v=boPEO2auJj4">this cppcon talk</a>) implemented in <a href="https://www.rust-lang.org/en-US/">Rust</a>.
</p>

<div id="outline-container-org92bddf4" class="outline-2">
<h2 id="org92bddf4">Messaging between threads</h2>
<div class="outline-text-2" id="text-org92bddf4">
<p>
Suppose we are developing a synthesizer which produces sounds when keys are pressed on a <a href="https://en.wikipedia.org/wiki/MIDI_controller#Keyboards">MIDI keyboard</a>.
The audio library calls a function we provide once ever 6 or so milliseconds to request a list of samples from us.
The library calls our function with 2 arguments: 1) How many samples it wants 2) what key presses we need to handle.
The callback function uses a precomputed list of samples to generate sounds every time it is called.
To modify the properties of the sounds that are produced, the user edits settings with a user interface.
</p>

<p>
It would be painful (and incorrect) to attempt to handle UI events in the realtime thread, so we will run a UI thread to handle the UI events.
Whenever the UI thread gets an event to handle, it needs to compute a new sample list, then send the list to the realtime thread.
</p>

<p>
Since we can't lock, let's use a queue to send some sort of message between threads.
The queue that we choose needs to have a few properties:
</p>
<ul class="org-ul">
<li>Must be a <a href="https://pdfs.semanticscholar.org/a909/1ef790788c5d252cad94dd6862adf457e073.pdf">lock free queue</a></li>
<li>Must be able to preallocate all of its nodes (cant't allocate or free memory for a node on a push or pull)</li>
</ul>

<p>
I want to place messages on the heap so that they do not need to be copied as we move them around.
If messages lives on the heap, we must ensure they are allocated and freed outside of the realtime thread (we can't call allocation functions on the realtime thread).
</p>
</div>

<div id="outline-container-orgc8cd8f9" class="outline-3">
<h3 id="orgc8cd8f9">Reference counted garbage collection</h3>
<div class="outline-text-3" id="text-orgc8cd8f9">
<p>
It is totally fine to allocate on the UI thread, so when the UI thread handles an event it will compute a new list of samples and stick them into a freshly allocated block of memory.
Then we will ship this message over to the realtime thread.
</p>

<p>
When the realtime thread takes ownership of the message, it will need to hold onto the data for some undefined period of time.
But, when the realtime thread is done with the message, it cannot free it (because we can't allocate or deallocatein the realtime thread).
</p>

<p>
To solve this, let's run one more thread to clean up messages which are no longer being used by the realtime thread.
</p>

<p>
Whenever the UI thread allocates space for a message using standard allocators, it will wrap the message in a <a href="https://doc.rust-lang.org/std/sync/struct.Arc.html">reference-counted pointer</a>.
It then will let the collector thread know it should start keeping an eye on the reference-counted pointer.
The collector will store the pointer in a list.
When the reference count falls to 1, the collector is the only thread with a reference, and it can safely free the memory.
The pointer is sent to the realtime thread, then, when the realtime thread drops the message, the reference count will drop.
Sometime later, the collector thread will observe the decreased reference count and free the message.
</p>

<p>
<a href="./static/sound/gc_queue.pdf">Here</a> is a slideshow/animation demonstrating this process.
</p>
</div>
</div>

<div id="outline-container-org56bc4bb" class="outline-3">
<h3 id="org56bc4bb">Tradeoffs</h3>
<div class="outline-text-3" id="text-org56bc4bb">
<p>
Let's consider the theoretical behavior of this approach.
Note that anything I have to say should be taken with grain of salt; I haven't benchmarked anything, so I really have no evidence to support anything I'm claiming.
</p>

<p>
First, let's talk about when we would not want to use this approach.
</p>

<p>
If the realtime thread always consumes new messages in a predictable amount of time, we can preallocate a certain number of messages and just keep reusing the same blocks of memory.
When the UI needs to send a message it can grab one of the preallocated messages and use it.
Some predictable amount of time later, when the realtime thread is done with it, the message can be returned to the pool (by the realtime thread).
</p>

<p>
This is also a bad idea if the UI thread generates messages significantly faster than the realtime thread consumes them.
It might be fine for the realtime thread to lag behind the UI thread (if it eventually catches up), but the GC pointer list is going to get quite large.
If we do our GC scan frequently, we will be using a lot of cpu time scanning this list.
If we slow the collector down, the list is going to keep growing, and so will our memory usage.
In other words, it's a sticky situation.
A modern computer can probably handle this load, but we should avoid generating more load than necessary so that other audio applications running at the same time can use as much time as they need.
</p>

<p>
Finally, if the realtime thread needs to send a message to the UI thread, it can't just allocate memory and toss it at the GC thread for cleanup later.
We could still use the GC+queue method discussed here to send messages to the realtime thread, but we probably only have time to build one good messaging system (we want to make audio, not send messages back and forth!)
</p>

<p>
If none of the above are true, a simple GC thread with some reference counted pointers might be a nice way to avoid adding lots of complexity to a small system.
It also saves us from the need for a custom allocation mechanism, lets us send messages of various and dynamic sizes, and frees us from the burden of strict capacity constraints.
So, if we don't need something more clever, maybe this is a good thing to try out.
</p>

<p>
Finally, since we are using reference counting to manage memory, there will be some runtime cost to increment and decrement the reference counts.
This isn't a big deal for us, in this case, because the performance is predictable (we won't be suddenly surprised by the non-deterministic reference count incrementing).
</p>

<p>
There are many other variations of this technique (some which involve extra threads, some which don't, some which reuse freed memory, etc).
Regardless of the actual efficacy of this approach, it will be interesting to try to build one in Rust, so let's get started.
</p>
</div>
</div>
</div>

<div id="outline-container-orgc44e5c5" class="outline-2">
<h2 id="orgc44e5c5">Let's make one</h2>
<div class="outline-text-2" id="text-orgc44e5c5">
<p>
For the sake of these examples, let's assume that the built-in Rust <a href="https://doc.rust-lang.org/std/sync/mpsc/index.html">mpsc channel</a> is an appropriate lock free queue.
It will be pretty easy to swap this with something different later, and, if we use the standard library, all of the examples will easily run in the Rust playground.
We are also going to fake a bunch of the details of the audio library.
</p>
</div>

<div id="outline-container-org2df7b62" class="outline-3">
<h3 id="org2df7b62">Fake audio library</h3>
<div class="outline-text-3" id="text-org2df7b62">
<p>
<a href="https://play.rust-lang.org/?gist=27d1b7a693ffe01ac899b991317b170f&amp;version=stable&amp;backtrace=0">Rust Playground Link</a>.
</p>

<p>
We don't need to walk through this code, it just makes some threads and calls some empty functions.
The important bits are the <code>RealtimeThread::realtime_callback</code> function and the <code>UIThread::run</code> functions.
In this example, the realtime callback function says "I'm done!" to let the realtime thread shutdown, and the UI thread does nothing at all.
</p>

<p>
Here's the code:
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-keyword">use</span> <span class="org-constant">std</span>::thread;

<span class="org-preprocessor">#[derive(PartialEq)]</span>
<span class="org-keyword">enum</span> <span class="org-type">CallbackStatus</span> {
    <span class="org-type">Continue</span>,
    <span class="org-type">Shutdown</span>,
}

<span class="org-comment-delimiter">// </span><span class="org-comment">"library" code starts here</span>
<span class="org-keyword">type</span> <span class="org-type">Samples</span> = [<span class="org-type">f32</span>; 64];

<span class="org-keyword">fn</span> <span class="org-function-name">run_threads</span>(<span class="org-keyword">mut</span> <span class="org-variable-name">rt</span>: <span class="org-type">RealtimeThread</span>, <span class="org-keyword">mut</span> <span class="org-variable-name">ui</span>: <span class="org-type">UIThread</span>) {
    <span class="org-keyword">let</span> <span class="org-variable-name">join_handle</span> = <span class="org-constant">thread</span>::spawn(<span class="org-keyword">move</span> || {
        <span class="org-rust-builtin-formatting-macro">println!</span>(<span class="org-string">"[ui] thread started"</span>);
        ui.run();
        <span class="org-rust-builtin-formatting-macro">println!</span>(<span class="org-string">"[ui] thread shutting down"</span>);
    });

    <span class="org-rust-builtin-formatting-macro">println!</span>(<span class="org-string">"[realtime] thread started"</span>);
    <span class="org-keyword">let</span> <span class="org-keyword">mut</span> <span class="org-variable-name">output</span> = [0.0; 64];
    <span class="org-keyword">while</span> rt.realtime_callback(&amp;<span class="org-keyword">mut</span> output) != <span class="org-type">CallbackStatus</span>::<span class="org-type">Shutdown</span> { }
    <span class="org-rust-builtin-formatting-macro">println!</span>(<span class="org-string">"[realtime] thread shutting down"</span>);

    join_handle.join().unwrap();
}
<span class="org-comment-delimiter">// </span><span class="org-comment">end of "library" code</span>

<span class="org-doc">/// A struct containing the realtime callback and all data owned by the realtime thread</span>
<span class="org-keyword">struct</span> <span class="org-type">RealtimeThread</span> {
    <span class="org-comment-delimiter">// </span><span class="org-comment">some members here eventually</span>
}

<span class="org-keyword">impl</span> <span class="org-type">RealtimeThread</span> {
    <span class="org-keyword">fn</span> <span class="org-function-name">new</span>() -&gt; <span class="org-type">Self</span> { <span class="org-type">RealtimeThread</span>{} }

    <span class="org-doc">/// realtime callback, called to get the list of samples</span>
    <span class="org-keyword">fn</span> <span class="org-function-name">realtime_callback</span>(&amp;<span class="org-keyword">mut</span> <span class="org-keyword">self</span>, <span class="org-variable-name">output_samples</span>: &amp;<span class="org-keyword">mut</span> <span class="org-type">Samples</span>) -&gt; <span class="org-type">CallbackStatus</span> {
        <span class="org-type">CallbackStatus</span>::<span class="org-type">Shutdown</span>
    }
}

<span class="org-doc">/// A struct which runs the UI thread and contains all of the data owned by the UI thread</span>
<span class="org-keyword">struct</span> <span class="org-type">UIThread</span> {
    <span class="org-comment-delimiter">// </span><span class="org-comment">some members here eventually</span>
}

<span class="org-keyword">impl</span> <span class="org-type">UIThread</span> {
    <span class="org-keyword">fn</span> <span class="org-function-name">new</span>() -&gt; <span class="org-type">Self</span> { <span class="org-type">UIThread</span>{} }

    <span class="org-doc">/// All of the UI thread code</span>
    <span class="org-keyword">fn</span> <span class="org-function-name">run</span>(&amp;<span class="org-keyword">mut</span> <span class="org-keyword">self</span>) {
        <span class="org-comment-delimiter">// </span><span class="org-comment">do nothing!</span>
    }
}

<span class="org-keyword">fn</span> <span class="org-function-name">main</span>() {
    <span class="org-keyword">let</span> <span class="org-variable-name">rt</span> = <span class="org-type">RealtimeThread</span>::new();
    <span class="org-keyword">let</span> <span class="org-variable-name">ui</span> = <span class="org-type">UIThread</span>::new();
    run_threads(rt, ui);
}
</pre>
</div>

<p>
Output (one of many possible):
</p>

<pre class="example">
[realtime] thread started
[realtime] thread shutting down
[ui] thread started
[ui] thread shutting down
</pre>
</div>
</div>


<div id="outline-container-org2a985d8" class="outline-3">
<h3 id="org2a985d8">Sending Arcs between threads</h3>
<div class="outline-text-3" id="text-org2a985d8">
<p>
Now that we have an "audio library," let's try to make some messages and pass them between threads.
</p>

<p>
The <code>RealtimeThread</code> struct will need to hold on to a list of samples which it will use to populate the <code>output</code> samples every time the callback is called.
We want these samples to be heap allocated and reference counted, so we wrap them in an <a href="https://doc.rust-lang.org/std/sync/struct.Arc.html"><code>Arc</code></a>.
Finally, we want to leave the samples uninitialized until the UI thread sends us some, so we wrap the <code>Arc&lt;Samples&gt;</code> in an <a href="https://doc.rust-lang.org/std/option/enum.Option.html"><code>Option</code></a>.
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-keyword">struct</span> <span class="org-type">RealtimeThread</span> {
  <span class="org-variable-name">current_samples</span>: <span class="org-type">Option</span>&lt;<span class="org-type">Arc</span>&lt;<span class="org-type">Samples</span>&gt;&gt;,
}
</pre>
</div>

<p>
Now that the realtime thread has a list of samples, we can fill in a bit of the body of the realtime callback function:
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-keyword">fn</span> <span class="org-function-name">realtime_callback</span>(&amp;<span class="org-keyword">mut</span> <span class="org-keyword">self</span>, <span class="org-variable-name">output_samples</span>: &amp;<span class="org-keyword">mut</span> <span class="org-type">Samples</span>) -&gt; <span class="org-type">CallbackStatus</span> {
    <span class="org-keyword">self</span>.current_samples.as_ref().map(|samples| {
        <span class="org-comment-delimiter">// </span><span class="org-comment">samples: &amp;Arc&lt;[f32; 64]&gt;</span>
        output_samples.copy_from_slice(samples.as_ref())
    });

    <span class="org-type">CallbackStatus</span>::<span class="org-type">Continue</span>
}
</pre>
</div>

<p>
The function <a href="https://doc.rust-lang.org/std/primitive.slice.html#method.copy_from_slice"><code>copy_from_slice</code></a> will <code>memcpy</code> the samples we are holding onto into the buffer provided by the audio library.
</p>

<p>
Moving over to the UI thread, first, we need to be able to compute a list of samples to compute.
Here is a function that computes 64 samples along a sine wave with a given peak amplitude:
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-doc">/// computes the samples needed for on cycle of a sine wave</span>
<span class="org-doc">/// the volume parameter sets the audible volume of sound produced</span>
<span class="org-keyword">fn</span> <span class="org-function-name">compute_samples</span>(&amp;<span class="org-keyword">self</span>, <span class="org-variable-name">volume</span>: <span class="org-type">f32</span>) -&gt; <span class="org-type">Samples</span> {
    <span class="org-preprocessor">assert!</span>(volume &gt;= 0.0);
    <span class="org-preprocessor">assert!</span>(volume &lt;= 1.0);

    <span class="org-comment-delimiter">// </span><span class="org-comment">we need to populate 64 samples with 1 cycle of a sine wave (arbitrary choice)</span>
    <span class="org-keyword">let</span> <span class="org-variable-name">constant_factor</span> = (1.0/64.0) * 2.0 * <span class="org-type">f32</span>::<span class="org-constant">consts</span>::<span class="org-type">PI</span>;
    <span class="org-keyword">let</span> <span class="org-keyword">mut</span> <span class="org-variable-name">samples</span> = [0.0; 64];
    <span class="org-keyword">for</span> <span class="org-variable-name">i</span> <span class="org-keyword">in</span> 0..64 {
        samples[i] = (constant_factor * i <span class="org-keyword">as</span> <span class="org-type">f32</span>).sin() * volume;
    }

    samples
}
</pre>
</div>

<p>
The UI thread will generate some fake events, and compute samples for these events:
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-doc">/// All of the UI thread code</span>
<span class="org-keyword">fn</span> <span class="org-function-name">run</span>(&amp;<span class="org-keyword">mut</span> <span class="org-keyword">self</span>) {
    <span class="org-comment-delimiter">// </span><span class="org-comment">create 5 "ui events"</span>
    <span class="org-keyword">for</span> <span class="org-variable-name">i</span> <span class="org-keyword">in</span> 0..5 {
        <span class="org-keyword">let</span> <span class="org-variable-name">volume</span> = i <span class="org-keyword">as</span> <span class="org-type">f32</span> / 10.0;
        <span class="org-keyword">let</span> <span class="org-variable-name">samples</span> = <span class="org-type">Arc</span>::new(<span class="org-keyword">self</span>.compute_samples(volume));

        <span class="org-comment-delimiter">// </span><span class="org-comment">send the samples to the other thread</span>
    }

    <span class="org-comment-delimiter">// </span><span class="org-comment">tell the other thread to shutdown</span>
}
</pre>
</div>

<p>
Now that we've done all of that, we need to send the samples between threads.
</p>
</div>
</div>

<div id="outline-container-orgf41450e" class="outline-3">
<h3 id="orgf41450e">Message type</h3>
<div class="outline-text-3" id="text-orgf41450e">
<p>
As discussed previously, we will create the <code>Arc</code> on the UI thread, then send it to the realtime thread.
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-keyword">enum</span> <span class="org-type">Message</span> {
    <span class="org-type">NewSamples</span>(<span class="org-type">Arc</span>&lt;<span class="org-type">Samples</span>&gt;),
    <span class="org-type">Shutdown</span>,
}
</pre>
</div>

<p>
Remember when I said that we would make a bunch of assumptions about the <code>mpsc</code> queues?
Here's where I'm going to do that.
We are going to assume that this queue follows all the properties we need a realtime queue to follow.
For a quick reminder, those are:
</p>
<ul class="org-ul">
<li>No locks</li>
<li>No allocation (or deallocation) in the realtime thread.</li>
</ul>

<p>
To send messages between the threads, we will use <a href="https://doc.rust-lang.org/std/sync/mpsc/fn.sync_channel.html"><code>mpcs::sync_channel</code></a> to create a synchronous channel (queue).
This channel is bounded, so a sender cannot add a new message to the queue unless there is currently space available.
We are going to set the buffer size to zero.
From <a href="https://doc.rust-lang.org/std/sync/mpsc/fn.sync_channel.html">the docs</a>:
</p>

<blockquote>
<p>
Note that a buffer size of 0 is valid, in which case this [channel] becomes "rendezvous channel" where each send will not return until a recv is paired with it.
</p>
</blockquote>

<p>
This "channel" will have two ends; one which can send messages and one which can receive messages.
Lets create both of them in the <code>main</code> method.
The send side will be called <code>tx</code> (for transmit) and the receive side is called <code>rx</code>.
Whenever a message is placed on <code>tx</code> it will become available on <code>rx</code>.
</p>

<p>
Then, we let each of our threads take ownership of the appropriate channel.
We give <code>rx</code> to the <code>RealtimeThread</code>, because it will receive messages, and <code>tx</code> to the <code>UIThread</code>, because it will be sending them.
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-keyword">fn</span> <span class="org-function-name">main</span>() {
    <span class="org-keyword">let</span> (tx, rx) = <span class="org-constant">mpsc</span>::sync_channel(0);
    <span class="org-keyword">let</span> <span class="org-variable-name">rt</span> = <span class="org-type">RealtimeThread</span>::new(rx);
    <span class="org-keyword">let</span> <span class="org-variable-name">ui</span> = <span class="org-type">UIThread</span>::new(tx);
    run_threads(rt, ui);
}
</pre>
</div>

<p>
Then, modify both thread structs and both <code>new</code> functions.
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-keyword">struct</span> <span class="org-type">RealtimeThread</span> {
    <span class="org-variable-name">current_samples</span>: <span class="org-type">Option</span>&lt;<span class="org-type">Arc</span>&lt;<span class="org-type">Samples</span>&gt;&gt;,
    <span class="org-variable-name">incoming</span>:        <span class="org-constant">mpsc</span>::<span class="org-type">Receiver</span>&lt;<span class="org-type">Message</span>&gt;,
}

<span class="org-comment-delimiter">// </span><span class="org-comment">...</span>

<span class="org-keyword">struct</span> <span class="org-type">UIThread</span> {
    <span class="org-variable-name">outgoing</span>: <span class="org-constant">mpsc</span>::<span class="org-type">SyncSender</span>&lt;<span class="org-type">Message</span>&gt;,
}

<span class="org-comment-delimiter">// </span><span class="org-comment">changes to new omitted</span>
</pre>
</div>

<p>
Now, let's get our threads sending messages, starting with the UI thread.
If any sends fails, something has gone horribly wrong, so its fine to <code>unwrap</code> the result of these sends.
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-doc">/// All of the UI thread code</span>
<span class="org-keyword">fn</span> <span class="org-function-name">run</span>(&amp;<span class="org-keyword">mut</span> <span class="org-keyword">self</span>) {
    <span class="org-comment-delimiter">// </span><span class="org-comment">create 10 "ui events"</span>
    <span class="org-keyword">for</span> <span class="org-variable-name">i</span> <span class="org-keyword">in</span> 0..10 {
        <span class="org-keyword">let</span> <span class="org-variable-name">volume</span> = i <span class="org-keyword">as</span> <span class="org-type">f32</span> / 10.0;
        <span class="org-keyword">let</span> <span class="org-variable-name">samples</span> = <span class="org-type">Arc</span>::new(<span class="org-keyword">self</span>.compute_samples(volume));

        <span class="org-comment-delimiter">// </span><span class="org-comment">send the samples to the other thread</span>
        <span class="org-rust-builtin-formatting-macro">println!</span>(<span class="org-string">"[ui] sending new samples. Second sample: </span><span class="org-rust-string-interpolation">{}</span><span class="org-string">"</span>, samples[1]);
        <span class="org-keyword">self</span>.outgoing.send(<span class="org-type">Message</span>::<span class="org-type">NewSamples</span>(samples)).unwrap();
    }

    <span class="org-comment-delimiter">// </span><span class="org-comment">tell the other thread to shutdown</span>
    <span class="org-keyword">self</span>.outgoing.send(<span class="org-type">Message</span>::<span class="org-type">Shutdown</span>).unwrap();
}
</pre>
</div>

<p>
In the realtime thread, we check if there is a new message on the queue.
If there is, handle it.
If not, just keep doing what we were doing.
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-doc">/// realtime callback, called to get the list of samples</span>
<span class="org-keyword">fn</span> <span class="org-function-name">realtime_callback</span>(&amp;<span class="org-keyword">mut</span> <span class="org-keyword">self</span>, <span class="org-variable-name">output_samples</span>: &amp;<span class="org-keyword">mut</span> <span class="org-type">Samples</span>) -&gt; <span class="org-type">CallbackStatus</span> {
    <span class="org-keyword">match</span> <span class="org-keyword">self</span>.incoming.try_recv() {
        <span class="org-comment-delimiter">// </span><span class="org-comment">we've received a messaged</span>
        <span class="org-type">Ok</span>(message) =&gt; <span class="org-keyword">match</span> message {
            <span class="org-type">Message</span>::<span class="org-type">NewSamples</span>(samples) =&gt; {
                <span class="org-rust-builtin-formatting-macro">println!</span>(<span class="org-string">"[realtime] received new samples. Second sample: </span><span class="org-rust-string-interpolation">{}</span><span class="org-string">"</span>, samples[1]);
                <span class="org-keyword">self</span>.current_samples = <span class="org-type">Some</span>(samples)
            },

            <span class="org-comment-delimiter">// </span><span class="org-comment">If we got a shutdown message, shutdown the realtime thread</span>
            <span class="org-type">Message</span>::<span class="org-type">Shutdown</span> =&gt; <span class="org-keyword">return</span> <span class="org-type">CallbackStatus</span>::<span class="org-type">Shutdown</span>
        },

        <span class="org-comment-delimiter">// </span><span class="org-comment">if we didn't receive anything, just keep sending samples</span>
        <span class="org-type">Err</span>(_) =&gt; ()
    }

    <span class="org-comment-delimiter">// </span><span class="org-comment">copy our current samples into the output buffer</span>
    <span class="org-keyword">self</span>.current_samples.as_ref().map(|samples| {
        <span class="org-comment-delimiter">// </span><span class="org-comment">samples: &amp;Arc&lt;[f32; 64&gt;</span>
        output_samples.copy_from_slice(samples.as_ref())
    });

    <span class="org-type">CallbackStatus</span>::<span class="org-type">Continue</span>
}
</pre>
</div>

<p>
I've used a <code>println!</code> here only for the sake of demonstration.
You shouldn't ever do this in real realtime code (because print statements usually allocate!)
</p>

<p>
<a href="https://play.rust-lang.org/?gist=6e37aa0a7f8d06f8b31b9822c8bbb79c&amp;version=stable&amp;backtrace=0">Here is a link</a> to this code in the Rust playground.
It might timeout if you try running it. If you see any messages about timeout, don't worry, just try running the code again.
</p>

<p>
Here is an example output:
</p>
<pre class="example">
[realtime] thread started
[ui] thread started
[ui] sending new samples. Second sample: 0
[realtime] received new samples. Second sample: 0
[ui] sending new samples. Second sample: 0.009801715
[realtime] received new samples. Second sample: 0.009801715
[ui] sending new samples. Second sample: 0.01960343
[realtime] received new samples. Second sample: 0.01960343
[ui] sending new samples. Second sample: 0.029405143
[realtime] received new samples. Second sample: 0.029405143
[ui] sending new samples. Second sample: 0.03920686
[realtime] received new samples. Second sample: 0.03920686
[realtime] thread shutting down
[ui] thread shutting down
</pre>
</div>
</div>

<div id="outline-container-org3d54831" class="outline-3">
<h3 id="org3d54831">Problems?</h3>
<div class="outline-text-3" id="text-org3d54831">
<p>
The last example <b>seems</b> to do the right thing, let's take a look at what the realtime callback does when it receives a new set of samples.
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
<span class="org-type">Message</span>::<span class="org-type">NewSamples</span>(samples) =&gt; {
    <span class="org-keyword">self</span>.current_samples = <span class="org-type">Some</span>(samples)
},
<span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
</pre>
</div>

<p>
What happens to the old array of samples?
Rust will insert a call to <code>drop</code> here, because the old value has just gone out of scope.
Something like this (in pseudo-Rust) sort of shows what is going on.
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
<span class="org-type">Message</span>::<span class="org-type">NewSamples</span>(samples) =&gt; {
    <span class="org-keyword">let</span> <span class="org-variable-name">tmp</span> = <span class="org-type">Some</span>(samples);
    <span class="org-constant">mem</span>::swap(<span class="org-keyword">self</span>.current_samples, tmp);
    drop(tmp);
},
<span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
</pre>
</div>

<p>
When an <code>Arc</code> gets <code>drop=ped, what happens?
Let's refer to the docs for =drop</code>.
</p>

<blockquote>
<p>
This will decrement the strong reference count. If the strong reference count becomes zero and the only other references are <code>Weak&lt;T&gt;</code> ones, drops the inner value.
</p>
</blockquote>

<p>
In this case, the inner value is some heap allocated memory, so calling drop will deallocate that memory (since no one else is holding any references).
This is a problem!
We can't let our realtime callback perform memory allocation.
</p>
</div>
</div>
</div>

<div id="outline-container-orgc57a241" class="outline-2">
<h2 id="orgc57a241">Build the GC</h2>
<div class="outline-text-2" id="text-orgc57a241">
<p>
We now need to build the GC that I promised we would build, to clean up after us, outside of the realtime thread.
Sneak peak, once the GC is implemented, all we have to change is <code>UIThread::run</code>, in a very small way:
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-doc">/// All of the UI thread code</span>
<span class="org-keyword">fn</span> <span class="org-function-name">run</span>(&amp;<span class="org-keyword">mut</span> <span class="org-keyword">self</span>) {
    <span class="org-keyword">let</span> <span class="org-keyword">mut</span> <span class="org-variable-name">gc</span> = <span class="org-type">GC</span>::new(); <span class="org-comment-delimiter">// </span><span class="org-comment">+ NEW LINE</span>

    <span class="org-comment-delimiter">// </span><span class="org-comment">create 5 "ui events"</span>
    <span class="org-keyword">for</span> <span class="org-variable-name">i</span> <span class="org-keyword">in</span> 0..5 {
        <span class="org-keyword">let</span> <span class="org-variable-name">volume</span> = i <span class="org-keyword">as</span> <span class="org-type">f32</span> / 5.0;
        <span class="org-keyword">let</span> <span class="org-variable-name">samples</span> = <span class="org-type">Arc</span>::new(<span class="org-keyword">self</span>.compute_samples(volume));
        <span class="org-keyword">self</span>.collector.track(samples.clone()); <span class="org-comment-delimiter">// </span><span class="org-comment">+ NEW LINE</span>

        <span class="org-comment-delimiter">// </span><span class="org-comment">send the samples to the other thread</span>
        <span class="org-rust-builtin-formatting-macro">println!</span>(<span class="org-string">"[ui] sending new samples. Second sample: </span><span class="org-rust-string-interpolation">{}</span><span class="org-string">"</span>, samples[1]);
        <span class="org-keyword">self</span>.outgoing.send(<span class="org-type">Message</span>::<span class="org-type">NewSamples</span>(samples)).unwrap();
    }

    <span class="org-comment-delimiter">// </span><span class="org-comment">tell the other thread to shutdown</span>
    <span class="org-keyword">self</span>.outgoing.send(<span class="org-type">Message</span>::<span class="org-type">Shutdown</span>).unwrap();
}
</pre>
</div>

<p>
With that in mind, lets sketch out the interface for the Garbage Collector.
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-doc">/// A garbage collector for Arc&lt;T&gt; pointers</span>
<span class="org-keyword">struct</span> <span class="org-type">GC</span>&lt;<span class="org-type">T</span>&gt; {
    <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
}

<span class="org-keyword">impl</span>&lt;<span class="org-type">T</span>&gt; <span class="org-type">GC</span>&lt;<span class="org-type">T</span>&gt; {
    <span class="org-doc">/// Construct a new garbage collector and start the collection thread</span>
    <span class="org-keyword">fn</span> <span class="org-function-name">new</span>() -&gt; <span class="org-type">Self</span> {
        <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
    }

    <span class="org-doc">/// Instruct the garbage collector to monitor this Arc&lt;T&gt;</span>
    <span class="org-doc">/// When no references remain, the collector will `drop` the value</span>
    <span class="org-keyword">fn</span> <span class="org-function-name">track</span>(&amp;<span class="org-keyword">mut</span> <span class="org-keyword">self</span>, <span class="org-variable-name">t</span>: <span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;) {
        <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
    }
}

</pre>
</div>

<p>
First think about the <code>track</code> method.
All this method needs to do is move it's argument into some list (vector) of pointers.
We will keep this vector in the GC thread struct so that each of the references will live until the GC thread is shut down or until the GC drops them.
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-keyword">struct</span> <span class="org-type">GC</span>&lt;<span class="org-type">T</span>&gt; {
    <span class="org-variable-name">pool</span>: <span class="org-type">Vec</span>&lt;<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;,
}

<span class="org-keyword">impl</span>&lt;<span class="org-type">T</span>&gt; <span class="org-type">GC</span>&lt;<span class="org-type">T</span>&gt; {
    <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>

    <span class="org-keyword">pub</span> <span class="org-keyword">fn</span> <span class="org-function-name">track</span>(&amp;<span class="org-keyword">mut</span> <span class="org-keyword">self</span>, <span class="org-variable-name">t</span>: <span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;) {
        <span class="org-keyword">self</span>.pool.push(t);
    }
}
</pre>
</div>

<p>
Now lets think about the garbage collection logic.
Since we have a <code>Vec&lt;Arc&lt;T&gt;&gt;</code>, we will want to iterate over it, removing any elements which meet (or fail) a condition.
We can use <code>Vec::retain</code> to do this.
Something like the following might work:
</p>

<div class="org-src-container">
<pre class="src src-rust">pool.retain(|e| {
    <span class="org-keyword">if</span> <span class="org-comment-delimiter">/* </span><span class="org-comment">has more than one reference */</span> {
        <span class="org-keyword">return</span> <span class="org-keyword">true</span>
    } <span class="org-keyword">else</span> {
        <span class="org-keyword">return</span> <span class="org-keyword">false</span>
    }
})
</pre>
</div>

<p>
Looking at the <a href="https://doc.rust-lang.org/std/sync/struct.Arc.html"><code>Arc</code> docs</a>, there are a few ways we can figure out if the <code>Arc</code> has only one remaining reference:
</p>
<ul class="org-ul">
<li>Attempt to consume the <code>Arc</code> with <code>Arc::try_unwrap</code>, if this fails, we know that it has more than one reference. Unforunately, this method requires moving the <code>Arc</code> out of the vector, which is not ideal if we want to use <code>Vec::retain</code>.</li>
<li><code>Arc::strong_count</code> - this is currently marked as unstable. Looks like what we might want to use though.</li>
<li><code>Arc::get_mut</code> could possibly be used the same way we would use <code>Arc::try_unwrap</code>, without moving the <code>Arc</code> containing in the vector unless we want to remove it.</li>
</ul>

<p>
We don't have lots of options, so I'm going to go ahead and use <code>Arc::strong_count</code>.
This is (for now) the most natural way to solve the problem:
</p>

<div class="org-src-container">
<pre class="src src-rust">pool.retain(|<span class="org-variable-name">e</span>: <span class="org-type">Arc</span>&lt;_&gt;| {
    <span class="org-keyword">if</span> <span class="org-type">Arc</span>::strong_count(&amp;e) {
        <span class="org-keyword">return</span> <span class="org-keyword">true</span>
    } <span class="org-keyword">else</span> {
        <span class="org-keyword">return</span> <span class="org-keyword">false</span>
    }
})
</pre>
</div>

<p>
Let's move on to <code>new</code>.
The <code>new</code> method needs to start new thread which will run the <code>pool.retain</code> thing every once and a while.
We also need to hold on to a thread handle so that we can eventually join the thread.
The join handle is wrapped in an <code>Option</code>, we will see why quite a bit later.
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-doc">/// A garbage collector for Arc&lt;T&gt; pointers</span>
<span class="org-keyword">struct</span> <span class="org-type">GC</span>&lt;<span class="org-type">T</span>&gt; {
    <span class="org-variable-name">pool</span>: <span class="org-type">Vec</span>&lt;<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;,
    <span class="org-variable-name">thread</span>: <span class="org-type">Option</span>&lt;<span class="org-constant">thread</span>::<span class="org-type">JoinHandle</span>&lt;()&gt;&gt;,
}

<span class="org-keyword">impl</span>&lt;<span class="org-type">T</span>&gt; <span class="org-type">GC</span>&lt;<span class="org-type">T</span>&gt; {
    <span class="org-comment-delimiter">// </span><span class="org-comment">private. cleans up any dead pointers in a pool</span>
    <span class="org-keyword">fn</span> <span class="org-function-name">cleanup</span>(<span class="org-variable-name">pool</span>: &amp;<span class="org-keyword">mut</span> <span class="org-type">Vec</span>&lt;<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;) {
        pool.retain(|<span class="org-variable-name">e</span>: &amp;<span class="org-type">Arc</span>&lt;_&gt;| {
            <span class="org-keyword">if</span> <span class="org-type">Arc</span>::strong_count(&amp;e) &gt; 1 {
                <span class="org-keyword">return</span> <span class="org-keyword">true</span>
            } <span class="org-keyword">else</span> {
                <span class="org-keyword">return</span> <span class="org-keyword">false</span>
            }
        });
    }

    <span class="org-keyword">pub</span> <span class="org-keyword">fn</span> <span class="org-function-name">new</span>() -&gt; <span class="org-type">Self</span> {
        <span class="org-keyword">let</span> <span class="org-variable-name">pool</span> = <span class="org-type">Vec</span>::new();

        <span class="org-comment-delimiter">// </span><span class="org-comment">create a closure which will become a new thread</span>
        <span class="org-keyword">let</span> <span class="org-variable-name">gc</span> = || {
            <span class="org-keyword">loop</span> {
                <span class="org-type">GC</span>::cleanup(&amp;<span class="org-keyword">mut</span> pool);

                <span class="org-comment-delimiter">// </span><span class="org-comment">wait for 100 milliseconds, then scan again</span>
                <span class="org-keyword">let</span> <span class="org-variable-name">sleep</span> = <span class="org-constant">std</span>::<span class="org-constant">time</span>::<span class="org-type">Duration</span>::from_millis(100);
                <span class="org-constant">thread</span>::sleep(sleep);
            }
        };

        <span class="org-comment-delimiter">// </span><span class="org-comment">spawns a new thread and returns a handle to the thread</span>
        <span class="org-keyword">let</span> <span class="org-variable-name">gc_thread</span> = <span class="org-constant">thread</span>::spawn(gc);

        <span class="org-type">GC</span> {
            <span class="org-variable-name">pool</span>:   pool,
            <span class="org-variable-name">thread</span>: <span class="org-type">Some</span>(gc_thread),
        }
    }

    <span class="org-keyword">pub</span> <span class="org-keyword">fn</span> <span class="org-function-name">track</span>(&amp;<span class="org-keyword">mut</span> <span class="org-keyword">self</span>, <span class="org-variable-name">t</span>: <span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;) {
        <span class="org-keyword">self</span>.pool.push(t);
    }
}

<span class="org-keyword">fn</span> <span class="org-function-name">main</span>() {
    <span class="org-keyword">let</span> (tx, rx) = <span class="org-constant">mpsc</span>::sync_channel(0);
    <span class="org-keyword">let</span> <span class="org-variable-name">rt</span> = <span class="org-type">RealtimeThread</span>::new(rx);
    <span class="org-keyword">let</span> <span class="org-variable-name">ui</span> = <span class="org-type">UIThread</span>::new(tx);
    run_threads(rt, ui);
}
</pre>
</div>

<p>
We've written a bunch of new code, better make sure it compiles (<a href="https://play.rust-lang.org/?gist=0740c7896b0dd8c37e1d57aa9e53ca0b&amp;version=stable&amp;backtrace=0">Rust playground</a>):
</p>

<div class="org-src-container">
<pre class="src src-rust">error[<span class="org-type">E0277</span>]: the <span class="org-keyword">trait</span> bound `<span class="org-variable-name">T</span>: <span class="org-constant">std</span>::<span class="org-constant">marker</span>::<span class="org-type">Send</span>` is not satisfied
   --&gt; &lt;anon&gt;:<span class="org-variable-name">154</span>:25
    |
154 |         <span class="org-keyword">let</span> <span class="org-variable-name">gc_thread</span> = <span class="org-constant">thread</span>::spawn(gc);
    |                         ^^^^^^^^^^^^^ the <span class="org-keyword">trait</span> `<span class="org-constant">std</span>::<span class="org-constant">marker</span>::<span class="org-type">Send</span>` is not implemented <span class="org-keyword">for</span> `<span class="org-type">T</span>`
    |
    = <span class="org-variable-name">help</span>: consider adding a `<span class="org-keyword">where</span> <span class="org-variable-name">T</span>: <span class="org-constant">std</span>::<span class="org-constant">marker</span>::<span class="org-type">Send</span>` bound
    = <span class="org-variable-name">note</span>: required because of the requirements on the <span class="org-keyword">impl</span> of `<span class="org-constant">std</span>::<span class="org-constant">marker</span>::<span class="org-type">Send</span>` <span class="org-keyword">for</span> `<span class="org-constant">std</span>::<span class="org-constant">sync</span>::<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;`
    = <span class="org-variable-name">note</span>: required because of the requirements on the <span class="org-keyword">impl</span> of `<span class="org-constant">std</span>::<span class="org-constant">marker</span>::<span class="org-type">Send</span>` <span class="org-keyword">for</span> `<span class="org-constant">std</span>::<span class="org-constant">ptr</span>::<span class="org-type">Unique</span>&lt;<span class="org-constant">std</span>::<span class="org-constant">sync</span>::<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;`
    = <span class="org-variable-name">note</span>: required because it appears within the <span class="org-keyword">type</span> `<span class="org-constant">alloc</span>::<span class="org-constant">raw_vec</span>::<span class="org-type">RawVec</span>&lt;<span class="org-constant">std</span>::<span class="org-constant">sync</span>::<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;`
    = <span class="org-variable-name">note</span>: required because it appears within the <span class="org-keyword">type</span> `<span class="org-constant">std</span>::<span class="org-constant">vec</span>::<span class="org-type">Vec</span>&lt;<span class="org-constant">std</span>::<span class="org-constant">sync</span>::<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;`
    = <span class="org-variable-name">note</span>: required because of the requirements on the <span class="org-keyword">impl</span> of `<span class="org-constant">std</span>::<span class="org-constant">marker</span>::<span class="org-type">Send</span>` <span class="org-keyword">for</span> `&amp;<span class="org-keyword">mut</span> <span class="org-constant">std</span>::<span class="org-constant">vec</span>::<span class="org-type">Vec</span>&lt;<span class="org-constant">std</span>::<span class="org-constant">sync</span>::<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;`
    = <span class="org-variable-name">note</span>: required because it appears within the <span class="org-keyword">type</span> `[closure@&lt;anon&gt;:<span class="org-variable-name">143</span>:1<span class="org-variable-name">8</span>: <span class="org-variable-name">151</span>:10 <span class="org-variable-name">pool</span>:&amp;<span class="org-keyword">mut</span> <span class="org-constant">std</span>::<span class="org-constant">vec</span>::<span class="org-type">Vec</span>&lt;<span class="org-constant">std</span>::<span class="org-constant">sync</span>::<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;]`
    = <span class="org-variable-name">note</span>: required by `<span class="org-constant">std</span>::<span class="org-constant">thread</span>::spawn`

error[<span class="org-type">E0277</span>]: the <span class="org-keyword">trait</span> bound `<span class="org-variable-name">T</span>: <span class="org-constant">std</span>::<span class="org-constant">marker</span>::<span class="org-type">Sync</span>` is not satisfied
   --&gt; &lt;anon&gt;:<span class="org-variable-name">154</span>:25
    |
154 |         <span class="org-keyword">let</span> <span class="org-variable-name">gc_thread</span> = <span class="org-constant">thread</span>::spawn(gc);
    |                         ^^^^^^^^^^^^^ the <span class="org-keyword">trait</span> `<span class="org-constant">std</span>::<span class="org-constant">marker</span>::<span class="org-type">Sync</span>` is not implemented <span class="org-keyword">for</span> `<span class="org-type">T</span>`
    |
    = <span class="org-variable-name">help</span>: consider adding a `<span class="org-keyword">where</span> <span class="org-variable-name">T</span>: <span class="org-constant">std</span>::<span class="org-constant">marker</span>::<span class="org-type">Sync</span>` bound
    = <span class="org-variable-name">note</span>: required because of the requirements on the <span class="org-keyword">impl</span> of `<span class="org-constant">std</span>::<span class="org-constant">marker</span>::<span class="org-type">Send</span>` <span class="org-keyword">for</span> `<span class="org-constant">std</span>::<span class="org-constant">sync</span>::<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;`
    = <span class="org-variable-name">note</span>: required because of the requirements on the <span class="org-keyword">impl</span> of `<span class="org-constant">std</span>::<span class="org-constant">marker</span>::<span class="org-type">Send</span>` <span class="org-keyword">for</span> `<span class="org-constant">std</span>::<span class="org-constant">ptr</span>::<span class="org-type">Unique</span>&lt;<span class="org-constant">std</span>::<span class="org-constant">sync</span>::<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;`
    = <span class="org-variable-name">note</span>: required because it appears within the <span class="org-keyword">type</span> `<span class="org-constant">alloc</span>::<span class="org-constant">raw_vec</span>::<span class="org-type">RawVec</span>&lt;<span class="org-constant">std</span>::<span class="org-constant">sync</span>::<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;`
    = <span class="org-variable-name">note</span>: required because it appears within the <span class="org-keyword">type</span> `<span class="org-constant">std</span>::<span class="org-constant">vec</span>::<span class="org-type">Vec</span>&lt;<span class="org-constant">std</span>::<span class="org-constant">sync</span>::<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;`
    = <span class="org-variable-name">note</span>: required because of the requirements on the <span class="org-keyword">impl</span> of `<span class="org-constant">std</span>::<span class="org-constant">marker</span>::<span class="org-type">Send</span>` <span class="org-keyword">for</span> `&amp;<span class="org-keyword">mut</span> <span class="org-constant">std</span>::<span class="org-constant">vec</span>::<span class="org-type">Vec</span>&lt;<span class="org-constant">std</span>::<span class="org-constant">sync</span>::<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;`
    = <span class="org-variable-name">note</span>: required because it appears within the <span class="org-keyword">type</span> `[closure@&lt;anon&gt;:<span class="org-variable-name">143</span>:1<span class="org-variable-name">8</span>: <span class="org-variable-name">151</span>:10 <span class="org-variable-name">pool</span>:&amp;<span class="org-keyword">mut</span> <span class="org-constant">std</span>::<span class="org-constant">vec</span>::<span class="org-type">Vec</span>&lt;<span class="org-constant">std</span>::<span class="org-constant">sync</span>::<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;]`
    = <span class="org-variable-name">note</span>: required by `<span class="org-constant">std</span>::<span class="org-constant">thread</span>::spawn`

<span class="org-variable-name">error</span>: aborting due to 2 previous errors
</pre>
</div>

<p>
Oops, this isn't good.
This error makes it feel sort of like Rust hates us, but the compiler is actually doing us a massive favor.
</p>

<p>
In Rust, there are a few thread safety "marker traits" called <code>Send</code> and <code>Sync</code>.
The compiler is telling us that our generic type <code>T</code> doesn't implement either of them.
</p>

<p>
Put very loosely, if something implements <code>Send</code>, it is safe to send it between threads.
<code>Sync</code> is considerably more subtle and quite difficult to wrap your head around, but we can sort of say that, if something implements <code>Sync</code>, we can <b>access</b> the same instance of it from multiple threads.
For more info, you can read <a href="http://huonw.github.io/blog/2015/02/some-notes-on-send-and-sync/">this blog post</a>, but you shouldn't need any more than what I've given to get through the rest of my post.
</p>

<p>
So anyway, Rust is telling us that we have a thread safety problem, but we haven't guaranteed that we can safely copy and access values of our type <code>T</code> between the garbage collector thread and any other threads.
</p>

<p>
I know that <code>T</code> must be <code>Send</code>, because it has to be sent between threads, so let's go ahead and add that restriction:
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-doc">/// A garbage collector for Arc&lt;T&gt; pointers</span>
<span class="org-keyword">struct</span> <span class="org-type">GC</span>&lt;<span class="org-variable-name">T</span>: <span class="org-type">Send</span>&gt; {
    <span class="org-variable-name">pool</span>: <span class="org-type">Vec</span>&lt;<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;,
    <span class="org-variable-name">thread</span>: <span class="org-type">Option</span>&lt;<span class="org-constant">thread</span>::<span class="org-type">JoinHandle</span>&lt;()&gt;&gt;,
}

<span class="org-keyword">impl</span>&lt;<span class="org-variable-name">T</span>: <span class="org-type">Send</span>&gt; <span class="org-type">GC</span>&lt;<span class="org-type">T</span>&gt; {
<span class="org-comment-delimiter">// </span><span class="org-comment">....</span>
</pre>
</div>

<p>
<a href="https://play.rust-lang.org/?gist=4f718d3a1795409d67894a8f4f86f010&amp;version=stable&amp;backtrace=0">Rust playground link</a>
</p>

<p>
Hoorary, the <code>Send</code> error is gone!
Unfortunately, we still have the issue with <code>Sync</code>.
Let's look more closely at the error we are getting:
</p>

<div class="org-src-container">
<pre class="src src-rust">error[<span class="org-type">E0277</span>]: the <span class="org-keyword">trait</span> bound `<span class="org-variable-name">T</span>: <span class="org-constant">std</span>::<span class="org-constant">marker</span>::<span class="org-type">Sync</span>` is not satisfied
   --&gt; &lt;anon&gt;:<span class="org-variable-name">154</span>:25
    |
154 |         <span class="org-keyword">let</span> <span class="org-variable-name">gc_thread</span> = <span class="org-constant">thread</span>::spawn(gc);
    |                         ^^^^^^^^^^^^^ the <span class="org-keyword">trait</span> `<span class="org-constant">std</span>::<span class="org-constant">marker</span>::<span class="org-type">Sync</span>` is not implemented <span class="org-keyword">for</span> `<span class="org-type">T</span>`
    |
    = <span class="org-variable-name">help</span>: consider adding a `<span class="org-keyword">where</span> <span class="org-variable-name">T</span>: <span class="org-constant">std</span>::<span class="org-constant">marker</span>::<span class="org-type">Sync</span>` bound
    = <span class="org-variable-name">note</span>: required because of the requirements on the <span class="org-keyword">impl</span> of `<span class="org-constant">std</span>::<span class="org-constant">marker</span>::<span class="org-type">Send</span>` <span class="org-keyword">for</span> `<span class="org-constant">std</span>::<span class="org-constant">sync</span>::<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;`
    = <span class="org-variable-name">note</span>: required because of the requirements on the <span class="org-keyword">impl</span> of `<span class="org-constant">std</span>::<span class="org-constant">marker</span>::<span class="org-type">Send</span>` <span class="org-keyword">for</span> `<span class="org-constant">std</span>::<span class="org-constant">ptr</span>::<span class="org-type">Unique</span>&lt;<span class="org-constant">std</span>::<span class="org-constant">sync</span>::<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;`
    = <span class="org-variable-name">note</span>: required because it appears within the <span class="org-keyword">type</span> `<span class="org-constant">alloc</span>::<span class="org-constant">raw_vec</span>::<span class="org-type">RawVec</span>&lt;<span class="org-constant">std</span>::<span class="org-constant">sync</span>::<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;`
    = <span class="org-variable-name">note</span>: required because it appears within the <span class="org-keyword">type</span> `<span class="org-constant">std</span>::<span class="org-constant">vec</span>::<span class="org-type">Vec</span>&lt;<span class="org-constant">std</span>::<span class="org-constant">sync</span>::<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;`
    = <span class="org-variable-name">note</span>: required because of the requirements on the <span class="org-keyword">impl</span> of `<span class="org-constant">std</span>::<span class="org-constant">marker</span>::<span class="org-type">Send</span>` <span class="org-keyword">for</span> `&amp;<span class="org-keyword">mut</span> <span class="org-constant">std</span>::<span class="org-constant">vec</span>::<span class="org-type">Vec</span>&lt;<span class="org-constant">std</span>::<span class="org-constant">sync</span>::<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;`
    = <span class="org-variable-name">note</span>: required because it appears within the <span class="org-keyword">type</span> `[closure@&lt;anon&gt;:<span class="org-variable-name">143</span>:1<span class="org-variable-name">8</span>: <span class="org-variable-name">151</span>:10 <span class="org-variable-name">pool</span>:&amp;<span class="org-keyword">mut</span> <span class="org-constant">std</span>::<span class="org-constant">vec</span>::<span class="org-type">Vec</span>&lt;<span class="org-constant">std</span>::<span class="org-constant">sync</span>::<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;]`
    = <span class="org-variable-name">note</span>: required by `<span class="org-constant">std</span>::<span class="org-constant">thread</span>::spawn`

<span class="org-variable-name">error</span>: aborting due to previous error
</pre>
</div>

<p>
This error is really confusing, and my solution for it is not going to be much better, but stick with me.
</p>

<p>
The origin of this error is the <code>Arc&lt;T&gt;</code>.
If we want an <code>Arc&lt;T&gt;</code> to implement <code>Send</code>, the <code>T</code> contained in it must implement BOTH <code>Send</code> and <code>Sync</code>.
It makes sense that <code>T</code> would need to implement <code>Send</code>, but why does <code>T</code> need to be <code>Sync</code>?
Basically, this is because the data the <code>Arc&lt;T&gt;</code> is holding will be shared by anyone who can access the <code>Arc&lt;T&gt;</code>.
An <code>Arc</code> can be =clone=ed at any time, so, if we are allowed to pass it to other threads, it must also be safe for multiple threads to access the underlying data at the same time.
</p>

<p>
We could add the <code>Sync</code> constraint to our type <code>T</code> to resolve this problem, but does this really make any sense?
Nowhere in our application will a message be accessible by more than one thread at a time.
</p>

<p>
When the UI thread creates a new message, it immediately surrenders all access to the underlying data, by moving the value into the channel.
Once the realtime thread has the data, it will be the only thread that actually accesses the data until the data needs to be freed.
The GC also is holding a reference to data, but it will never actually touch the data in any way, until it frees it.
When the GC thread frees the memory holding the data, we know that there will be no other references to the memory in the program.
</p>

<p>
I might be wrong about this (please let me know if I am), but I think that we don't actually <b>need</b> the type <code>T</code> to be <code>Sync</code>.
The compiler will never let us get away with this (because it doesn't know all of these properties) but we can let it know that it should trust us, with a new struct:
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-keyword">struct</span> <span class="org-type">TrustMe</span>&lt;<span class="org-type">T</span>&gt; {
    <span class="org-keyword">pub</span> <span class="org-variable-name">inner</span>: <span class="org-type">T</span>
}

<span class="org-rust-unsafe">unsafe</span> <span class="org-keyword">impl</span>&lt;<span class="org-type">T</span>&gt; <span class="org-type">Send</span> <span class="org-keyword">for</span> <span class="org-type">TrustMe</span>&lt;<span class="org-type">T</span>&gt; {}
</pre>
</div>

<p>
This will tell the compiler "yes, this thing is <code>Send</code>", even when it actually isn't, so the implementation of the trait <code>Send</code> is unsafe.
</p>

<p>
Now, we can create a <code>Send=able =TrustMe&lt;Arc&lt;T&gt;&gt;</code>, and the compiler will trust us when we share these =Arc&lt;T&gt;=s between threads.
</p>

<p>
Now, lets add this to our GC:
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-doc">/// A garbage collector for Arc&lt;T&gt; pointers</span>
<span class="org-keyword">struct</span> <span class="org-type">GC</span>&lt;<span class="org-variable-name">T</span>: <span class="org-type">Send</span>&gt; {
    <span class="org-variable-name">pool</span>: <span class="org-type">Vec</span>&lt;<span class="org-type">TrustMe</span>&lt;<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;&gt;,
    <span class="org-variable-name">thread</span>: <span class="org-type">Option</span>&lt;<span class="org-constant">thread</span>::<span class="org-type">JoinHandle</span>&lt;()&gt;&gt;,
}

<span class="org-keyword">impl</span>&lt;<span class="org-variable-name">T</span>: <span class="org-type">Send</span>&gt; <span class="org-type">GC</span>&lt;<span class="org-type">T</span>&gt; {
    <span class="org-comment-delimiter">// </span><span class="org-comment">private. cleans up any dead pointers in a pool</span>
    <span class="org-keyword">fn</span> <span class="org-function-name">cleanup</span>(<span class="org-variable-name">pool</span>: &amp;<span class="org-keyword">mut</span> <span class="org-type">Vec</span>&lt;<span class="org-type">TrustMe</span>&lt;<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;&gt;) {
        pool.retain(|<span class="org-variable-name">e</span>: &amp;<span class="org-type">TrustMe</span>&lt;<span class="org-type">Arc</span>&lt;_&gt;&gt;| {
            <span class="org-keyword">if</span> <span class="org-type">Arc</span>::strong_count(&amp;e.inner) &gt; 1 {
                <span class="org-keyword">return</span> <span class="org-keyword">true</span>
            } <span class="org-keyword">else</span> {
                <span class="org-keyword">return</span> <span class="org-keyword">false</span>
            }
        });
    }

    <span class="org-keyword">pub</span> <span class="org-keyword">fn</span> <span class="org-function-name">new</span>() -&gt; <span class="org-type">Self</span> {
        <span class="org-keyword">let</span> <span class="org-keyword">mut</span> <span class="org-variable-name">pool</span> = <span class="org-type">Vec</span>::new();

        <span class="org-comment-delimiter">// </span><span class="org-comment">create a closure which will become a new thread</span>
        <span class="org-keyword">let</span> <span class="org-variable-name">gc</span> = || {
            <span class="org-keyword">loop</span> {
                <span class="org-type">GC</span>::cleanup(&amp;<span class="org-keyword">mut</span> pool);

                <span class="org-comment-delimiter">// </span><span class="org-comment">wait for 100 milliseconds, then scan again</span>
                <span class="org-keyword">let</span> <span class="org-variable-name">sleep</span> = <span class="org-constant">std</span>::<span class="org-constant">time</span>::<span class="org-type">Duration</span>::from_millis(100);
                <span class="org-constant">thread</span>::sleep(sleep);
            }
        };

        <span class="org-comment-delimiter">// </span><span class="org-comment">spawns a new thread and returns a handle to the thread</span>
        <span class="org-keyword">let</span> <span class="org-variable-name">gc_thread</span> = <span class="org-constant">thread</span>::spawn(gc);

        <span class="org-type">GC</span> {
            <span class="org-variable-name">pool</span>:   pool,
            <span class="org-variable-name">thread</span>: <span class="org-type">Some</span>(gc_thread),
        }
    }

    <span class="org-keyword">pub</span> <span class="org-keyword">fn</span> <span class="org-function-name">track</span>(&amp;<span class="org-keyword">mut</span> <span class="org-keyword">self</span>, <span class="org-variable-name">t</span>: <span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;) {
        <span class="org-keyword">let</span> <span class="org-variable-name">t</span> = <span class="org-type">TrustMe</span> { <span class="org-variable-name">inner</span>: t };
        <span class="org-keyword">self</span>.pool.push(t);
    }
}
</pre>
</div>

<p>
<a href="https://play.rust-lang.org/?gist=b23d5e7a7541eda3096daac685d309bf&amp;version=stable&amp;backtrace=0">Rust playground link</a>
</p>

<p>
When we try to compile this, we get YET ANOTHER compiler error.
This time, the compiler is whining at us with "the parameter type <code>T</code> may not live long enough".
This error message is frustrating, but, we are using Rust because we want to be very careful with memory safety, so lets try to keep going.
</p>

<p>
The new thread that we have created could run until the termination of the program, so any data which the thread might be holding onto also must be able to live until the termination of the program.
</p>

<p>
The compiler is telling us that we need to add a "lifetime specifier" to our type <code>T</code>.
In this case, it is telling us that the lifetime of any <code>T</code> which is managed by the GC must be <code>'static</code>.
The <code>'static</code> lifetime indicates that values of type <code>T + 'static</code> <b>might</b> live for the entire duration of the program.
</p>

<p>
This might seem excessive, but, it is not possible for the compiler to determine when in the program our thread will terminate (if it could we would have solved the halting problem), so the maximum lifetime MUST potentially be the entire duration of the program.
Note that, this doesn't mean that all the values stored in the GC will necessarily live for the entire lifetime of the program (if they did, we wouldn't be cleaning up garbage).
This condition just means that they might live that long.
</p>

<p>
Anyway, we can now add the <code>+ 'static</code> specifier the compiler has asked us to add, and try to compile this one more time.
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-doc">/// A garbage collector for Arc&lt;T&gt; pointers</span>
<span class="org-keyword">struct</span> <span class="org-type">GC</span>&lt;<span class="org-variable-name">T</span>: <span class="org-type">Send</span> + '<span class="org-keyword">static</span>&gt; {

<span class="org-comment-delimiter">// </span><span class="org-comment">...</span>

<span class="org-keyword">impl</span>&lt;<span class="org-variable-name">T</span>: <span class="org-type">Send</span> + '<span class="org-keyword">static</span>&gt; <span class="org-type">GC</span>&lt;<span class="org-type">T</span>&gt; {

<span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
</pre>
</div>

<p>
GUESS WHAT IT DIDN'T WORK.
</p>

<div class="org-src-container">
<pre class="src src-rust">error[<span class="org-type">E0373</span>]: closure may outlive the current function, but it borrows `pool`, which is owned by the current function
   --&gt; &lt;anon&gt;:<span class="org-variable-name">149</span>:18
    |
149 |         <span class="org-keyword">let</span> <span class="org-variable-name">gc</span> = || {
    |                  ^^ may outlive borrowed value `pool`
150 |             <span class="org-keyword">loop</span> {
151 |                 <span class="org-type">GC</span>::cleanup(&amp;<span class="org-keyword">mut</span> pool);
    |                                  ---- `pool` is borrowed here
    |
<span class="org-variable-name">help</span>: to force the closure to take ownership of `pool` (and any other referenced variables), <span class="org-keyword">use</span> <span class="org-constant">the</span> `<span class="org-keyword">move</span>` keyword, <span class="org-keyword">as</span> <span class="org-variable-name">shown</span>:
    |         <span class="org-keyword">let</span> <span class="org-variable-name">gc</span> = <span class="org-keyword">move</span> || {

<span class="org-variable-name">error</span>: aborting due to previous error
</pre>
</div>

<p>
Once again, this is a good thing, I promise!
Now, the compiler is trying to tell us that the vector named <code>pool</code> is being accessed from two different places.
The compiler wants us to have the new thread take ownership of the vector, but this highlights an interesting problem.
We need to allow both the GC thread, and any other non-realtime thread, to access the vector, at the same time.
</p>

<p>
The compiler has prevented us from accessing the same data from multiple threads.
</p>

<p>
To solve this, we can just wrap the vector in a <code>Mutex</code> <b><b>and</b></b> an <code>Arc</code>.
The <code>Arc</code> allows us to create one instance of the vector on the heap, and the <code>Mutex</code> makes sure that only one thread can access the heap allocated vector at any given time.
</p>

<p>
Here are most of the changes:
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-comment-delimiter">// </span><span class="org-comment">introduce some news type aliases to make life a little bit easier</span>
<span class="org-keyword">type</span> <span class="org-type">TrustedArc</span>&lt;<span class="org-type">T</span>&gt; = <span class="org-type">TrustMe</span>&lt;<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;;
<span class="org-keyword">type</span> <span class="org-type">ArcPool</span>&lt;<span class="org-type">T</span>&gt; = <span class="org-type">Vec</span>&lt;<span class="org-type">TrustedArc</span>&lt;<span class="org-type">T</span>&gt;&gt;;

<span class="org-doc">/// A garbage collector for Arc&lt;T&gt; pointers</span>
<span class="org-keyword">struct</span> <span class="org-type">GC</span>&lt;<span class="org-variable-name">T</span>: <span class="org-type">Send</span> + '<span class="org-keyword">static</span>&gt; {
    <span class="org-variable-name">pool</span>: <span class="org-type">Mutex</span>&lt;<span class="org-type">Arc</span>&lt;<span class="org-type">ArcPool</span>&lt;<span class="org-type">T</span>&gt;&gt;&gt;,
    <span class="org-variable-name">thread</span>: <span class="org-type">Option</span>&lt;<span class="org-constant">thread</span>::<span class="org-type">JoinHandle</span>&lt;()&gt;&gt;,
}

<span class="org-comment-delimiter">// </span><span class="org-comment">...</span>

<span class="org-keyword">impl</span>&lt;<span class="org-variable-name">T</span>: <span class="org-type">Send</span> + '<span class="org-keyword">static</span>&gt; <span class="org-type">GC</span>&lt;<span class="org-type">T</span>&gt; {
    <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
    <span class="org-keyword">pub</span> <span class="org-keyword">fn</span> <span class="org-function-name">new</span>() -&gt; <span class="org-type">Self</span> {
        <span class="org-keyword">let</span> <span class="org-variable-name">pool</span> = <span class="org-type">Arc</span>::new(<span class="org-type">Mutex</span>::new(<span class="org-type">Vec</span>::new()));

        <span class="org-comment-delimiter">// </span><span class="org-comment">create a copy of the pool. The GC thread will own this clone</span>
        <span class="org-comment-delimiter">// </span><span class="org-comment">and the reference count will be incremented by one</span>
        <span class="org-keyword">let</span> <span class="org-variable-name">thread_arc_copy</span> = pool.clone();

        <span class="org-comment-delimiter">// </span><span class="org-comment">create a closure which will become a new thread</span>
        <span class="org-keyword">let</span> <span class="org-variable-name">gc</span> = <span class="org-keyword">move</span> || {
            <span class="org-keyword">loop</span> {
                <span class="org-comment-delimiter">// </span><span class="org-comment">lock the mutex, then let go of it.</span>
                <span class="org-comment-delimiter">// </span><span class="org-comment">If we hold the mutex, the UI thread will be blocked every time it asks the</span>
                <span class="org-comment-delimiter">// </span><span class="org-comment">collector to track something.</span>
                {
                    <span class="org-keyword">let</span> <span class="org-keyword">mut</span> <span class="org-variable-name">pool</span> = thread_arc_copy.lock().unwrap();
                    <span class="org-type">GC</span>::cleanup(&amp;<span class="org-keyword">mut</span> pool);
                }

                <span class="org-comment-delimiter">// </span><span class="org-comment">wait for a bit, then scan again</span>
                <span class="org-keyword">let</span> <span class="org-variable-name">sleep</span> = <span class="org-constant">std</span>::<span class="org-constant">time</span>::<span class="org-type">Duration</span>::from_millis(5);
                <span class="org-constant">thread</span>::sleep(sleep);

            }
        };

        <span class="org-comment-delimiter">// </span><span class="org-comment">....</span>
    }

    <span class="org-keyword">pub</span> <span class="org-keyword">fn</span> <span class="org-function-name">track</span>(&amp;<span class="org-keyword">mut</span> <span class="org-keyword">self</span>, <span class="org-variable-name">t</span>: <span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;) {
        <span class="org-keyword">let</span> <span class="org-variable-name">t</span> = <span class="org-type">TrustMe</span> { <span class="org-variable-name">inner</span>: t };
        <span class="org-keyword">let</span> <span class="org-keyword">mut</span> <span class="org-variable-name">pool</span> = <span class="org-keyword">self</span>.pool.lock().unwrap();
        pool.push(t);
    }
}
</pre>
</div>

<p>
We can finally compile this!
Here's a link to the <a href="https://play.rust-lang.org/?gist=7f41622e104d07f9b106495c2a5373a7&amp;version=nightly&amp;backtrace=0">Rust playground</a>.
Note that you will need to make sure you compile with the "Nightly" channel.
</p>

<p>
There are only a few things left to do.
</p>
</div>

<div id="outline-container-org3176522" class="outline-3">
<h3 id="org3176522">Start and Stop the GC</h3>
<div class="outline-text-3" id="text-org3176522">
<p>
The GC thread that we have created will never terminate.
</p>

<p>
Ideally, when the GC goes out of scope, it will shut down the GC thread and clean up any tracked memory (if it can).
Any <code>Arc</code>  which can't be freed when the GC is shut down will not be freed, but (this is important) the reference count will drop by one.
Now, if one of the previously tracked <code>Arc</code> s goes out of scope, it will be freed on whatever thread drops it (this could be the realtime thread!)
</p>

<p>
So, as long as the realtime thread keeps running, we must keep the GC thread running.
</p>

<p>
First, edit main:
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-keyword">fn</span> <span class="org-function-name">main</span>() {
    <span class="org-comment-delimiter">// </span><span class="org-comment">start the collector</span>
    <span class="org-keyword">let</span> <span class="org-variable-name">collector</span> = <span class="org-type">GC</span>::new();

    <span class="org-comment-delimiter">// </span><span class="org-comment">create the channels</span>
    <span class="org-keyword">let</span> (tx, rx) = <span class="org-constant">mpsc</span>::sync_channel(0);

    <span class="org-comment-delimiter">// </span><span class="org-comment">set up both of the threads</span>
    <span class="org-keyword">let</span> <span class="org-variable-name">rt</span> = <span class="org-type">RealtimeThread</span>::new(rx);
    <span class="org-keyword">let</span> <span class="org-variable-name">ui</span> = <span class="org-type">UIThread</span>::new(tx);

    <span class="org-comment-delimiter">// </span><span class="org-comment">start the threads</span>
    run_threads(rt, ui);

    <span class="org-comment-delimiter">// </span><span class="org-comment">GC thread will be shutdown here, where the GC goes out of scope</span>
}
</pre>
</div>

<p>
Then, edit the <code>UIThread</code> struct appropriately.
</p>

<div class="org-src-container">
<pre class="src src-rust">
<span class="org-keyword">struct</span> <span class="org-type">UIThread</span> {
    <span class="org-variable-name">outgoing</span>: <span class="org-constant">mpsc</span>::<span class="org-type">SyncSender</span>&lt;<span class="org-type">Message</span>&gt;,
    <span class="org-variable-name">collector</span>: <span class="org-type">GC</span>&lt;<span class="org-type">Samples</span>&gt;
}

<span class="org-keyword">impl</span> <span class="org-type">UIThread</span> {
    <span class="org-keyword">fn</span> <span class="org-function-name">new</span>(<span class="org-variable-name">outgoing</span>: <span class="org-constant">mpsc</span>::<span class="org-type">SyncSender</span>&lt;<span class="org-type">Message</span>&gt;, <span class="org-variable-name">collector</span>: <span class="org-type">GC</span>&lt;<span class="org-type">Samples</span>&gt;) -&gt; <span class="org-type">Self</span> {
        <span class="org-type">UIThread</span> { <span class="org-variable-name">outgoing</span>: outgoing, <span class="org-variable-name">collector</span>: collector }
    }

    <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
}
</pre>
</div>

<p>
Next, update the <code>UIThread::run</code> method:
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-doc">/// All of the UI thread code</span>
<span class="org-keyword">fn</span> <span class="org-function-name">run</span>(&amp;<span class="org-keyword">mut</span> <span class="org-keyword">self</span>) {
    <span class="org-comment-delimiter">// </span><span class="org-comment">create 5 "ui events"</span>
    <span class="org-keyword">for</span> <span class="org-variable-name">i</span> <span class="org-keyword">in</span> 0..5 {
        <span class="org-keyword">let</span> <span class="org-variable-name">volume</span> = i <span class="org-keyword">as</span> <span class="org-type">f32</span> / 5.0;
        <span class="org-keyword">let</span> <span class="org-variable-name">samples</span> = <span class="org-type">Arc</span>::new(<span class="org-keyword">self</span>.compute_samples(volume));

        <span class="org-comment-delimiter">// </span><span class="org-comment">tell the GC thread to track our list of samples</span>
        <span class="org-keyword">self</span>.collector.track(samples.clone());

        <span class="org-comment-delimiter">// </span><span class="org-comment">send the samples to the other thread</span>
        <span class="org-rust-builtin-formatting-macro">println!</span>(<span class="org-string">"[ui] sending new samples. Second sample: </span><span class="org-rust-string-interpolation">{}</span><span class="org-string">"</span>, samples[1]);
        <span class="org-keyword">self</span>.outgoing.send(<span class="org-type">Message</span>::<span class="org-type">NewSamples</span>(samples)).unwrap();
    }

    <span class="org-comment-delimiter">// </span><span class="org-comment">tell the other thread to shutdown</span>
    <span class="org-keyword">self</span>.outgoing.send(<span class="org-type">Message</span>::<span class="org-type">Shutdown</span>).unwrap();
}
</pre>
</div>
</div>
</div>

<div id="outline-container-org7138c48" class="outline-3">
<h3 id="org7138c48">Drop the GC</h3>
<div class="outline-text-3" id="text-org7138c48">
<p>
Rust will make sure that <code>Drop</code> is called when the struct goes out of scope.
This gives us a change to shut down the GC thread.
We also set up a shared atomic boolean to indicate when the GC thread should shut down.
</p>

<p>
Here is most of that:
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-doc">/// A garbage collector for Arc&lt;T&gt; pointers</span>
<span class="org-keyword">struct</span> <span class="org-type">GC</span>&lt;<span class="org-variable-name">T</span>: <span class="org-type">Send</span> + '<span class="org-keyword">static</span>&gt; {
    <span class="org-variable-name">pool</span>: <span class="org-type">Arc</span>&lt;<span class="org-type">Mutex</span>&lt;<span class="org-type">ArcPool</span>&lt;<span class="org-type">T</span>&gt;&gt;&gt;,
    <span class="org-variable-name">thread</span>: <span class="org-type">Option</span>&lt;<span class="org-constant">thread</span>::<span class="org-type">JoinHandle</span>&lt;()&gt;&gt;,
    <span class="org-variable-name">running</span>: <span class="org-type">Arc</span>&lt;<span class="org-type">AtomicBool</span>&gt;,
}

<span class="org-comment-delimiter">// </span><span class="org-comment">initialize the running flag to false in GC::new</span>

<span class="org-comment-delimiter">// </span><span class="org-comment">....</span>

<span class="org-keyword">impl</span>&lt;<span class="org-variable-name">T</span>: <span class="org-type">Send</span> + '<span class="org-keyword">static</span>&gt; <span class="org-type">Drop</span> <span class="org-keyword">for</span> <span class="org-type">GC</span>&lt;<span class="org-type">T</span>&gt; {
    <span class="org-keyword">fn</span> <span class="org-function-name">drop</span>(&amp;<span class="org-keyword">mut</span> <span class="org-keyword">self</span>) {
        <span class="org-keyword">self</span>.running.store(<span class="org-keyword">false</span>, <span class="org-type">Ordering</span>::<span class="org-type">Relaxed</span>);
        <span class="org-keyword">match</span> <span class="org-keyword">self</span>.thread.take() {
            <span class="org-type">Some</span>(t) =&gt; t.join().unwrap(),
            <span class="org-type">None</span>    =&gt; ()
        };
    }
}
</pre>
</div>

<p>
And, here's the <a href="https://play.rust-lang.org/?gist=c33dec5b9aad44864035de4c81c1f492&amp;version=nightly&amp;backtrace=0">Rust playground link</a>.
You may have some trouble getting this to run (timeouts occur), but I promise it works sometimes.
</p>

<p>
Example output:
</p>

<pre class="example">
[realtime] thread started
[ui] thread started
[ui] sending new samples. Second sample: 0
[ui] sending new samples. Second sample: 0.01960343
[realtime] received new samples. Second sample: 0
[ui] sending new samples. Second sample: 0.03920686
[realtime] received new samples. Second sample: 0.01960343
[realtime] received new samples. Second sample: 0.03920686
[ui] thread shutting down
[realtime] thread shutting down
</pre>
</div>
</div>

<div id="outline-container-orgd507a80" class="outline-3">
<h3 id="orgd507a80">Proof</h3>
<div class="outline-text-3" id="text-orgd507a80">
<p>
Let's add some logging so we can see when things are getting freed:
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-comment-delimiter">// </span><span class="org-comment">private. cleans up any dead pointers in a pool</span>
<span class="org-keyword">fn</span> <span class="org-function-name">cleanup</span>(<span class="org-variable-name">pool</span>: &amp;<span class="org-keyword">mut</span> <span class="org-type">Vec</span>&lt;<span class="org-type">TrustMe</span>&lt;<span class="org-type">Arc</span>&lt;<span class="org-type">T</span>&gt;&gt;&gt;) {
    pool.retain(|<span class="org-variable-name">e</span>: &amp;<span class="org-type">TrustMe</span>&lt;<span class="org-type">Arc</span>&lt;_&gt;&gt;| {
        <span class="org-keyword">if</span> <span class="org-type">Arc</span>::strong_count(&amp;e.inner) &gt; 1 {
            <span class="org-keyword">return</span> <span class="org-keyword">true</span>
        } <span class="org-keyword">else</span> {
            <span class="org-rust-builtin-formatting-macro">println!</span>(<span class="org-string">"[gc] dropping a value!"</span>);
            <span class="org-keyword">return</span> <span class="org-keyword">false</span>
        }
    });
}
</pre>
</div>

<p>
The completed code lives at <a href="https://play.rust-lang.org/?gist=7c48a9e595463cb4b8a2c155feb50234&amp;version=nightly&amp;backtrace=0">this Rust playground link</a>.
</p>

<p>
Example Output:
</p>
<pre class="example">
[realtime] thread started
[ui] thread started
[ui] sending new samples. Second sample: 0
[realtime] received new samples. Second sample: 0
[ui] sending new samples. Second sample: 0.01960343
[realtime] received new samples. Second sample: 0.01960343
[gc] dropping a value!
[ui] sending new samples. Second sample: 0.03920686
[realtime] received new samples. Second sample: 0.03920686
[gc] dropping a value!
[ui] sending new samples. Second sample: 0.058810286
[realtime] received new samples. Second sample: 0.058810286
[gc] dropping a value!
[ui] sending new samples. Second sample: 0.07841372
[realtime] received new samples. Second sample: 0.07841372
[gc] dropping a value!
[ui] thread shutting down
[realtime] thread shutting down
</pre>
</div>
</div>
</div>

<div id="outline-container-org566c7bf" class="outline-2">
<h2 id="org566c7bf">Conclusion</h2>
<div class="outline-text-2" id="text-org566c7bf">
<p>
We did it!
</p>

<p>
For me, this post exemplifies the reasons I am so excited about Rust.
The realtime audio world places us into a world where many programming languages are simply not usable.
Languages with runtimes that may behave unpredictably cannot meet the extremely strict requirements we must meet for correct realtime operation.
Rust allows us to meet all of those requirements and gives us some nice abstractions.
</p>

<p>
On top of that, the Rust compiler meticulously checks for thread safety violations and memory safety violations.
While writing this post, some of the issues the compiler threw at me (<code>'static</code>, for example), are issues I never considered.
The compiler caught me and told me "no," so I had to think about what was actually going on.
</p>

<p>
These checks are absolutely irritating, and sometimes we might want to work around them (like we did with <code>TrustMe</code>).
I'm glad to be exposed to potential issues, even if I have to work around the compiler sometimes (so far).
</p>

<p>
If you made it this far, thank you for reading.
I hope you've learned something interesting (maybe even useful).
</p>

<p>
Discussion on <a href="https://www.reddit.com/r/rust/comments/5qxqo8/show_and_ask_rrust_garbage_collection_for_rust/">reddit</a>.
</p>
</div>
</div>
]]>
    </content>
  </entry>
  <entry>
    <title>Audio programming 101</title>
    <link rel="alternate" type="text/html" href="http://dpzmick.com/posts/2016-12-17-audio-basics.html"/>
    <id>urn:uuid:e6cc9d9c-e414-5af5-99f8-6f442e6b0e29</id>
    <updated>2016-12-17</updated>
    <content type="html">
      <![CDATA[<style>.org-src-container {white-space: pre; font-family: monospace }</style>
               <p>
Recently, I've been working on a synthesizer (the kind that makes
sounds) in Rust. I am hoping to write a large number of little articles
about the things I learn as I work on this project.
</p>

<p>
To start off this series, here's a short article about audio
programming.
</p>

<div id="outline-container-orgc74b899" class="outline-2">
<h2 id="orgc74b899">Digital audio</h2>
<div class="outline-text-2" id="text-orgc74b899">
<p>
To generate audio, audio software sends some digital audio signals to
the audio card. Digital audio signals are just lists of floating point
(decimal) numbers. Think of these numbers as "sound pressure" over time
(see <a href="https://docs.cycling74.com/max5/tutorials/msp-tut/mspdigitalaudio.html">this page</a> for more)
</p>

<p>
Because sound is continuous, we can't record every possible value.
Instead, we take measurements of the sound pressure values at some
evenly spaced interval. For CD quality audio, we take 44100 samples per
second, or, one sample every 23ish microseconds. We might sample a sine
wave like this (from Wikipedia):
</p>


<figure id="org6e4647b">
<img src="../static/sound/sine_samples.png" alt="sine_samples.png" class="light-invert">

<figcaption><span class="figure-number">Figure 1: </span>Samples</figcaption>
</figure>

<p>
The audio card turns these lists of samples into some "real-world"
audio, which is then played through the speakers.
</p>
</div>

<div id="outline-container-orgb10b07f" class="outline-3">
<h3 id="orgb10b07f">Types of audio software</h3>
<div class="outline-text-3" id="text-orgb10b07f">
<p>
Next let's think about a few different kinds of audio software (this
list is by no means complete):
</p>

<ol class="org-ol">
<li>Media players (your browser, whatever you listen to music with, a
game, etc)</li>
<li>Software instruments (think of a virtual piano)</li>
<li>Audio plugins (an equalizer in a music player, effects like
distortion and compression)</li>
<li>Software audio systems</li>
</ol>

<p>
Media players are pretty self explanatory, but the others might need
some explanation. Next on the list is "Software instruments." These are
just pieces of software that can be used to generate sounds. They are
played with external keyboards, or "programmed" with cool user
interfaces.
</p>


<figure id="org8ef327c">
<img src="../static/sound/reason_drums.jpg" alt="reason_drums.jpg"> 

<figcaption><span class="figure-number">Figure 2: </span>drum machine in some software</figcaption>
</figure>

<p>
Next up are audio plugins. These are pieces of software which take audio
as input, transform it in some way, then output the transformed audio.
For example, a graphical equalizer can adjust the volume of different
frequency ranges (make the bass louder, make the treble quieter):
</p>


<figure id="orgd8cfe46">
<img src="../static/sound/itunes_eq.jpg" alt="itunes_eq.jpg">

<figcaption><span class="figure-number">Figure 3: </span>equalizer</figcaption>
</figure>

<p>
Finally, we come to what I'm calling a software audio system. Because
there is only one sound card on your system, any audio you are playing
on your computer must be mixed together, then sent to the audio card. On
windows, using the default audio system, I can mix audio with this
little mixer thing:
</p>


<figure id="org8a46f73">
<img src="../static/sound/win_mixer.png" alt="win_mixer.png">

<figcaption><span class="figure-number">Figure 4: </span>windows mixer</figcaption>
</figure>

<p>
<a href="http://www.jackaudio.org/">Some audio systems</a> may also be able to send audio between
applications, send <a href="https://en.wikipedia.org/wiki/MIDI">MIDI</a> signals, keep audio applications in sync, and
perform many other tasks.
</p>

<p>
The software audio system provides a library which application
developers use to develop audio applications.
</p>
</div>
</div>
</div>

<div id="outline-container-orgbebd7dd" class="outline-2">
<h2 id="orgbebd7dd">Software audio systems</h2>
<div class="outline-text-2" id="text-orgbebd7dd">
<p>
Most software audio systems (as far as I know) tend to work the same
way. There is a realtime thread that generates samples and a bunch of
other threads that deal with everything else. The audio thread is
usually set up by the audio system's library. The library calls a user
provided callback function to get the samples it needs to deliver to the
audio card.
</p>
</div>

<div id="outline-container-orgbd23d3f" class="outline-3">
<h3 id="orgbd23d3f">How fast is realtime?</h3>
<div class="outline-text-3" id="text-orgbd23d3f">
<p>
In the previous section, I claimed that, at 44.1 kHz (the standard CD
sample rate), we need to take one audio sample approximately every 23
microseconds. 23 microseconds seems pretty quick, but 192 kHz, a sample
must be taken about every 5 microseconds (192 kHz is becoming a bit of
an industry standard)!
</p>

<p>
At these speeds, it would not be possible for the audio system to call
our callback function to get every individual sample. Instead, the audio
library system ask us for larger batches of samples. If we simplify the
real world a bit, we can approximate how often our callback function
will be called. Here's a table comparing batch size to the time between
callback function calls (all times in milliseconds):
</p>

<table>


<colgroup>
<col  class="org-right">

<col  class="org-right">

<col  class="org-right">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">Batch Size</th>
<th scope="col" class="org-right">Time between calls @ 44.1 kHz (millis)</th>
<th scope="col" class="org-right">Time between calls @ 192 kHz (millis)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">64</td>
<td class="org-right">1.45</td>
<td class="org-right">0.33</td>
</tr>

<tr>
<td class="org-right">128</td>
<td class="org-right">2.90</td>
<td class="org-right">0.67</td>
</tr>

<tr>
<td class="org-right">256</td>
<td class="org-right">5.80</td>
<td class="org-right">1.33</td>
</tr>

<tr>
<td class="org-right">512</td>
<td class="org-right">11.61</td>
<td class="org-right">2.67</td>
</tr>

<tr>
<td class="org-right">1024</td>
<td class="org-right">23.22</td>
<td class="org-right">5.33</td>
</tr>

<tr>
<td class="org-right">2048</td>
<td class="org-right">46.44</td>
<td class="org-right">10.67</td>
</tr>

<tr>
<td class="org-right">4096</td>
<td class="org-right">92.88</td>
<td class="org-right">21.33</td>
</tr>
</tbody>
</table>

<p>
There are many complicated trade offs between sample rate/and batch
size, so I don't want to get into them now. You can read <a href="http://www.penguinproducer.com/Blog/2011/10/balancing-performance-and-reliability-in-jack/">this</a> for a
bit more information. Long story short, use the smallest batch size
your computer can handle.
</p>

<p>
As audio application developers, we should make sure that our code runs
as quickly as possible no matter what the batch size is. The time we
spend is time other audio applications cannot use. Even if we
theoretically have 5 milliseconds to run, using the entire 5
milliseconds can slow everyone else down.
</p>
</div>
</div>

<div id="outline-container-org1e16b05" class="outline-3">
<h3 id="org1e16b05">Time keeps on ticking</h3>
<div class="outline-text-3" id="text-org1e16b05">
<p>
If our callback function fails to generate samples quickly enough (or
uses up all of the CPU time), the audio system will produce crackles,
pops, and bad sounds. We call these buffer underruns (or xruns).
<b>Avoiding buffer underruns must be our top priority!</b>
</p>

<p>
Everything we do in our callback function must <i>always</i> complete quickly
and in a very predictable amount of time. Unfortunately, this constraint
eliminates many things we often take for granted, including:
</p>

<ul class="org-ul">
<li>Synchronization with locks</li>
<li>Blocking operations</li>
<li>Operations with high worst case runtime</li>
<li>Memory allocation with standard allocators</li>
</ul>

<p>
First, we can't use locks or semaphores or conditional variables or any
of those kinds of things inside of our realtime callback function. If
one of our other threads is holding the lock, it might not let go soon
enough for us to generate our samples on time! If you try to make sure
you locks will always be released quickly, the scheduler might step in
and ruin your plans (this is called
<a href="https://en.wikipedia.org/wiki/Priority_inversion">Priority
Inversion</a>). There are some cases in which it <i>might</i> be okay to use
locks, but, in general, it is a good idea to avoid them.
</p>

<p>
Second, we cannot perform blocking operations in the realtime callback
function. Things that might block include access to the network, access
to a disk, and other system calls which might block while performing a
task. In general, if I/O needs to be performed, it is best to perform
the I/O on another thread and communicate the results to the realtime
thread. There are some interesting subtleties to this, for example, can
the following code perform I/O?
</p>

<div class="org-src-container">
<pre class="src src-C++"><span class="org-type">int</span> <span class="org-function-name">callback</span>(<span class="org-comment-delimiter">/* </span><span class="org-comment">args</span><span class="org-comment-delimiter"> */</span>) {
  <span class="org-type">float</span>* <span class="org-variable-name">samples</span> = <span class="org-comment-delimiter">// </span><span class="org-comment">get a contiguous array of samples in a nonblocking way</span>
  <span class="org-keyword">for</span> (<span class="org-type">size_t</span> <span class="org-variable-name">i</span> = 0; i &lt; N; i++) {
    output_sample( samples[i] );
  }
}
</pre>
</div>

<p>
Unfortunately, it can. If the array of samples is <i>extremely</i> large,
the samples might not all actually be in physical memory. When the
operating system must contend with increasing memory pressure, it may
move some of the <a href="https://en.wikipedia.org/wiki/Page_(computer_memory)">virtual memory pages</a> it manages out of physical
memory. If the page isn't in main memory, the operating system has to
go get it from somewhere. These pages are often moved to a hard disk,
so getting them will require blocking I/O.
</p>

<p>
Luckily, this sort of thing is only an issue if your program uses
extremely large amounts of memory. Audio applications usually do not
have high memory requirements, but, if yours does, you operating system
may provide you with a workaround. On linux, we can use the system call
<code>mlockall</code> to make sure certain pages never leave physical memory:
</p>

<blockquote>
<p>
mlock(), mlock2(), and mlockall() lock part or all of the calling
process's virtual address space into RAM, preventing that memory from
being paged to the swap area.
</p>
</blockquote>

<p>
Next, we want to avoid operations which have a high worst case
runtime.  This can be tricky because some things with bad worst case
runtime things have a reasonable <a href="https://en.wikipedia.org/wiki/Amortized_analysis">amortized</a> runtime.  The canonical
example of this is a <a href="https://en.wikipedia.org/wiki/Dynamic_array">dynamic array</a>. A dynamic array can be inserted
into very quickly most of the time, but every so often it must
reallocate itself and copy all of its data somewhere else. For a large
array, this expensive copy might cause us to miss our deadline every
once and a while. Fortunately, for some data structures, we can push
these worst case costs around and make the operations realtime safe
(see <a href="https://en.wikipedia.org/wiki/Hash_table#Dynamic_resizing">Incremental resizing</a>).
</p>

<p>
Finally, memory allocation with standard library allocators can cause
problems. Memory allocators are usually thread safe, which usually
means that the are locking something. Additionally, allocation
algorithms rarely make any time guarantees; the algorithms they use
can have very poor worst case runtimes. Standard library allocators
break both of our other rules! Luckily, we can still perform dynamic
memory allocation if we use <a href="http://www.gii.upv.es/tlsf/">specially designed allocators</a> or <a href="https://github.com/supercollider/supercollider/blob/master/common/SC_AllocPool.h">pool
allocators</a> which do not violate our realtime constraints.
</p>
</div>
</div>
</div>

<div id="outline-container-orgd47d746" class="outline-2">
<h2 id="orgd47d746">What do we do?</h2>
<div class="outline-text-2" id="text-orgd47d746">
<p>
In general, there are a few cool tricks we can use to design around
these problems, but I'm not going to discuss any of them in this post.
Future posts will discuss possible solutions and many of their
tradeoffs, eventually.
</p>

<p>
If you can't wait, here are some interesting things you can read to
learn more:
</p>
<ul class="org-ul">
<li><a href="http://www.cs.cmu.edu/~rbd/doc/icmc2005workshop/real-time-systems-concepts-design-patterns.pdf">Overview of Design Patterns for Real-Time Computer Music Systems</a></li>
<li><a href="http://supercolliderbook.net/rossbencinach26.pdf">SuperCollider implementation details</a> from the <a href="http://supercolliderbook.net/">SuperCollider book</a></li>
<li><a href="http://tim.klingt.org/publications/tim_blechmann_supernova.pdf">Supernova for SuperCollider</a> a Masters thesis discussing some of these issues</li>
<li><a href="http://www.rossbencina.com/code/real-time-audio-programming-101-time-waits-for-nothing">this excellent blog post</a></li>
</ul>

<p>
See you next time!
</p>
</div>
</div>
]]>
    </content>
  </entry>
  <entry>
    <title>Rust Image Processing DSL</title>
    <link rel="alternate" type="text/html" href="http://dpzmick.com/posts/2016-08-11-rust-jit-image-processing.html"/>
    <id>urn:uuid:752c6e13-a7df-5c0f-9b16-76f41b9b53a2</id>
    <updated>2016-08-11</updated>
    <content type="html">
      <![CDATA[<style>.org-src-container {white-space: pre; font-family: monospace }</style>
               <p>
<a href="https://github.com/dpzmick/rust_img_dsl">This Project</a> is essentially an attempt to recreate some of the <a href="http://halide-lang.org/">Halide</a> project in Rust as a means of learning that Rust language.
Halide is a really clever C++ library that allows programmers to define image processing algorithms in domain specific language which are compiled according to some sort of execution strategy.
These strategies might be "tile for cache efficiency" or "optimize for execution on a GPU."
The project is definitely worth poking at for a few minutes.
</p>

<p>
The project I will be discussing in this blog post is an implementation of the first "half" of Halide, using <a href="https://www.rust-lang.org">Rust</a>.
Specifically, I've implemented a simple DSL for image processing which is JIT compiled with LLVM.
I picked this project mostly to learn rust, so my result is certainly not production code but it may still be interesting to read a bit about.
</p>

<div id="outline-container-org2e7e755" class="outline-2">
<h2 id="org2e7e755">Annotated example</h2>
<div class="outline-text-2" id="text-org2e7e755">
<p>
Before jumping into a discussion about how all of this works, lets look at an example of the DSL.
In this example, we will define the sobel operator, then process an images with it.
For a great overview of the sobel operator, check out <a href="https://blog.saush.com/2011/04/20/edge-detection-with-the-sobel-operator-in-ruby/">this article</a>.
</p>

<p>
In the DSL, there are things to worry about: Functions and Chains.
A function is a single unit of work that takes an \((x,y)\) coordinate and an arbitrary number of inputs.
For example, suppose we have a function \(Grad(x,y)\) that returns the magnitude of the gradient of two images \(I_1\) and \(I_2\) at the point \((x,y)\).
We might denote this function with mathematical notation as:
</p>

<p>
\[ Grad(x,y) = \sqrt{I_1(x,y)^2 + I_2(x,y)^2} \]
</p>

<p>
In the DSL I have defined, we would denote this operation in a similar manner, sans syntactic differences:
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-comment-delimiter">// </span><span class="org-comment">create a new function named grad</span>
<span class="org-comment-delimiter">// </span><span class="org-comment">Function::new takes a number and a lambda as an argument.</span>
<span class="org-comment-delimiter">// </span><span class="org-comment">The number indicates how many inputs the function has</span>
<span class="org-comment-delimiter">// </span><span class="org-comment">This lambda is always called with (x,y) coordinate values</span>
<span class="org-comment-delimiter">// </span><span class="org-comment">and an array of inputs of the length specified.</span>
<span class="org-keyword">let</span> <span class="org-variable-name">grad</span> = <span class="org-type">Function</span>::new(2, |x, y, inputs| {
    <span class="org-comment-delimiter">// </span><span class="org-comment">first we pull out references to the InputExpressions representing our inputs</span>
    <span class="org-keyword">let</span> <span class="org-variable-name">input0</span> = &amp;inputs[0];
    <span class="org-keyword">let</span> <span class="org-variable-name">input1</span> = &amp;inputs[1];

    <span class="org-comment-delimiter">// </span><span class="org-comment">compute the squares using the input expressions</span>
    <span class="org-comment-delimiter">// </span><span class="org-comment">Notice that x and y are both treated like functions.</span>
    <span class="org-comment-delimiter">// </span><span class="org-comment">This is essentially a hack to get around the way I've stored the AST</span>
    <span class="org-keyword">let</span> <span class="org-variable-name">t1</span> = input0(x(), y()) * input0(x(), y());
    <span class="org-keyword">let</span> <span class="org-variable-name">t2</span> = input1(x(), y()) * input1(x(), y());

    <span class="org-comment-delimiter">// </span><span class="org-comment">Compute the sum and the square root of the sum</span>
    <span class="org-comment-delimiter">// </span><span class="org-comment">The last expression generated by this lambda is result of the function we are defining</span>
    <span class="org-comment-delimiter">// </span><span class="org-comment">The Box::new trick is needed, again, because of the way I've store the AST</span>
    <span class="org-type">Box</span>::new(<span class="org-type">SqrtExpr</span>::new(t1 + t2))
});
</pre>
</div>

<p>
This isn't the most beautiful way to build a representation of our function, but it works and I learned a lot implementing the magic that makes it work.
Each function stores is a syntax tree representing the expression that the function computes.
The syntax tree defined by the <code>grad</code> function looks something like this:
</p>

<p>
<img src='/static/rust_img/ast.svg' class='light-invert' />
</p>

<p>
There is also helper function that can be used to generate functions which perform a convolution on a single image with a kernel matrix
For example, to generate a function that takes a single image as input and returns the convolution with the horizontal sobel matrix, use the following code:
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-keyword">let</span> <span class="org-variable-name">sobel_x</span> = [[-1, 0, 1],
               [-2, 0, 2],
               [-1, 0, 1]];

<span class="org-keyword">let</span> <span class="org-variable-name">sobel_x</span> = <span class="org-type">Function</span>::gen_3x3_kernel(sobel_x);
</pre>
</div>

<p>
Notice that these functions are defined in a purely functional, mathematical sense.
They do not mutate their inputs, nor do they store any state, nor are they coupled to any particular inputs.
</p>
</div>

<div id="outline-container-org5e5ae7d" class="outline-3">
<h3 id="org5e5ae7d">Connecting functions</h3>
<div class="outline-text-3" id="text-org5e5ae7d">
<p>
Now that we have some abstract functions, we need to compose functions to create something meaningful.
The composition of functions in my DSL is called a function chain.
Chains may be thought of as a stream of pixels, starting from =ImageSource=s, flowing through a number of transformation functions, and finally, resulting in a new image.
ImageSources define the starting inputs for the entire chain.
Then, any number of functions may be chained together.
For example, the entire sobel image processing chain looks something like this:
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-keyword">let</span> <span class="org-variable-name">sobel_x_fun</span> = <span class="org-comment-delimiter">// </span><span class="org-comment">define the sobel_x function as shown above</span>
<span class="org-keyword">let</span> <span class="org-variable-name">sobel_y_fun</span> = <span class="org-comment-delimiter">// </span><span class="org-comment">define the sobel_y function similar to the sobel_x function given above</span>
<span class="org-keyword">let</span> <span class="org-variable-name">grad_fun</span>    = <span class="org-comment-delimiter">// </span><span class="org-comment">define the gradient function, exactly as given above</span>

<span class="org-comment-delimiter">// </span><span class="org-comment">make an ImageSource defining the start of the chain</span>
<span class="org-comment-delimiter">// </span><span class="org-comment">In this case, we only need a single image source</span>
<span class="org-keyword">let</span> <span class="org-variable-name">image</span> = <span class="org-type">ChainLink</span>::<span class="org-type">ImageSource</span>(0);

<span class="org-comment-delimiter">// </span><span class="org-comment">image source pixels flow into sobel_x</span>
<span class="org-keyword">let</span> <span class="org-variable-name">c1</span> = <span class="org-type">ChainLink</span>::link(<span class="org-preprocessor">vec!</span>[&amp;image], &amp;sobel_x_fun);

<span class="org-comment-delimiter">// </span><span class="org-comment">image source pixels flow into sobel_y</span>
<span class="org-keyword">let</span> <span class="org-variable-name">c2</span> = <span class="org-type">ChainLink</span>::link(<span class="org-preprocessor">vec!</span>[&amp;image], &amp;sobel_y_fun);

<span class="org-comment-delimiter">// </span><span class="org-comment">pixels from sobel_x and sobel_y flow into the gradient function</span>
<span class="org-keyword">let</span> <span class="org-variable-name">c3</span> = <span class="org-type">ChainLink</span>::link(<span class="org-preprocessor">vec!</span>[&amp;c1, &amp;c2], &amp;grad_fun);
</pre>
</div>
</div>
</div>

<div id="outline-container-orgb0c94d4" class="outline-3">
<h3 id="orgb0c94d4">Compiling and executing the chain</h3>
<div class="outline-text-3" id="text-orgb0c94d4">
<p>
Now that we have built a chain representing the entire sobel algorithm, we only need to compile the chain and use the chain to process an image:
</p>

<div class="org-src-container">
<pre class="src src-rust"><span class="org-keyword">let</span> <span class="org-variable-name">cc</span> = c3.compile(); <span class="org-comment-delimiter">// </span><span class="org-comment">create a compiled chain for this chain</span>
<span class="org-keyword">let</span> <span class="org-variable-name">resulting_image</span> = cc.run_on(&amp;[&amp;my_image]);
</pre>
</div>

<p>
Invoking <code>.compile()</code> on an image chain compiles each function in the chain provided into an LLVM module, optimizes the module with LLVM's optimizer, and uses LLVM's MCJIT to compile to machine code.
A compiled chain essentially just holds a function pointer to a function which will be called when the chain is executed (and some things used for bookkeeping).
The only work I had to do to go from AST to function pointer is code generation.
For this reason, LLVM is decidedly <b>awesome</b>.
</p>

<p>
Note: For the full sobel code, see <a href="https://github.com/dpzmick/rust_img_dsl/blob/master/examples/sobel.rs">sobel.rs</a>.
</p>
</div>
</div>
</div>

<div id="outline-container-orgba72ec4" class="outline-2">
<h2 id="orgba72ec4">Details, Details, Details</h2>
<div class="outline-text-2" id="text-orgba72ec4">
<p>
There's lots of little details which may be interesting to discuss, but I'm only going to discuss the compilation method.
First, we need some slightly more rigorous definitions of things:
</p>

<ul class="org-ul">
<li>All of the user defined functions take two 64 bit integer values and a list of inputs. They return a 64 bit integer value.</li>
<li>Because functions themselves have no concept of chaining (the gradient function doesn't call <code>sobel_x</code> and <code>sobel_y</code>), the "inputs" to a user defined function can be thought of as function-pointers which will eventually be resolved to real functions, although this is not how they are implemented.</li>
<li>All of the system-defined expressions (eg. <code>SqrtExpr</code>) take a 64 bit integer and return a 64 bit integer.</li>
</ul>

<p>
The compilation strategy for the DSL is very simple: Every DSL function is compiled into a function with a signature that would look something like this in C:
</p>
<div class="org-src-container">
<pre class="src src-c"><span class="org-keyword">inline</span> <span class="org-type">int64_t</span> <span class="org-function-name">function</span>(<span class="org-type">int64_t</span> <span class="org-variable-name">x</span>, <span class="org-type">int64_t</span> <span class="org-variable-name">y</span>, <span class="org-type">image</span> <span class="org-variable-name">inputs</span>[], <span class="org-type">size_t</span> <span class="org-variable-name">num_inputs</span>);
</pre>
</div>

<p>
The array of image inputs provided here is <b><b>not</b></b> equivalent to the list of the inputs given to the DSL function.
The inputs given to the DSL function are resolved to other compiled functions (using the chain) during code generation, so our generated <code>grad</code> function will directly call the <code>sobel_x</code> and <code>sobel_y</code> functions.
</p>

<p>
Since every value is a 64 bit integer, the code generation for an expression essentially just involves spitting out adds and multiplies for integers.
The generated <code>grad</code> code roughly corresponds to:
</p>

<div class="org-src-container">
<pre class="src src-c"><span class="org-keyword">inline</span> <span class="org-type">int64_t</span> <span class="org-function-name">grad</span>(<span class="org-type">int64_t</span> <span class="org-variable-name">x</span>, <span class="org-type">int64_t</span> <span class="org-variable-name">y</span>, <span class="org-type">image</span> <span class="org-variable-name">inputs</span>[], <span class="org-type">size_t</span> <span class="org-variable-name">num_inputs</span>) {
    <span class="org-type">int64_t</span> <span class="org-variable-name">partial1</span> = sobel_x(x, y, inputs, num_inputs) * sobel_x(x, y, inputs, num_inputs);
    <span class="org-type">int64_t</span> <span class="org-variable-name">partial2</span> = sobel_y(x, y, inputs, num_inputs) * sobel_x(x, y, inputs, num_inputs);
    <span class="org-type">int64_t</span> <span class="org-variable-name">partial3</span> = partial1 + partial2;

    <span class="org-keyword">return</span> core_isqrt(partial3);
}
</pre>
</div>
</div>
</div>

<div id="outline-container-orge530507" class="outline-2">
<h2 id="orge530507">Execution and image inputs</h2>
<div class="outline-text-2" id="text-orge530507">
<p>
A driver function is injected into the module.
This function performs some bookkeeping tasks, then just loops over the pixels in the output image, calling the appropriate function (whichever was last in the chain) for every pixel:
</p>

<div class="org-src-container">
<pre class="src src-c"><span class="org-keyword">for</span> (<span class="org-type">int</span> <span class="org-variable-name">x</span> = 0; x &lt; output.width; x++) {
  <span class="org-keyword">for</span> (<span class="org-type">int</span> <span class="org-variable-name">y</span> = 0; y &lt; output.height; y++) {
    <span class="org-type">int64_t</span> <span class="org-variable-name">res</span> = function(x, y, inputs, num_inputs);

    <span class="org-comment-delimiter">/* </span><span class="org-comment">output image at x, y</span><span class="org-comment-delimiter"> */</span> = (<span class="org-type">uint8_t</span>) res;
  }
</pre>
</div>

<p>
Image inputs (the actual images we are processing), are passed to each function.
When the compiler reaches an <code>ImageSource</code> in the function chain, it emits a call to a function which returns the pixel in the image at a given \((x,y)\) coordinate.
</p>

<p>
For anyone interested, I've dumped the entire LLVM IR module for an unoptimized sobel chain <a href="https://gist.github.com/dpzmick/a55851a9dd5ef87dd925c7f69151c0d0">here</a>.
Some of the code is generated from the file <a href="https://github.com/dpzmick/rust_img_dsl/blob/master/src/core.c">core.c</a> in the github repo for the project, if you need some hints to figure out what's going on here.
The entry point is the function <code>jitfunction</code>.
There's lots of other interesting little idiosyncrasies in this code but I don't have space and you don't have time to read about all of them.
</p>
</div>
</div>

<div id="outline-container-orgaa1a536" class="outline-2">
<h2 id="orgaa1a536">Performance</h2>
<div class="outline-text-2" id="text-orgaa1a536">
<p>
Anyone who knows a little bit about computers and performance is probably hurting a little bit thinking about how this might perform.
You've noticed all of the function calls, don't these have lots of overhead?
Uou've noticed that I'm computing the <code>sobel_x</code> and <code>sobel_y</code> values twice in the gradient function.
Don't worry, it isn't quite so bad.
</p>

<p>
Anyone who knows a fair amount about computers and performance noticed that <code>inline</code> keyword and is wondering if I'm somehow relying on function inlining to extract performance from this technique.
The answer is yes.
Every generated function is marked with an LLVM attribute <code>AlwaysInline</code> which, when combined with the appropriate LLVM optimization passes, guarantees that these functions will always be inlined into their caller.
</p>
</div>

<div id="outline-container-orgd050c4b" class="outline-3">
<h3 id="orgd050c4b">Aside: Function inlining</h3>
<div class="outline-text-3" id="text-orgd050c4b">
<p>
For those who are not totally familiar with the concept of function ininling, here's a quick example (note that the <code>inline</code> keyword in C doesn't guarantee this behavior, it is just a hint to the compiler):
</p>

<div class="org-src-container">
<pre class="src src-c"><span class="org-comment-delimiter">// </span><span class="org-comment">before AlwaysInlinePass</span>
<span class="org-keyword">inline</span> <span class="org-type">int</span> <span class="org-function-name">foo</span>() { <span class="org-keyword">return</span> 12; }

<span class="org-type">int</span> <span class="org-function-name">bar</span>() {
    <span class="org-keyword">for</span> (<span class="org-type">size_t</span> <span class="org-variable-name">i</span> = 0; i &lt; 100; i++) {
        <span class="org-keyword">if</span> (foo() &gt; 13) <span class="org-keyword">return</span> 1;
    }

    <span class="org-keyword">return</span> 0;
}

<span class="org-comment-delimiter">// </span><span class="org-comment">after AlwaysInlinePass</span>
<span class="org-type">int</span> <span class="org-function-name">bar</span>() {
    <span class="org-keyword">for</span> (<span class="org-type">size_t</span> <span class="org-variable-name">i</span> = 0; i &lt; 100; i++) {
        <span class="org-keyword">if</span> (12 &gt; 13) <span class="org-keyword">return</span> 1;
    }

    <span class="org-keyword">return</span> 0;
}
</pre>
</div>

<p>
It may seem that this optimization is useful because it removes function call overhead.
This is true, but it isn't the only critical reason that the optimization is useful.
Many compiler optimizations cannot (or do not) cross function boundaries.
Instead, they often view functions as black boxes about which nothing can be known (this is obviously an oversimplification).
This often makes sense because functions may be defined in different compilations units or in shared libraries, where the compiler cannot access their source.
Function inlining allows the compiler to "see" inside functions, then perform additional optimizations which would not have been possible otherwise.
For example, because the call to <code>foo</code> has been inlined, the compiler can now (easily) optimize the function <code>bar</code> to:
</p>

<div class="org-src-container">
<pre class="src src-c"><span class="org-type">int</span> <span class="org-function-name">bar</span>() {
    <span class="org-keyword">return</span> 0;
}
</pre>
</div>
</div>
</div>

<div id="outline-container-org607cb00" class="outline-3">
<h3 id="org607cb00">Impact</h3>
<div class="outline-text-3" id="text-org607cb00">
<p>
Aggressive function inlining gives me lots of freedom in my code generation.
I can generate code which is totally inefficient, then inline everything and let the compiler do some of its magic.
If course, this isn't a general rule, but for this problem the generated code is highly uniform, doesn't do much with memory (other than reading from <code>readonly</code> images), and has a few other compiler freindly properties.
It the end of the day, LLVM is doing a pretty good job turning my functional style code into a big fat loop and eliminating redundant computations.
If you're interested in looking at the optimized sobel LLVM module, here it is: <a href="https://gist.github.com/dpzmick/95587dbb2972e256e51b888add3364a6">gist</a>.
</p>
</div>
</div>

<div id="outline-container-org16c3d6b" class="outline-3">
<h3 id="org16c3d6b">Numbers</h3>
<div class="outline-text-3" id="text-org16c3d6b">
<p>
To benchmark this code, I compared the JITed code with an implementation of the exact same thing written directly in Rust.
My benchmarking is not extremely rigorous, but I've taken steps to try to create an honest benchmark.
</p>

<p>
Benchmarking environment:
</p>

<ul class="org-ul">
<li>single core, 3.75GB RAM Google cloud compute virtual machine</li>
<li>Ubuntu 16.04</li>
<li>Rust stable (1.10.0)</li>
<li>LLVM 3.9, built from source. Release build with assertions disabled</li>
<li>Sobel code presented above</li>
</ul>

<p>
The benchmark input was a 1.2 gig collection of 3255 images of various sizes, ranging from 160x120 to 6922x6922 pixels.
The image sizes were mixed to try to stave off cache effects and other size-related effects so that I could hopefully just use averages to compare performance.
</p>

<p>
Long story short, the average JIT/native speedup is 1.05x, so the LLVM JITed code is 1.05x faster than the direct Rust implementation (this AST construction time and compile time).
This means that my JIT compiled code runs at the same speed (subject to some jitter) as the native rust code.
</p>

<p>
Here is a plot of image vs average speedup (the images are sorted by the total number of pixels in the image):
</p>


<figure id="orgd0e4de1">
<img src="../static/rust_img/img_vs_speedup.png" alt="img_vs_speedup.png" class="light-invert">

</figure>

<p>
There are many more plots, but the overall conclusion is pretty clear: compared to the Rust, I'm not performing very poorly.
Is this a win? I am not sure, I would need to do many more comparisons.
These results do indicate to me that I have at least achieved reasonable performance, with a dramatically different programming style.
</p>
</div>

<div id="outline-container-org51099be" class="outline-4">
<h4 id="org51099be">Final performance notes</h4>
<div class="outline-text-4" id="text-org51099be">
<p>
It should be noted that these results are not entirely surprising.
Rust is also using LLVM as a backend.
It is probably reasonable to assume that the code Rust is generating looks pretty similar to the code I am generating, although I have not verified this.
</p>

<p>
If you've been nodding your head along with me, I have a confession to make: I've tricked you a little bit.
LLVM is doing an awesome job (considering the code I've generated), but I'm certainly missing out on lots of opportunities for performance because of my code generation technique.
Also, LLVM (or any compiler) should never be expected to be able to totally understand the problem a piece of code is trying to solve and optimize it perfectly.
To really get good performance, I would need to pay attention to caching and quite a few other things which I have totally ignored.
Hand tuned code should (and certainly would) run in circles around the code JIT compiled algorithm I've generated here.
</p>

<p>
If you want something that gives you an awesome DSL AND all sorts of control over cache scheduling and whatnot, take a look at Halide.
If you have no idea what I'm talking about or why any of this matters, take a look at Halide anyway.
The Halide talks give fantastic descriptions of many of the problems it aims to solve.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgb3f6044" class="outline-2">
<h2 id="orgb3f6044">Conclusions</h2>
<div class="outline-text-2" id="text-orgb3f6044">
<p>
Overall, this project was extremely enjoyable.
I had yet another opportunity to fiddle with LLVM, which is always lots of fun (but sometimes very painful).
I learned a little bit about image processing and some of the challenges that arise when shuffling pixels around.
Finally, I learned a little bit of Rust.
I have only one thing to say about Rust: Rust is an amazing language.
Go learn Rust.
</p>
</div>
</div>
]]>
    </content>
  </entry>
  <entry>
    <title>Ants</title>
    <link rel="alternate" type="text/html" href="http://dpzmick.com/posts/2015-06-09-ants.html"/>
    <id>urn:uuid:f7cb1a6c-4d72-5bfa-8496-6613719df54e</id>
    <updated>2015-06-09</updated>
    <content type="html">
      <![CDATA[<style>.org-src-container {white-space: pre; font-family: monospace }</style>
               <p>
For my CS242 final project, I simulated ants with Erlang.
</p>

<p>
In the following video we have a 1000 by 1000 grid with 2000 ants
running around on it. There is food in a small square on the upper left.
The simulation ran for 1 hour (53,840,103 events were recorded).
</p>

<center>
  <iframe width="560" height="315" src="https://www.youtube.com/embed/tThh8m7cezw?start=185" frameborder="0" allowfullscreen>
  </iframe>
</center>

<div id="outline-container-orgd1e2dff" class="outline-2">
<h2 id="orgd1e2dff">Model</h2>
<div class="outline-text-2" id="text-orgd1e2dff">
<p>
I've attempted to capture two behaviors of real ants with my simulation:
ants communicate using pheromones (scents), and they want to find food.
</p>

<p>
Pheromones are a way for an ant to communicate the location of food to
other ants. Each cell in the simulation has a pheromone strength
associated with it. When an ant moves, it uses the pheromone strength of
the cells around it to compute the probability that it will move in that
direction.
</p>

<p>
Additionally, my ants have multiple movement modes.<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup> They can be in
"away from home" mode or "towards home" mode. When an ant starts moving,
it favors moving away from its starting cell. When it finds food, it
switches modes and favors moving towards its starting cell. If it ever
reaches the starting cell, it switches modes again and goes to find more
food. I believe that real ants also use pheromones to find their way
back home (instead of magically remembering the absolute coordinates of
their homes), but I used this method to simplify the model while still
sort of capturing the return to home behavior.
</p>

<p>
There is a mechanism to change the relative importance of distance and
pheromone strength. When an ant finds food, it ignores pheromones until
it gets back home.
</p>

<p>
Food can be placed on any cell in the simulation. Nothing actually
tracks how much food gets carried home, and the supply of food at a
given cell does not change when an ant discovers food. If I were to
continue the project, it would be really interesting to see how much
food actually gets "home" and to include a changing food supply in the
model.
</p>
</div>
</div>

<div id="outline-container-orge426028" class="outline-2">
<h2 id="orge426028">Pheromone Propagation</h2>
<div class="outline-text-2" id="text-orge426028">
<p>
When an ant is at a given cell, it gets the max pheromone strength of
its neighbors. It then checks if the strength of the its current cell is
greater than the max strength of its neighboring cells. If its current
cell has a lower pheromone value than the max of its neighbors, the ant
updates the strength of the current cell to half of the max strength of
its neighbors.
</p>

<p>
When ants find food, they set the pheromone strength of the cell the
food was on to a high value, then they start moving back home. This
should, in theory, cause the pheromone trail to follow them. This
doesn't work as well as I had hoped because the pheromone trail drops
off too quickly, but, it is easy to implement, so I stuck with it.
</p>
</div>
</div>

<div id="outline-container-orgc8cfead" class="outline-2">
<h2 id="orgc8cfead">The Erlang Stuff</h2>
<div class="outline-text-2" id="text-orgc8cfead">
<p>
If you aren't familiar with Erlang, here is a super quick overview of
the concurrency construct in the language.
</p>

<p>
Actors (processes in Erlang terminology) run concurrently and send
messages to each other. These messages are asynchronous, so if actor A
sends a message to actor B, it doesn't wait for B to respond to proceed
with it's next instruction. A message can also contain any sort of data
you care to send.
</p>

<p>
An ant actor and a grid cell actor form the core of my simulation.
</p>

<p>
Cells know who their neighbors are, their pheromone strength, if they
have food on them, and which ant occupies the cell (can be undefined).
Only one ant can occupy a given cell at any moment in time.
</p>

<p>
A cell knows how to handle the following messages (and a few others):
</p>

<ul class="org-ul">
<li><code>who_are_your_neighbors</code> - asks the cell to send a message to someone
with its neighbors</li>
<li><code>move_me_to_you</code> - tells the cell to set its current occupant to the ant
sending the message</li>
<li><code>ive_left</code> - tells the cell that the ant sending the message has left
the cell</li>
</ul>

<p>
Ants know what cell they are on, what direction they are going, where
they started, and a few other less important things.
</p>

<p>
Ants know how to handle these messages (and a few others):
</p>

<ul class="org-ul">
<li><code>wakeup_and_move</code> - tells the ant to try to move somewhere</li>
<li><code>neighbors</code> - a message sent to an ant by a cell when the cell reports
who its neighbors are</li>
<li><code>move_to</code> - tells an ant to change its current cell to some other cell</li>
<li><code>move_failed</code> - tells an ant that its move failed</li>
</ul>
</div>
</div>

<div id="outline-container-org19be83d" class="outline-2">
<h2 id="org19be83d">Simulation</h2>
<div class="outline-text-2" id="text-org19be83d">
<p>
When the simulator starts, it loads a config file
specifying things like the size of the grid and the location of food,
builds the grid of cells, puts ants on the upper edge of the board, then
starts a wakeup_and_move_loop for each ant.
</p>

<p>
These loops tell their respective ants to wake up and perform their move
over and over again until the simulation is shut down.
</p>
</div>

<div id="outline-container-orgf24aadf" class="outline-4">
<h4 id="orgf24aadf">Wakeup_and_move</h4>
<div class="outline-text-4" id="text-orgf24aadf">
<p>
When an ant receives a wakeup_and_move message, it has to figure where
it wants to move, if it can move there, and it needs to perform the
pheromone propagation step. I don't want to let two ants occupy the
same cell at once, but it isn't so bad if one ant is sort of in two
places at once (I think). Those rules motivate the following sequence
of messages for an ant move:
</p>

<ol class="org-ol">
<li>The ant receives a <code>wakeup_and_move</code> message</li>
<li>The ant sends a <code>who_are_your_neighbors</code> message to its current cell</li>
<li>The current cell receives the <code>who_are_your_neighbors</code> message and
sends the ant a <code>neighbors</code> message with the list of its neighboring
cells</li>
<li>The ant decides which of these neighbors to move to using
probabilities explained earlier</li>
<li>The ant performs the pheromone propagation step</li>
<li>The ant sends a <code>move_me_to_you</code> message to its selected cell</li>
<li>The selected cell checks if it is currently occupied

<ul class="org-ul">
<li>If it isn't occupied, it sets its current occupant to the ant
trying to move and sends a you_moved message back to the ant</li>
<li>If it is occupied, it sends a move_failed message back to the ant</li>
</ul></li>

<li>The ant receives either a <code>move_failed</code> or a <code>you_moved</code> message

<ul class="org-ul">
<li>If the ant received a <code>you_moved</code> message, it sends an <code>ive_left</code>
message to its current cell, updates its current cell to the
selected cell, then goes back to sleep</li>
<li>If the ant receives a move_failed message, it does nothing and goes
back to sleep</li>
</ul></li>
</ol>


<figure id="org0bb2ba9">
<img src="../static/ants/sequence.png" alt="sequence.png" class="light-invert">

</figure>

<p>
If you look carefully, you might notice that, between steps 7 and 8, two
cells think they are occupied by the same ant. This prevents collisions
but introduces this strange "ant in flux" state. I would rather accept
the double occupancy issue than the collision issue.
</p>
</div>
</div>
</div>

<div id="outline-container-orgcdf2d23" class="outline-2">
<h2 id="orgcdf2d23">Data</h2>
<div class="outline-text-2" id="text-orgcdf2d23">
<p>
As the simulation runs, the ants are generating all sorts of data that
should probably be recorded somewhere. This is a bit of a challenge
because there is no single entity that knows the state of the entire
system at any given time, so you can't just record a sample of the state
of the simulation somewhere every once in a while.
</p>

<p>
So, I decided that ants should be responsible for reporting their own
movements and should report the pheromone strength changes they make.
For a couple of reasons that don't totally make sense, I decided to
create one file per ant, and have the ants log timestamped (wall clock
time) events to those files. So, for a 2000 ant simulation, I end up
with 2000 ant-event files on disk somewhere.
</p>

<p>
There are all sorts of things wrong with the one file per ant approach.
</p>

<p>
The biggest is speed. Having one file per ant means I have to merge all
of these ant-event files before making a visualization. These files can
get large so this is a slow process (and memory intensive if you write
your script poorly (oops)).
</p>

<p>
Other than speed, one file per ant puts an upper limit on the number of
ants I can simulate at a time because I can't open an unlimited number
of files on any sort of machine. I won't even mention the strange I/O
behavior.
</p>

<p>
Fortunately, computers are fast, events are small, and I have a decent
amount of memory in my laptop, so this technique was "fast enough" given
the scope of the project.
</p>
</div>
</div>

<div id="outline-container-org0c627aa" class="outline-2">
<h2 id="org0c627aa">Making the Video</h2>
<div class="outline-text-2" id="text-org0c627aa">
<p>
Ants are moving all the time in an uncoordinated manner, moving a lot,
and sometimes moving at exactly the same time so there isn't a totally
obvious way to decide when to draw a video frame. I took 100 miliseconds
worth of simulation data (timestamps are in real earth time) and used
the last position of every ant in that time slice to make a frame.
</p>

<p>
MoviePy makes the rest really easy. All I have to do is build a frame by
populating a numpy array, and throw that array at MoviePy. MoviePy
treats that like an array of pixels and spits out a video that plays
some number of frames per second.
</p>
</div>
</div>

<div id="outline-container-org3f64528" class="outline-2">
<h2 id="org3f64528">Conclusions</h2>
<div class="outline-text-2" id="text-org3f64528">
<p>
The naive model of any movement I used almost works. If I were to
improve the pheromone propagation mechanism and add changing food
supplies, I suspect the behavior would become a bit more interesting.
Another next step would be the addition of some obstacles on the grid so
that the "always favor moving away" approach would fail, necessitating a
more intricate "looking for food" mechanism.
</p>

<p>
Erlang is an interesting language and I'm glad I had an opportunity to
fiddle with it, but some of its peculiarities can be annoying. First of
all, the lack of static typing is a pain (I know about dialyzer). It is
also difficult to do things like prioritize certain messages over others
(if I want to shut down the simulation, I want my stop message to take
precedence over anything else), and badly behaving actors can create
strange situations. For example, it is possible that some misbehaving
actor can fly in and start sending wakeup_and_move messages to ants
while they are executing the 8 step move and confuse the ant, the cell
the ant is trying to move to, and the cell the ant is currently on.
Despite its oddities, the language and the VM are super cool and I would
use them again when appropriate.
</p>

<p>
Unfortunately, I would not say that this project was particularly
appropriate for Erlang. The actor model was an interesting way to think
about ants and cells, but the problem doesn't quite fit Erlang's
strengths as a fault-tolerant language for distributed systems. There is
a possibility that the distributed nature of Erlang might enable some
interesting simulation sort of things, but there is little reason to
take advantage of the fault tolerance in a project like this.
Additionally, there are others ways to implement a simulation like this
which mitigate many of the issues I encountered along the way (but might
introduce other ones).
</p>

<p>
Overall, this was a fun project and I'm glad to have gotten to work on
it.
</p>

<p>
The buggy, messy code is on <a href="https://github.com/dpzmick/ants">github</a>.
</p>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
I like my ants like I like my editors.
</p></div></div>


</div>
</div>]]>
    </content>
  </entry>
  <entry>
    <title>Study Groups Pt. 2</title>
    <link rel="alternate" type="text/html" href="http://dpzmick.com/posts/2014-07-07-study-groups-pt-2.html"/>
    <id>urn:uuid:f9d7a477-a372-5cff-87dc-38f49c3de78e</id>
    <updated>2014-06-01</updated>
    <content type="html">
      <![CDATA[<style>.org-src-container {white-space: pre; font-family: monospace }</style>
               <p>
This post is intended to be a continuation of the previous post
discussing study groups. You can probably find that post pretty easily
on this site. If you haven't seen it, go back and read it!
</p>

<p>
I would also like to say that I am quite interested in criticism of this
little article. I don't intend to go much farther with this project, but
discussion about it could be quite interesting. And of course if you
find errors, let me know!
</p>

<div id="outline-container-org8a19f37" class="outline-2">
<h2 id="org8a19f37">Simulation</h2>
<div class="outline-text-2" id="text-org8a19f37">
<p>
I've expanded the model a bit for this one. Here is what happens:
</p>

<ol class="org-ol">
<li>People enter a room one at a time</li>
<li>People will chose the study group with best fitness and join it</li>
<li>On occasion, an individual will decide to be selfless, meaning, the
individual will not join a group if their joining will hurt the
fitness of the group

<ul class="org-ul">
<li>if an individual attempts to be selfless, but there are no groups
they can join without hurting them, they will have to join normally</li>
</ul></li>

<li>After a member has joined, for every group, if the group's size is
greater than the optimal group size, there is some chance the group
will split

<ul class="org-ul">
<li>if the group splits, half of the groups members stay in the group
and the other half all join groups one at a time (goto 1
essentially for every member that has left the group)</li>
</ul></li>
</ol>

<p>
Just to recap, fitness is determined based on group size, using this
differential equation:
</p>

<p>
\[ \frac{dF}{dn} = \alpha - \beta n \]
</p>

<p>
where $ &alpha; $ is an individual member's contribution (every member is
assumed to have the same contribution) and $ &beta; $ is the amount the
member will detract from the group (also assumed the same for all
members).
</p>

<p>
The chance an individual will be selfless is some percentage, also
assumed constant for all members.
</p>

<p>
And finally, the chance come group will split, given that it has
exceeded the optimal size, is another percentage.
</p>

<p>
One more detail, in the simulation, I have set a fixed number of
available groups. All groups start with 0 members. As long as the number
of available groups is substantially larger than the number of people to
join the groups, this fact doesn't seem to have an effect on the
results. However, if we do something like try and cram 16 people into 10
groups, that can get kind of interesting.
</p>

<p>
Because there is some element of randomness, I will run many trials of
the simulation to get results.
</p>
</div>
</div>

<div id="outline-container-org5d6cc46" class="outline-2">
<h2 id="org5d6cc46">Criticism of the model</h2>
<div class="outline-text-2" id="text-org5d6cc46">
<p>
There are a couple obvious problems with this model.
</p>

<ol class="org-ol">
<li>Realistically, people do not all contribute equally to a group, this
is a pretty dramatic simplification. Same for member detriment and
selflessness.

<ul class="org-ul">
<li>However, I did a small test to see how much of an impact this has
and I (weakly) concluded that for large $ n $ this effect goes
away. Unfortunatley, I'm not really testing with large $ n $, so we
will have to take this fact into consideration in our
interpretation of the results.</li>
</ul></li>

<li>The selflessness idea is fairly unrealistic. In reality an individual
can't determine if their joining will damage the fitness of the
group, because they have no concrete way to compute the group's
fitness. However, because this is done by chance, if we keep the
chance low we can approximate the effect of this optimal size
uncertainty. To explain what I mean, consider this. Assume an
individual can only estimate whether or not they will hurt a group,
and lets say they are only right 50 percent of the time. Then, lets
say that this person has a 50 percent chance of being selfless. So
essentially, given a group above optimal size, there is a 25 percent
chance the person won't join it. We can consider this when setting
the selflessness parameter.</li>
<li>The way groups split is fairly unrealistic. Groups, like individuals,
can't say for sure if they are above optimal size. However, using the
same argument as the previous, I think we can get away with this.</li>
</ol>

<p>
So, the biggest flaw we will have to overcome is (1), but, the specific
kind of experiment I plan to run lessens the impact of this problem.
</p>
</div>
</div>

<div id="outline-container-org7b04134" class="outline-2">
<h2 id="org7b04134">The Experiment</h2>
<div class="outline-text-2" id="text-org7b04134">
<p>
I intend to try and figure out if group splitting or individual
selflessness will do a better job at keeping groups close to optimal
size (or at least not create tons of 1 person groups or tons of very
large groups).
</p>

<p>
So, for some values of $ &alpha; $, and $ &beta; $, I varied both the
selflessness chance and the group split chance from 0.0 to 0.95, in 5
percent increments. So every pair is tested.
</p>

<p>
I've created some plots to demonstrate my results. I'm also kind of
lazy, so I didn't label them, but, I'll give you a badly labeled example
here just to be nice.
</p>

<p>
This particular image is shows the percentage of groups at optimal size
for various values of selflessness and group split chance.
</p>


<figure id="org9da3a98">
<img src="../static/study_groups/2/5_optimal.png" alt="5_optimal.png">

<figcaption><span class="figure-number">Figure 1: </span>joiners=5, max=50, optimal</figcaption>
</figure>

<p>
All of the images from here on out are essentially the same, although
they grayness may have a different meaning, I'll be careful to explain
what you are looking at in the file names and in this document, but I'm
not going to go back all the images, sorry!
</p>
</div>
</div>

<div id="outline-container-org73c7270" class="outline-2">
<h2 id="org73c7270">Results</h2>
<div class="outline-text-2" id="text-org73c7270">
<p>
First test we are going to look at.
</p>

<pre class="example" id="org65f9611">
member_contrib=1.000000
member_detriment=0.500000
num_joiners=5
max_groups=50
trials=1000
</pre>

<p>
Here are the images for all the results:
</p>
</div>

<ul class="org-ul">
<li><a id="orgeb38e37"></a>Percentage of groups at optimal size<br>
<div class="outline-text-5" id="text-orgeb38e37">

<figure id="org9dd19d2">
<img src="../static/study_groups/2/5_optimal.png" alt="5_optimal.png">

<figcaption><span class="figure-number">Figure 2: </span>joiners=5, at optimal</figcaption>
</figure>
</div>
</li>

<li><a id="org5719500"></a>Percentage of groups above optimal size<br>
<div class="outline-text-5" id="text-org5719500">

<figure id="orgfd5b091">
<img src="../static/study_groups/2/5_above_optimal.png" alt="5_above_optimal.png">

<figcaption><span class="figure-number">Figure 3: </span>joiners=5, above optimal</figcaption>
</figure>
</div>
</li>

<li><a id="org66341f8"></a>Percentage of groups below optimal size<br>
<div class="outline-text-5" id="text-org66341f8">

<figure id="org5987109">
<img src="../static/study_groups/2/5_below_optimal.png" alt="5_below_optimal.png">

<figcaption><span class="figure-number">Figure 4: </span>joiners=5, below optimal</figcaption>
</figure>
</div>
</li>

<li><a id="orge3c94fa"></a>Average fitness of groups<br>
<div class="outline-text-5" id="text-orge3c94fa">

<figure id="orgce10dff">
<img src="../static/study_groups/2/5_avg_fit.png" alt="5_avg_fit.png">

<figcaption><span class="figure-number">Figure 5: </span>joiners=5, average fitness</figcaption>
</figure>

<p>
So, what did we learn here? Well, it looks like the best way to increase
chances of getting groups to their optimal size is having a moderate
percentage of groups splitting, with no selflessness. We can also see
that we get the fewest groups below optimal size at this point (not many
really small groups), but we still end up with a decent percentage above
optimal size.
</p>

<p>
One possible explanation for this seemingly unintuitive result could lie
in my group splitting logic. When groups split, they split down to
optimal size, then the other members get a chance to go join other
groups. These members are likely to join small groups, close to their
optimal size, bringing the number of small groups down. We can end up
with a decent number of groups above optimal size because splitting
doesn't happen all that often (the split percent in the region we are
investigating is only 30%).
</p>

<p>
We can also kind of see the same thing happen with selflessness around
65-70 percent, but the effect is nowhere near as pronounced.
</p>

<p>
The other notable fact is that, although the greatest percentage of
optimal groups seems to happen with a small chance of groups splitting,
this is not where the average group fitness is highest. The greatest
rate of chance in average fitness still looks to be occurring as the
chance a group splits increases, meaning that the averages fitness of
groups improves faster as we increase the chance of a group splitting
than it improves as we increase the chance of selflessness.
</p>

<p>
Also, something strange seems to be happening with the average fitness
graph. I can't really explain that or form much of a conclusion about
it. Nothing else grabbed my attention.
</p>

<p>
I ran only a few more tests, here are the parameters and links to my
results.
</p>

<pre class="example" id="org66d7e04">
member_contrib=1.000000
member_detriment=0.500000
num_joiners=10
max_groups=50
trials=1000
</pre>

<p>
<a href="../static/study_groups/2/10_optimal.png">at optimal</a>
<a href="../static/study_groups/2/10_above_optimal.png">above optimal</a>
<a href="../static/study_groups/2/10_below_optimal.png">below optimal</a>
<a href="../static/study_groups/2/10_avg_fit.png">average fitness</a>
</p>

<pre class="example" id="org0850808">
member_contrib=1.000000
member_detriment=0.500000
num_joiners=15
max_groups=50
trials=1000
</pre>

<p>
<a href="../static/study_groups/2/15_optimal.png">at optimal</a>
<a href="../static/study_groups/2/15_above_optimal.png">above optimal</a>
<a href="../static/study_groups/2/15_below_optimal.png">below optimal</a>
<a href="../static/study_groups/2/15_avg_fit.png">average fitness</a>
</p>

<pre class="example" id="org4a381d8">
member_contrib=1.000000
member_detriment=0.500000
num_joiners=20
max_groups=50
trials=1000
</pre>

<p>
<a href="../static/study_groups/2/20_optimal.png">at optimal</a>
<a href="../static/study_groups/2/20_above_optimal.png">above optimal</a>
<a href="../static/study_groups/2/20_below_optimal.png">below optimal</a>
<a href="../static/study_groups/2/20_avg_fit.png">average fitness</a>
</p>

<p>
As you can see, if you looked at these images, the results are fairly
consistent across my tests.
</p>
</div>
</li>
</ul>
</div>

<div id="outline-container-orgf4d5ad4" class="outline-2">
<h2 id="orgf4d5ad4">Conclusion</h2>
<div class="outline-text-2" id="text-orgf4d5ad4">
<p>
It would seem that selflessness doesn't much help, but group splitting
does. If you are shooting to find the optimal group size, have a
moderate to low percent chance of splitting, and if you want to maximize
the average fitness of all the study groups in you meta-study-group
group, have a moderate to high chance of splitting. Either way,
splitting groups seems to improve the effectiveness of study groups more
so than people choosing not to join them when they think they would hurt
the group, which seems somewhat intuitive.
</p>

<p>
To briefly consider the real world, consider that some people damage the
group more than others, or help the group more. I think that this effect
doesn't really damage the strength of my conclusion, because I am simply
proposing that splitting groups when they seem to be becoming
unproductive may be an effective way to increase study group
effectiveness. When splitting the groups in real life, it is probably a
good idea to consider who's who, and of course, if a major productivity
killer person decides to not join a group, that will help the group out
quite a bit more than choosing to split later.
</p>

<p>
Hopefully this was interesting to you! If you've seen anything I haven't
please tell me!!
</p>
</div>
</div>

<div id="outline-container-orga52e9e1" class="outline-2">
<h2 id="orga52e9e1">A bit about implementation</h2>
<div class="outline-text-2" id="text-orga52e9e1">
<p>
I wrote this code a couple of different ways, but eventually settled on
Haskell as the language for the simulation. I also chose to leave
randomness in the simulation, instead of finding all possible outcomes
for some given set of parameters and computing percentages from that
(the number of cases seems quite large, although performance of what I
came up with isn't phenomenal). I also wrote a python script to run
multiple instances of the simulation (which is single threaded), and
collect the results. The, finally, I used R to spit out the rather
unpolished graphics I used.
</p>

<p>
The code is on github, here is a <a href="https://github.com/dpzmick/study_groups/tree/8731b5285997c88f0b3e889e57d60cbd9dbab3f3">link</a> to the commit used to write this
post.
</p>
</div>
</div>
]]>
    </content>
  </entry>
  <entry>
    <title>Study Groups Pt. 1</title>
    <link rel="alternate" type="text/html" href="http://dpzmick.com/posts/2014-05-31-study-groups-pt-1.html"/>
    <id>urn:uuid:3bbf5848-588f-57cc-8d0b-74d13e2cf045</id>
    <updated>2014-05-31</updated>
    <content type="html">
      <![CDATA[<style>.org-src-container {white-space: pre; font-family: monospace }</style>
               <p>
Almost without fail, whenever my friends and I get together to study for
something in a group, we end up in a group that, due to it's size,
decreases our productivity. I'm reading a book at the moment about
animal behavior, and one of the chapters referenced some research done
about animal group formation. In some models, with simulation, it can be
shown that groups almost always grow to be larger than their optimal
size. In this post I will discuss a preliminary attempt at modeling the
behavior of study group formation using similar methods. In later posts
I plan to strengthen the model and (hopefully) present possible
solutions to the problem.
</p>

<div id="outline-container-orgdcab736" class="outline-2">
<h2 id="orgdcab736">The Model</h2>
<div class="outline-text-2" id="text-orgdcab736">
<p>
The basic assumptions of the simulation we will use are as follows:
</p>

<ul class="org-ul">
<li>When a person "arrives" they must chose a study group to join (can't
walk away)</li>
<li>People arrive one at a time</li>
<li>Once someone has joined a group, they do not leave</li>
<li>People will always join the best group they can</li>
</ul>

<p>
We need a method to determine which group is "best," a group fitness
function. So, lets consider how the fitness of a group changes as people
join the group. Every person the joins benefits the group in some way,
but, if the group is large, adding another member will likely decrease
the group's productivity. To simplify this a bit more, add the following
assumptions:
</p>

<ul class="org-ul">
<li>Every person contributes the same amount to the group.</li>
<li>Every person hurts productivity by the same amount.</li>
<li>Both of these amounts are quantifiable</li>
</ul>

<p>
Using these assumptions we can write the following equation:
</p>

<p>
\[ \frac{dF}{dn} = \alpha - \beta n \]
</p>

<p>
where:
</p>

<ul class="org-ul">
<li>$ F $ is the fitness function.</li>
<li>$ n $ is the number of people currently in the group</li>
<li>$ &alpha; $ is an individual's contribution to the study group</li>
<li>$ &beta; $ is an individual's detriment to the study group</li>
</ul>

<p>
Let's explore this for a moment before moving on. Consider just $
\frac{dF}{dn} = &alpha; $. This piece of the equation tells us that as
the number of people in the study group changes, the change in the
fitness of the study group is proportional to $ &alpha; $, the individual
contribution rate. But, we know that as the number of people increases,
the effectiveness of the group decreases, so subtract something that
grows as the population grows: $ &beta; n $.
</p>

<p>
This equations is simple to solve, and we should impose the initial
condition $ F(0) = 0 $, as a group with zero members has 0 fitness. The
solutions then are:
</p>

<p>
\[ F(n) = \alpha n - \frac{\beta}{2} n^2 \]
</p>

<p>
Additionally, we probably want to know what the optimal study group size
is. We can easily find (by setting $ \frac{dF}{dn} = 0 $) that the
optimal size is $ \frac{\alpha}{\beta} $.
</p>
</div>
</div>

<div id="outline-container-org1c5731d" class="outline-2">
<h2 id="org1c5731d">The Simulation</h2>
<div class="outline-text-2" id="text-org1c5731d">
<p>
The simulation I have in mind is fairly simple.
</p>

<ol class="org-ol">
<li>Someone shows up</li>
<li>They evaluate all the study groups available to them</li>
<li>They join the best on available (best evaluated using fitness
function)</li>
<li>If all available groups have even fitness, join one at random</li>
<li>Repeat until there is no one left to join</li>
</ol>
</div>
</div>

<div id="outline-container-org0e1d76b" class="outline-2">
<h2 id="org0e1d76b">Implementation and Results</h2>
<div class="outline-text-2" id="text-org0e1d76b">
<p>
I wrote some python code to see how this system would perform. I will
leave the code at the bottom of the document. The results seem to
correspond with reality.
</p>


<figure id="orgfeca3c4">
<img src="../static/study_groups/1x.5.png" alt="1x.5.png" class="light-invert">

</figure>


<figure id="orgaea5683">
<img src="../static/study_groups/1x.4.png" alt="1x.4.png" class="light-invert">

</figure>

<p>
And here are links to all of the images I've generated at the time of
writing.
</p>

<ul class="org-ul">
<li><a href="../static/study_groups/1x.1.png">(a=1, b=.1)</a></li>
<li><a href="../static/study_groups/1x.2.png">(a=1, b=.2)</a></li>
<li><a href="../static/study_groups/1x.4.png">(a=1, b=.4)</a></li>
<li><a href="../static/study_groups/1x.5.png">(a=1, b=.5)</a></li>
<li><a href="../static/study_groups/1x.8.png">(a=1, b=.8)</a></li>
<li><a href="../static/study_groups/1x1.png">(a=1, b=1)</a></li>
</ul>
</div>
</div>

<div id="outline-container-orga9c9b6d" class="outline-2">
<h2 id="orga9c9b6d">Conclusion</h2>
<div class="outline-text-2" id="text-orga9c9b6d">
<p>
From this model, it seems like we tend to form groups larger than would
be optimal, because people continue to join the group once their joining
will decrease the productivity of the group but would increase their
personal productivity.
</p>

<p>
As I said at the top I do plan to explore this idea more. I hope to
build a more complicated simulator allowing groups to split, experiment
with a "selflessness factor," some chance that a person will not join a
group if it hurts the group but helps the person, and a few other
things. Please leave some feedback if this is interesting to you, we
could discuss more ideas!
</p>
</div>
</div>

<div id="outline-container-orgc6569d9" class="outline-2">
<h2 id="orgc6569d9">Simulation Code (not pythonic!)</h2>
<div class="outline-text-2" id="text-orgc6569d9">
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">from</span> math <span class="org-keyword">import</span> <span class="org-builtin">pow</span>
<span class="org-keyword">from</span> random <span class="org-keyword">import</span> randint
<span class="org-comment-delimiter">###### </span><span class="org-comment">Parameters</span>
<span class="org-variable-name">a</span> = 1.0 <span class="org-comment-delimiter"># </span><span class="org-comment">alpha define above</span>
<span class="org-variable-name">b</span> = .5  <span class="org-comment-delimiter"># </span><span class="org-comment">beta, also define above</span>
<span class="org-variable-name">maxgroups</span> = 15
<span class="org-variable-name">numjoiners</span> = 10

<span class="org-comment-delimiter">###### </span><span class="org-comment">Globals</span>
<span class="org-variable-name">pool</span> = [0] * maxgroups

<span class="org-comment-delimiter">###### </span><span class="org-comment">Functions</span>
<span class="org-keyword">def</span> <span class="org-function-name">fitness</span>(n):
    <span class="org-keyword">return</span> a*n - (b/2)*<span class="org-builtin">pow</span>(n,2)

<span class="org-comment-delimiter">##### </span><span class="org-comment">Simulation</span>
<span class="org-keyword">if</span> <span class="org-builtin">__name__</span> == <span class="org-string">"__main__"</span>:
    <span class="org-keyword">print</span>(<span class="org-string">"Starting the simulation"</span>)
    <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">xrange</span>(0, numjoiners):
        best = (-1, <span class="org-constant">None</span>)
        <span class="org-keyword">for</span> index, fits <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(<span class="org-builtin">map</span>(fitness, pool)):
            <span class="org-keyword">if</span> best[1] == <span class="org-constant">None</span> <span class="org-keyword">or</span> fits &gt; best[1]:
                best = (index, fits)
        <span class="org-keyword">if</span> best[0] &gt; 0:
            pool[<span class="org-variable-name">best</span>[0]] += 1
        <span class="org-keyword">elif</span> best[0] == 0:
            pool[randint(0, <span class="org-builtin">len</span>(pool) - 1)] += 1
        <span class="org-keyword">else</span>:
            <span class="org-keyword">print</span>(<span class="org-string">"I refuse to join these groups"</span>)
            <span class="org-keyword">break</span>
    <span class="org-keyword">print</span>(pool)
    <span class="org-keyword">print</span>(<span class="org-builtin">map</span>(fitness,pool))

    <span class="org-keyword">print</span>(<span class="org-string">"optimal group size %d"</span> % (a/b) )
    <span class="org-keyword">print</span>(<span class="org-string">"average non_empty group size %d"</span> %
            (<span class="org-builtin">sum</span>(pool) / <span class="org-builtin">len</span>(<span class="org-builtin">filter</span>(<span class="org-keyword">lambda</span> a: a != 0, pool))))
</pre>
</div>
</div>
</div>
]]>
    </content>
  </entry>
  <entry>
    <title>Decision Matrix</title>
    <link rel="alternate" type="text/html" href="http://dpzmick.com/posts/2014-01-17-decision-matrix.html"/>
    <id>urn:uuid:096b70d7-6a2e-5c35-982d-209720884fc8</id>
    <updated>2014-01-17</updated>
    <content type="html">
      <![CDATA[<style>.org-src-container {white-space: pre; font-family: monospace }</style>
               <p>
Decision matrix analysis is a simple way of selecting one of many
options. This post exists to allow me to dump thoughts somewhere (so I
don't forget them) and share them with others easily. I will probably
update this document as I have new ideas.
</p>

<div id="outline-container-org752b9fb" class="outline-2">
<h2 id="org752b9fb">Brief Description</h2>
<div class="outline-text-2" id="text-org752b9fb">
<p>
Simply put, a decision consists of objectives and alternatives. An
objective is something you want to fulfill by making the decision. For
example, a career decision objective may be "Decent Pay" or "Short
Commute." In deciding what university to attend, objectives may be
things such as "Academic Rating," "Class Size," and "Cost." These
objectives all have an importance, or weight. When selecting a car to
purchase, the objective "Low Fuel Consumption" may be extremely
important to you, but "Heated Seats" might be less important. I would
say that fuel economy is weighted more heavily than heated seats, if
this were the case.
</p>

<p>
Alternatives, or options, are the different options you have to choose
from, so, in the car example, my options may be a Honda Accord, Toyota
Camry, and a Maserati. To evaluate these options, assign a rating for
each of your chosen objectives. So, say my objectives were Cost and
Style. The Maserati would get a "Very Displeased" for cost, but a "Very
Pleased" for style, and the Camry would get a "Very pleased" for cost,
and a "Somewhat Pleased" for style.
</p>

<p>
So, this decision in table form would look something like this:
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-left">

<col  class="org-left">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Objectives</th>
<th scope="col" class="org-left">Cost: Important</th>
<th scope="col" class="org-left">Style: Somewhat Important</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Maserati</td>
<td class="org-left">Very Displeased</td>
<td class="org-left">Very Pleased</td>
</tr>

<tr>
<td class="org-left">Camry</td>
<td class="org-left">Very Pleased</td>
<td class="org-left">Somewhat Pleased</td>
</tr>
</tbody>
</table>

<p>
You can then define a scale for your ratings and weights and use the
matrix to determine which option best meets your needs. The score for
each option is the sum of each rating multiplied by the ratings weight.
</p>

<p>
This technique is extremely useful for increasing self awareness, as it
forces you to explain your thought process to yourself, place value on
your objectives, and collect fairly decent data about your options. The
technique also allows you to understand trade offs. In the example
above, if we chose the Camry, we would be sacrificing a bit on Style to
save on Cost. Because a low cost is important to us, we may be willing
to make that trade. Using the technique also creates a mechanism to
experiment. You can ask question like, "How much cheaper would the
Maserati need to be for it to become 'better' than the Camry," or, "How
much of a pay cut am I willing to take to continue living in Houston?"
</p>
</div>
</div>

<div id="outline-container-org0152d7b" class="outline-2">
<h2 id="org0152d7b">Mathematical Definitions</h2>
<div class="outline-text-2" id="text-org0152d7b">
<p>
To model a decision with \(n\) objectives and \(m\) alternatives, define:
</p>

<p>
The objectives vector:
</p>

<p>
\[ \mathbf{o} =
    \begin{pmatrix}
        w_1 \\
        w_2 \\
        \vdots  \\
        w_n
    \end{pmatrix} \]
</p>

<p>
where:
</p>

<p>
\(w_i\) = the weight given to the ith objective.
</p>

<p>
The vector
\[ \mathbf{a_i} = \begin{pmatrix} r_1 & r_2 & \cdots & r_m \end{pmatrix} \]
for the ith alternative, where \(r_k\) is the ith alternative's rating
for the kth objective.
</p>

<p>
The alternative matrix
</p>

<p>
\[ A = \begin{pmatrix}
    \mathbf{a_1} \\
    \mathbf{a_2} \\
    \vdots \\
    \mathbf{a_m}
\end{pmatrix} \]
</p>

<p>
The relative strengths of each alternative are given by \(A\mathbf{o}\).
</p>
</div>
</div>

<div id="outline-container-orgdf6128b" class="outline-2">
<h2 id="orgdf6128b">Example</h2>
<div class="outline-text-2" id="text-orgdf6128b">
<p>
Let us revist the car example. Suppose my objectives are Cost, Style,
and Comfort. Using a 1-5 importance scale, Cost would have an importance
of 5, Style and importance of 2, and Comfort an importance of 4.
</p>

<p>
This means
</p>

<p>
\[ \mathbf{o} = \begin{pmatrix}
    5 \\
    2 \\
    4
\end{pmatrix} \]
</p>

<p>
Now, let us consider 3 options and evaluate them on a using a negative 3
to 3 scale.
</p>

<p>
The Maserati would get a -3 for Cost, a 3 for Style, and a 3 for
Comfort.
</p>

<p>
A Camry would get a 1 for Cost, a 1 for Style, and a 2 for comfort.
</p>

<p>
And, a Civic would get a 3 for Cost (I have no idea if this is true), a
1 for style, and a 2 for comfort.
</p>

<p>
So, we build our alternatives matrix.
</p>

<p>
\[ A = \begin{pmatrix}
    -3 & 3 & 3 \\
     1 & 1 & 2 \\
     3 & 1 & 2
\end{pmatrix} \]
</p>

<p>
And get each alternative's score:
</p>

<p>
\[ A\mathbf{o} =
\begin{pmatrix}
    -3 & 3 & 3 \\
     1 & 1 & 2 \\
     3 & 1 & 2
\end{pmatrix} \begin{pmatrix}
    5 \\
    2 \\
    4
\end{pmatrix} = \begin{pmatrix}
    -3(5) + 3(2) + 3(4) \\
     1(5) + 1(2) + 2(4) \\
     3(5) + 1(2) + 2(4)
\end{pmatrix} = \begin{pmatrix}
    3 \\
    15 \\
    25
\end{pmatrix} \]
</p>

<p>
So, given our objectives, their importances, and our evaluation of our
options using those objectives, a Civic is probably the best option for
us.
</p>
</div>
</div>

<div id="outline-container-org7674a2d" class="outline-2">
<h2 id="org7674a2d">Analysis</h2>
<div class="outline-text-2" id="text-org7674a2d">
<p>
Now that we have a model of a decision, it is interesting to ask a few
questions about the model, for example:
</p>

<ol class="org-ol">
<li>What are we trading for what? In the car example we traded our desire
for comfort and style for our desire to keep cost low.</li>
<li>How much would we have to sacrifice on cost to make the Maserati the
best option?</li>
<li>Which of our objectives have impact? If an objective gets the same,
or close to the same rating for each candidate, it doesn't really
impact the decision.</li>
</ol>
</div>
</div>

<div id="outline-container-org1656372" class="outline-2">
<h2 id="org1656372">Trade offs</h2>
<div class="outline-text-2" id="text-org1656372">
<p>
Referring back to our car example, how can we try and understand what we
are trading if we chose the Maserati over the Civic?
</p>

<p>
Let's define \(\mathbf{t}(i,j) = \mathbf{a_i} - \mathbf{a_j}\) to be the
trades made if alternative \(i\) is selected over alternative \(j\). Any
negative value in \(\mathbf{t}\) represents a sacrificed objective in
the trade, and any positive value represents something gained. So, if
the 1st element is negative and the last two are positive, we've
sacrificed on our first objective for gains on our second and third.
</p>

<p>
Remember that \(\mathbf{a\_1} = \begin{pmatrix} -3 & 3 & 3 \end{pmatrix}\) for the Maserati
and \(\mathbf{a\_3} = \begin{pmatrix} 3 & 1 & 2 \end{pmatrix}\) for the Civic.
</p>

<p>
\(\mathbf{t}(1,3) = \mathbf{a\_1} - \mathbf{a\_3} = \begin{pmatrix} -6 & 1 & 1 \end{pmatrix}\)
So, if we were to choose the Maserati over the Civic, we would be
sacrificing money (first objective) to gain comfort and style (second
and third objectives). But, we aren't willing to make this trade; we've
demonstrated that in the previous example. Let us investigate the trade
here again. A trade make sense if the gains in the trade outweigh the
loses ($gains - losses &gt; 0 $). The total gain is the sum of each
positive number in \(t\) multiplied by the weight associated with it.
Similarly, total loss is just the sum of each negative number multiplied
by the associated weight.
</p>

<p>
Remember
</p>

<p>
\[ \mathbf{o} = \begin{pmatrix}
    5 \\
    2 \\
    4
\end{pmatrix} \]
</p>

<p>
In this example, \(\text{gains} = 1(2) + 1(4) = 6\) and \(\text{losses} = 6(5) = 30\).
We can see this is not a valid trade because \(6 - 30 = -24\) is much
less than zero! If we were to go the other way (what do we trade if we
chose the Civic over the Maserati), all the signs would reverse, and the
trade would be a good trade.
</p>

<p>
Since gains are positive and losses are negative in the vector we get by
subtracting alternatives, we can express the validity of a choice of
alternative \(i\) over alternative \(j\) more simply with the statement:
\(\sum_{k=0}^{n} \mathbf{t}(i,j)_k w_k \gt 0\)
</p>

<p>
Check: \(\sum_{k=0}^{3} \mathbf{t}(1,3)_k w_k = -6(5) + 1(2) + 1(4) = -24\)
</p>

<p>
To understand what adjustments in objective importance might be needed
to make the Maserati a better choice, we can try adjusting weights and
recalculating Or, we can try using a bit of linear programming.
</p>

<p>
We are attempting to satisfy \(-6w_1 + 1w_2 + 1w_3 \gt 0\) under the
constraint \(0 \ge w_1, w_2, w_3 \ge 5\) (from our rating scale). A good
solver can give you results in this region. To simplify the solution,
lets say we feel very strongly about the importance of cost, we don't
plan on assigning any less importance to that, but a good salesman may
be able to convince us that our comfort or style is more important than
we think at the moment.
</p>

<p>
This leaves us with $-30 +1w_2 + 1w_3 &gt; 0 $ bounded by $ 0 &ge; w_2, w_3 &ge; 5$
</p>

<p>
Making a plot of this region, we can see that there exit no feasible
solutions, so, we can tell that it is not possible for us to choose to
purchase the Maserati over the Civic without compromising on cost.
</p>

<p>
The green region is the region given by our weighting system (1-5) and
the orange region is the region of weights for Comfort and Style that
would make the Maserati reasonable for us.
</p>


<figure id="org4064969">
<img src="../static/decision_matrix/not_feasible.png" alt="not_feasible.png" class="light-invert">

<figcaption><span class="figure-number">Figure 1: </span>plot of feasible region</figcaption>
</figure>

<p>
If we decided to become flexible on cost and make comfort extremely
important (weight of 5), then the region would look like this (where the
red region is the region in which we would the Maserati, the importance
of cost is along the y-axis and the importance of style is along the
x-axis)
</p>


<figure id="org7af8b01">
<img src="../static/decision_matrix/feasible.png" alt="feasible.png" class="light-invert">

<figcaption><span class="figure-number">Figure 2: </span>plot of feasible region</figcaption>
</figure>

<p>
This analysis could continue and could be done in more dimensions
analytically, but I believe I have demonstrated the methodology I've
found to be interesting (maybe even useful?). I may explain in detail
how I choose to use some of these ideas in WhichOne in a future post.
</p>
</div>

<div id="outline-container-org357c3d3" class="outline-3">
<h3 id="org357c3d3">Other ideas:</h3>
<div class="outline-text-3" id="text-org357c3d3">
<ol class="org-ol">
<li>Determine analytically what weights to assign so that an alternative
beats another (instead of just guessing).</li>
<li>Find a way to represent all the trades that exist in a decision with
a single structure.</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-org3cc76d2" class="outline-2">
<h2 id="org3cc76d2">Objective Impact</h2>
<div class="outline-text-2" id="text-org3cc76d2">
<p>
Say I give you
</p>

<p>
\[ \mathbf{o} = \begin{pmatrix}
    5 \\
    1 \\
    3
\end{pmatrix} \]
</p>

<p>
and
</p>

<p>
\[ A = \begin{pmatrix}
    3 & -1 & 3 \\
    3 & 3  & 1 \\
    3 & -1 & -3
\end{pmatrix} \]
</p>

<p>
Notice that the rankings for the first objective are all exactly the
same! This means that the first objective has no impact on the decision;
it only inflates scores. This fact motivates a method of determining
objective impact.
</p>

<p>
My dad suggests using the variance of the weighted ratings to determine
this impact score. Before I discuss my thoughts about this method let me
explain it. First a bit more notation.
</p>

<p>
let
</p>

<p>
\[ \mathbf{o_k} = \begin{pmatrix}
    0 \\
    \vdots \\
    0 \\
    w_k \\
    0 \\
    \vdots \\
    0
\end{pmatrix} \]
</p>

<p>
be the vector containing the weight of the kth objective, in the
appropriate space, with all other weights set to zero.
</p>

<p>
I've decided to call \(A\mathbf{o}_k\) the impact vector for objective
\(k\) because the vector represents how the objective \(k\) changes
alternatives scores in this decision.
</p>

<p>
Using the above defined objectives vector and alternatives matrix we get
the following impact vectors:
</p>

<p>
\[ A \begin{pmatrix}
    5 \\
    0 \\
    0
\end{pmatrix} = \begin{pmatrix}
    15 \\\\
    15 \\\\
    15
\end{pmatrix} \]
</p>

<p>
\[ A \begin{pmatrix}
    0 \\
    1 \\
    0
\end{pmatrix} = \begin{pmatrix}
    -1 \\
    3 \\
    -1
\end{pmatrix} \]
</p>

<p>
\[ A \begin{pmatrix}
    0 \\
    0 \\
    3
\end{pmatrix} = \begin{pmatrix}
    9 \\
    1 \\
    -9
\end{pmatrix} \]
</p>

<p>
Now, let the impact of the kth objective \(Impact(k) = PopulationVariance( A\mathbf{o\_k} )\),
so in this example \(Impact(1) = 0\), \(Impact(2) = \frac{32}{9} \approx 3.5556\)
\(Impact(3) = \frac{488}{9} \approx 54.222\)
</p>

<p>
These results seem to be a good indicator of how much impact each
objective has on the decision. However, it may be better to use the
standard deviation instead of variance to reduce the effect squaring.
Variance/standard deviation of the impact vectors is also a good measure
of impact because it not only factors in ratings for each objective and
the score each objective was given. However, I'm not entirely convinced
that the variance or s.d. gives the best possible picture of how an
objectives "changes" a decision, because it only looks at impact
vectors, not at how these vectors pull your choices one way or another
(doesn't factor in trade offs to determine influence). Again, this is
more of a theoretical question, practically variance/s.d. performs well.
</p>
</div>

<div id="outline-container-org457efd7" class="outline-3">
<h3 id="org457efd7">WARNING: What follows is not well structured or explained.</h3>
<div class="outline-text-3" id="text-org457efd7">
<p>
Here is another idea for understanding trade offs and objective impact
I've been toying with.
</p>

<p>
Lets make the problem a 2D problem, for the sake of visualization, by
dropping the last alternative. This leaves us with
</p>

<p>
\[ \mathbf{o} = \begin{pmatrix}
    5 \\
    1 \\
    3
\end{pmatrix} \]
</p>

<p>
and
</p>

<p>
\[ A = \begin{pmatrix}
    3 & -1 & 3 \\
    3 & 3  & 1 \\
\end{pmatrix} \]
</p>

<p>
And, our impact vectors are
</p>

<p>
\[ \begin{pmatrix}
    15 \\
    15
\end{pmatrix} \]
</p>

<p>
\[ \begin{pmatrix}
    -1 \\
    3
\end{pmatrix} \]
</p>

<p>
\[ \begin{pmatrix}
    9 \\
    3
\end{pmatrix} \]
</p>

<p>
Let's plot those along with the line \(y = x\)
</p>


<figure id="orgb7adbfe">
<img src="../static/decision_matrix/impact.png" alt="impact.png" class="light-invert">

<figcaption><span class="figure-number">Figure 3: </span>plot of impact vectors</figcaption>
</figure>

<p>
This plot may be a bit difficult to wrap your head around (it is for
me), but let's walk through it. Our x and y axis represent alternative
scores.
</p>

<p>
Think about what would happen if an objective resulted in an impact
vector of
</p>

<p>
\[ \begin{pmatrix}
    15 \\\
    0
\end{pmatrix} \]
</p>

<p>
This objective clearly favors the first alternative (it adds 15 to \(a_1\)'s
score, and 0 to \(a_2\)'s score. In this case that would be something
that very much favors the first alternative. Plotted, we would get
this.
</p>


<figure id="orgd6d46ed">
<img src="../static/decision_matrix/wow.png" alt="wow.png" class="light-invert">

<figcaption><span class="figure-number">Figure 4: </span>wow plot</figcaption>
</figure>

<p>
So, we can say, in the 2D case, that the closer to the positive x-axis a
vector is (\(x \gt y\)), the more it favors the first alternative. The
closer to the positive y-axis the vector is (\(x \lt y\)), the more it
favors the second alternative. So, looking back at our example for this
section, the big blue vector has no impact.
</p>

<p>
It may be possible to define an importance function using these vectors
and their distance from the neutral line (\(x_1 = x_2 = \cdots = x_n\))
for n alternatives, but I haven't yet explored this entirely. If I do,
I will post again probably explaining the process. Practically,
variance works well enough. But, I think this is a really cool, fun
way to think about objectives.
</p>
</div>
</div>

<div id="outline-container-org57bd40e" class="outline-3">
<h3 id="org57bd40e">Maserati and Camry example</h3>
<div class="outline-text-3" id="text-org57bd40e">
<p>
Let's do this with the Maserati and Camry again. Same objectives.
</p>

<p>
\[ \mathbf{o} = \begin{pmatrix}
    5 \\
    2 \\
    4
\end{pmatrix} \]
</p>

<p>
and only two alternatives (to avoid going into 3d space)
</p>

<p>
\[ A = \begin{pmatrix}
    -3 & 3 & 3 \\
     3 & 1 & 2
\end{pmatrix} \]
</p>

<p>
Impact Vectors:
</p>

<p>
For Cost (in blue):
</p>

<p>
$$ A
</p>
\begin{pmatrix} 5 \\ 0 \\ 0 \end{pmatrix}
<p>
= \begin{pmatrix}
    -15 <br>
    15
\end{pmatrix} $$
</p>

<p>
For Comfort (in orange):
</p>

<p>
\[ A \begin{pmatrix} 0 \\ 2 \\ 0 \end{pmatrix} =
\begin{pmatrix}
    6 \\
    2
\end{pmatrix} \]
</p>

<p>
For Style (in red):
</p>

<p>
\[ A \begin{pmatrix} 0 \\ 0 \\ 4 \end{pmatrix} =
\begin{pmatrix}
    12 \\
    8
\end{pmatrix} \]
</p>

<p>
Here is a plot:
</p>


<figure id="orga6073d2">
<img src="../static/decision_matrix/maserati_camry.png" alt="maserati_camry.png" class="light-invert">

<figcaption><span class="figure-number">Figure 5: </span>maserati camry plot</figcaption>
</figure>

<p>
In terms of impact, the cost vector is perpendicular to the neutral
line. This is as far from neutral as possible! Cost clearly has a large
amount of impact. Understanding the "which direction does this objective
pull my decision" thing is quite a bit harder here and I can only kind
of see it. But, this train of though may still hold some potential.
</p>
</div>
</div>

<div id="outline-container-org906a0ad" class="outline-3">
<h3 id="org906a0ad">Other ideas:</h3>
<div class="outline-text-3" id="text-org906a0ad">
<ol class="org-ol">
<li>Finish determining impact using distance of impact vector to neutral
line.</li>
<li>Evaluate trade offs using the "pull" of an impact vector.</li>
</ol>

<p>
Thanks for reading! If you have any thoughts please drop them in the
comments.
</p>
</div>
</div>
</div>
]]>
    </content>
  </entry>

</feed>
